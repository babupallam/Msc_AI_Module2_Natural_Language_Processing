{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5.1: GCN Model Integration in an NLP Pipeline\n",
        "\n",
        "In this section, we’ll integrate our prepared data into a GCN model to perform an NLP task. We’ll build on the adjacency matrix and node features developed in previous sections to create a GCN model that can analyze sentence structure. We’ll go through each stage of model integration, including defining the GCN architecture, setting up a sample NLP task (such as sentence classification), and running training and evaluation.\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "1. **Defining the GCN Model Architecture**\n",
        "2. **Setting Up the NLP Task (Sentence Classification)**\n",
        "3. **Preparing the Data for the Model**\n",
        "4. **Training the GCN Model**\n",
        "5. **Evaluating the Model**\n",
        "6. **Code Walkthrough**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FoDDjLvK2c6M"
      },
      "id": "FoDDjLvK2c6M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Defining the GCN Model Architecture\n",
        "\n",
        "The GCN architecture we’ll use will have:\n",
        "   - **Input Layer**: Accepts node features (one-hot encoding, POS tags, embeddings).\n",
        "   - **Hidden Layers**: Graph convolutional layers for message passing and feature aggregation.\n",
        "   - **Output Layer**: Generates final predictions for each node or an aggregated graph-level output.\n"
      ],
      "metadata": {
        "id": "SnQVtfio2eI-"
      },
      "id": "SnQVtfio2eI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: GCN Model Definition\n"
      ],
      "metadata": {
        "id": "EuYL_QtD2eNW"
      },
      "id": "EuYL_QtD2eNW"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        A single GCN layer that performs a graph convolution operation.\n",
        "\n",
        "        Parameters:\n",
        "        - in_features (int): Number of input features for each node.\n",
        "        - out_features (int): Number of output features for each node after the layer.\n",
        "        \"\"\"\n",
        "        super(GCNLayer, self).__init__()\n",
        "        # Linear transformation to map input features to out_features\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        \"\"\"\n",
        "        Forward pass of the GCN layer.\n",
        "\n",
        "        Parameters:\n",
        "        - node_features (torch.Tensor): Input node features matrix of shape (num_nodes, in_features).\n",
        "        - adj_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output node features after applying the graph convolution, shape (num_nodes, out_features).\n",
        "        \"\"\"\n",
        "        # Step 1: Apply a linear transformation to node features\n",
        "        transformed_features = self.linear(node_features)\n",
        "\n",
        "        # Step 2: Aggregate the features from neighboring nodes using the adjacency matrix\n",
        "        aggregated_features = torch.matmul(adj_matrix, transformed_features)\n",
        "\n",
        "        # Step 3: Normalize by node degrees to account for varying neighbor counts\n",
        "        degree_matrix = adj_matrix.sum(dim=1, keepdim=True)  # Degree matrix derived from adjacency matrix\n",
        "        normalized_features = aggregated_features / degree_matrix  # Divide by degree to normalize\n",
        "\n",
        "        # Step 4: Apply a ReLU non-linear activation function\n",
        "        return F.relu(normalized_features)\n",
        "\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        A two-layer GCN model, stacking two GCN layers for message passing.\n",
        "\n",
        "        Parameters:\n",
        "        - input_dim (int): Dimension of input features for each node.\n",
        "        - hidden_dim (int): Dimension of hidden layer output features.\n",
        "        - output_dim (int): Dimension of the final output layer (e.g., number of classes for classification).\n",
        "        \"\"\"\n",
        "        super(GCNModel, self).__init__()\n",
        "        # First GCN layer from input_dim to hidden_dim\n",
        "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
        "        # Second GCN layer from hidden_dim to output_dim\n",
        "        self.gcn2 = GCNLayer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        \"\"\"\n",
        "        Forward pass of the two-layer GCN model.\n",
        "\n",
        "        Parameters:\n",
        "        - node_features (torch.Tensor): Input node features matrix of shape (num_nodes, input_dim).\n",
        "        - adj_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output node features after two layers, shape (num_nodes, output_dim).\n",
        "        \"\"\"\n",
        "        # First layer: apply GCN layer 1\n",
        "        x = self.gcn1(node_features, adj_matrix)\n",
        "        # Second layer: apply GCN layer 2\n",
        "        x = self.gcn2(x, adj_matrix)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "INtFAZ27eHBs"
      },
      "id": "INtFAZ27eHBs",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Setting Up the NLP Task (Sentence Classification)\n",
        "\n",
        "In this example, we’ll set up a simple **sentence classification** task where the model classifies a sentence as either **positive** or **negative**. Each word in the sentence will be a node, and the sentence-level prediction will be obtained by aggregating information across nodes.\n",
        "\n",
        "#### Task Setup:\n",
        "- **Labels**: Assign binary labels (0 for negative, 1 for positive) to sentences.\n",
        "- **Aggregation**: After passing node features through the GCN, aggregate the node embeddings to get a graph-level representation, which we then use for sentence classification.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RYhDN_c82eRj"
      },
      "id": "RYhDN_c82eRj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Preparing the Data for the Model\n",
        "\n",
        "To prepare the data, we’ll:\n",
        "1. Use the adjacency matrix created from dependency parsing.\n",
        "2. Define node features (e.g., one-hot, POS, embeddings).\n",
        "3. Combine the node features and adjacency matrix as input to the GCN model.\n"
      ],
      "metadata": {
        "id": "S8uDqo912eWI"
      },
      "id": "S8uDqo912eWI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Example Data Setup\n"
      ],
      "metadata": {
        "id": "yQSbfUdB2ebK"
      },
      "id": "yQSbfUdB2ebK"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model for POS tagging and embeddings\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define vocabulary and one-hot encoding function\n",
        "vocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "def one_hot_encode(sentence_tokens, vocab_dict):\n",
        "    features = []\n",
        "    for token in sentence_tokens:\n",
        "        one_hot = [0] * len(vocab_dict)\n",
        "        if token in vocab_dict:\n",
        "            one_hot[vocab_dict[token]] = 1\n",
        "        features.append(one_hot)\n",
        "    return np.array(features)\n",
        "\n",
        "def pos_tag_features(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    unique_tags = list(set(pos_tags))\n",
        "    pos_dict = {tag: i for i, tag in enumerate(unique_tags)}\n",
        "    features = []\n",
        "    for tag in pos_tags:\n",
        "        one_hot = [0] * len(pos_dict)\n",
        "        one_hot[pos_dict[tag]] = 1\n",
        "        features.append(one_hot)\n",
        "    return np.array(features)\n",
        "\n",
        "def word_embedding_features(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    features = [token.vector for token in doc]\n",
        "    return np.array(features)\n",
        "\n",
        "def create_combined_features(sentence, vocab_dict):\n",
        "    doc = nlp(sentence)\n",
        "    sentence_tokens = [token.text for token in doc]\n",
        "    one_hot_feats = one_hot_encode(sentence_tokens, vocab_dict)\n",
        "    pos_feats = pos_tag_features(sentence)\n",
        "    embedding_feats = word_embedding_features(sentence)\n",
        "    combined_feats = np.concatenate((one_hot_feats, pos_feats, embedding_feats), axis=1)\n",
        "    return combined_feats\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "combined_features = create_combined_features(sentence, vocab_dict)\n",
        "\n",
        "# Create an adjacency matrix with self-loops for the sentence\n",
        "def create_adjacency_matrix_with_loops(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    num_tokens = len(doc)\n",
        "    adj_matrix = np.zeros((num_tokens, num_tokens), dtype=int)\n",
        "    for token in doc:\n",
        "        adj_matrix[token.i][token.head.i] = 1\n",
        "        adj_matrix[token.head.i][token.i] = 1\n",
        "    np.fill_diagonal(adj_matrix, 1)\n",
        "    return adj_matrix\n",
        "\n",
        "adj_matrix_with_loops = create_adjacency_matrix_with_loops(sentence)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "node_features = torch.tensor(combined_features, dtype=torch.float32)\n",
        "adj_matrix = torch.tensor(adj_matrix_with_loops, dtype=torch.float32)\n",
        "label = torch.tensor([1], dtype=torch.long)  # Example label (1 for positive, 0 for negative)\n",
        "\n",
        "# Display tensors to confirm setup\n",
        "print(\"Node Features Tensor:\\n\", node_features)\n",
        "print(\"Adjacency Matrix Tensor:\\n\", adj_matrix)\n",
        "print(\"Label Tensor:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vf3s-7Oea6o",
        "outputId": "a02fb4a2-9561-4b09-dcad-c2472ee69168"
      },
      "id": "8vf3s-7Oea6o",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features Tensor:\n",
            " tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0466e+00, -6.3125e-01, -5.6540e-01,  2.7119e+00,\n",
            "         -1.0801e+00, -4.9187e-02, -7.9210e-01,  6.1598e-02, -6.1989e-01,\n",
            "          1.6166e+00,  1.4493e+00,  1.3127e+00, -6.7903e-01, -1.2306e+00,\n",
            "         -7.8954e-01, -1.0821e+00, -8.0464e-01,  1.6262e+00, -8.7126e-01,\n",
            "          4.0537e-01, -1.1336e+00, -3.7326e-01, -6.6686e-01, -1.6324e+00,\n",
            "          1.8673e+00, -2.4132e-01,  1.0853e+00,  8.6994e-02, -9.4281e-02,\n",
            "          6.0370e-01,  1.2150e+00, -1.2031e+00,  9.7626e-01, -2.0013e+00,\n",
            "         -6.6515e-02,  9.5435e-01,  2.6909e-01, -7.1802e-01,  2.5988e-01,\n",
            "          3.8899e+00, -8.0076e-02,  1.2519e+00, -1.3616e+00,  9.7839e-01,\n",
            "         -9.9233e-01, -8.0711e-02, -4.8829e-01,  2.3329e+00,  1.2838e+00,\n",
            "          9.2897e-02, -9.7115e-01, -3.6849e-01,  5.5837e-01,  5.8041e-01,\n",
            "          8.4477e-01,  7.8262e-01, -1.1075e+00,  9.8441e-01, -9.1765e-01,\n",
            "         -1.3616e-01, -4.8621e-01,  3.5007e-01, -4.7011e-02, -4.8244e-01,\n",
            "          3.0105e-01, -2.5362e-01, -1.7427e-01, -9.8540e-01, -6.6830e-01,\n",
            "         -1.2227e+00,  4.2672e-02,  9.4135e-01,  3.6272e-01, -3.7862e-01,\n",
            "         -3.6757e-01, -1.0384e+00,  7.6296e-02, -8.0155e-01,  8.9522e-01,\n",
            "          1.2948e-01, -2.0569e-01, -1.2827e+00, -8.4814e-01, -1.5223e+00,\n",
            "          2.1006e+00,  2.6072e-01,  9.9722e-01, -5.7762e-01,  1.3976e-01,\n",
            "         -4.4610e-01, -1.7746e-01, -2.7478e-01,  2.4672e-02, -2.6006e-01,\n",
            "         -4.9468e-02, -2.7745e-01],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00, -2.7830e-01, -2.7215e-01, -5.6672e-01,  3.4908e-01,\n",
            "          7.5255e-02,  5.1887e-01,  2.7152e+00,  7.8152e-01, -3.2749e-02,\n",
            "         -3.9680e-01,  8.1274e-01, -5.9804e-01, -4.7571e-01, -2.1330e-01,\n",
            "         -5.7066e-01,  3.9056e-01,  7.6172e-01, -7.9528e-01, -6.4704e-02,\n",
            "          4.0032e-01,  8.5267e-01, -2.1082e-03,  8.4364e-01, -1.6039e-01,\n",
            "         -4.7434e-03,  1.6328e-01,  6.0075e-01, -3.8785e-01,  1.2790e+00,\n",
            "          1.1133e-01, -5.3935e-01, -1.3522e+00, -8.3967e-01, -7.1345e-01,\n",
            "         -4.0346e-01, -1.0706e+00, -3.3082e-01, -6.6639e-01,  4.3903e-01,\n",
            "          4.0830e-01, -1.7506e+00,  8.1692e-01,  3.7977e-01,  1.6986e+00,\n",
            "         -7.0632e-01,  6.5648e-02,  1.1635e+00,  2.4660e+00,  1.7616e-02,\n",
            "          3.6189e-01, -7.5553e-01, -5.2672e-01, -2.1548e-01, -4.7824e-01,\n",
            "          9.8119e-01, -2.1651e-01,  1.2059e+00,  8.5868e-02, -2.8156e-02,\n",
            "         -2.0614e-01, -8.5902e-02, -5.1092e-01, -1.1201e+00,  3.2073e-01,\n",
            "          3.4802e-01, -9.2195e-01, -6.7805e-01,  7.4530e-03,  2.3161e-01,\n",
            "         -1.2330e+00,  6.2556e-01, -3.5276e-01,  1.0257e+00, -1.7551e+00,\n",
            "          3.2152e-01, -8.6125e-01, -4.0212e-01, -1.2579e+00,  7.9955e-01,\n",
            "         -1.9703e+00,  1.3464e-01,  8.3858e-01, -1.5642e+00, -5.9977e-01,\n",
            "          2.4085e-01, -1.6819e-01, -2.7681e-01, -1.4349e-01, -6.4928e-01,\n",
            "          4.7014e-01, -5.3835e-02,  5.7702e-01,  1.7431e+00, -1.4278e-01,\n",
            "          3.7690e-01,  1.7775e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.2089e-01,  9.6797e-01,  5.7002e-01, -8.6154e-01,\n",
            "         -1.9329e-01, -9.4405e-01, -1.4777e+00, -8.4947e-01,  4.4858e-01,\n",
            "          4.7003e-01,  4.5836e-01,  5.4919e-01, -8.8852e-01, -8.4964e-01,\n",
            "         -5.4730e-01, -3.6774e-01, -6.1365e-01, -7.4975e-01,  2.6458e-01,\n",
            "          7.7260e-01, -3.0023e-01,  8.9829e-01, -5.4176e-01, -1.1204e+00,\n",
            "         -5.3247e-01, -5.5761e-01, -8.6956e-01,  6.0031e-01,  7.8129e-02,\n",
            "         -7.3234e-01,  2.2336e-02, -3.8570e-02,  4.4550e-02,  9.9966e-01,\n",
            "         -8.5930e-01,  4.0388e-02, -6.1088e-01, -3.6136e-01, -2.9084e-01,\n",
            "         -1.3732e+00,  1.8201e+00,  1.3455e+00,  8.5533e-01, -6.5450e-01,\n",
            "          3.9870e-01, -8.8511e-01,  1.0991e+00, -1.1684e+00,  6.6168e-01,\n",
            "          1.5853e+00, -1.1566e+00, -4.9199e-01, -1.1498e+00, -5.2573e-01,\n",
            "          8.4986e-01, -2.5071e-01,  1.2003e+00,  2.7931e-01, -5.7366e-01,\n",
            "          1.1968e-02, -1.1808e-01,  4.3878e-01, -7.3117e-01,  1.9779e+00,\n",
            "          1.0692e+00, -1.2015e-02, -2.3207e-01, -1.3452e+00,  1.0574e+00,\n",
            "          9.8239e-01,  6.4031e-01,  8.8139e-01,  2.2901e-01, -8.5030e-01,\n",
            "         -3.5013e-01,  4.8815e-01, -8.3415e-01,  1.4968e-01, -5.3698e-01,\n",
            "         -1.0501e+00, -6.4595e-02, -1.7392e-01, -4.6627e-01,  1.4185e+00,\n",
            "          1.4131e+00,  1.9416e-01,  5.4089e-01, -9.6600e-01, -8.6368e-01,\n",
            "         -4.1394e-01,  5.6898e-01, -6.6868e-01, -3.8467e-01,  8.4789e-01,\n",
            "          1.2682e+00,  1.4624e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  8.8363e-02,  6.8898e-01,  3.7087e-01, -9.9162e-01,\n",
            "          3.2143e-01, -1.0220e+00, -6.4119e-01, -1.8897e+00,  4.0421e-01,\n",
            "          8.5032e-01,  4.4113e-01, -8.0841e-01, -7.3891e-01, -8.9062e-01,\n",
            "          5.0821e-01, -4.8691e-01, -1.4978e+00,  1.3634e+00,  2.6757e-01,\n",
            "          6.5181e-01, -2.4141e-01, -6.2450e-01, -6.6104e-01, -5.8367e-01,\n",
            "          1.1655e+00, -5.1826e-01,  1.2206e+00, -6.9513e-01,  7.5096e-01,\n",
            "          3.2317e-01,  8.2030e-02, -3.7350e-01,  6.5122e-01, -3.7203e-01,\n",
            "         -4.9859e-01,  6.5745e-01,  9.1281e-01,  4.1721e-01,  8.8202e-01,\n",
            "         -1.0903e+00, -3.6643e-01,  2.7609e-01, -9.0137e-01, -1.2292e+00,\n",
            "          3.6166e-01,  5.3718e-01,  7.6810e-01, -4.2810e-01, -5.6994e-02,\n",
            "         -1.0932e+00, -1.1633e+00,  6.8617e-01,  8.0640e-01,  3.2472e-01,\n",
            "         -1.7163e+00, -2.2577e-01, -7.1181e-01, -5.4241e-01,  1.1366e+00,\n",
            "         -1.0909e+00, -1.1210e+00, -1.2996e+00, -3.4786e-01,  1.1831e+00,\n",
            "         -3.3749e-02,  1.0548e+00,  1.2703e+00, -6.4579e-01, -3.3163e-01,\n",
            "          1.5699e-01,  1.3454e+00, -1.9342e-01, -8.6390e-02, -2.4086e-02,\n",
            "         -8.2817e-01,  1.0954e+00, -4.9836e-01,  1.1588e+00,  5.7212e-02,\n",
            "         -5.6857e-01,  4.0182e-01,  5.2155e-01, -3.0204e-01,  7.9412e-01,\n",
            "          1.4695e+00,  1.4173e-01, -3.0142e-01,  1.4899e+00, -6.8626e-01,\n",
            "          8.5028e-01,  7.0571e-02, -2.8820e-01, -2.7844e-01,  5.8317e-01,\n",
            "         -8.9058e-01,  1.4469e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0608e+00,  4.6738e-02, -8.4570e-01,  1.0409e+00,\n",
            "          8.8011e-02,  4.2018e-01, -9.2696e-01, -4.4648e-01,  4.9820e-01,\n",
            "          1.4338e+00,  2.8563e-01,  5.3766e-01,  1.3688e+00,  5.5870e-01,\n",
            "         -1.0579e+00, -1.2381e+00,  5.8224e-01,  1.2231e+00,  3.7855e-02,\n",
            "         -6.7967e-01, -1.4820e+00, -1.3614e-01, -9.6220e-01, -1.3247e+00,\n",
            "          1.0489e+00, -4.2452e-01,  2.9633e-01, -6.6203e-02,  9.4682e-01,\n",
            "          9.7303e-01,  7.3550e-01, -1.5553e-01,  1.5346e-01, -5.5300e-01,\n",
            "          1.2249e+00,  1.0922e-01,  1.5794e+00,  5.9524e-01, -1.0335e+00,\n",
            "         -2.2806e-01,  4.5175e-01,  1.2293e+00, -1.4437e+00,  8.4738e-01,\n",
            "         -8.7419e-01,  7.6511e-01, -2.4370e-01,  8.4341e-01, -8.4143e-01,\n",
            "         -2.0008e-01, -8.0523e-01,  6.9975e-01,  9.7540e-02,  1.3566e+00,\n",
            "         -6.9780e-02,  9.9242e-01, -1.0409e+00,  1.8707e+00, -7.3151e-01,\n",
            "         -9.2074e-01,  6.9700e-01, -1.3637e+00,  5.3379e-01, -1.9976e+00,\n",
            "          3.2352e-01, -5.3999e-01,  3.8370e-01,  1.5695e-01,  4.4637e-03,\n",
            "          1.1257e+00,  5.1451e-01,  1.0989e+00, -1.3787e+00,  2.1094e-01,\n",
            "         -8.3332e-01, -9.8499e-01, -2.7440e-01,  1.2838e-01, -5.8584e-01,\n",
            "          3.0970e-01, -6.3880e-01,  7.5544e-01, -4.7980e-01, -1.4346e+00,\n",
            "         -3.1373e-01, -2.9685e-01,  2.3855e+00, -5.9635e-01, -1.2849e+00,\n",
            "          8.6141e-01, -3.1208e-01, -4.5290e-02,  1.6822e-01, -6.9363e-01,\n",
            "         -1.7618e+00, -1.3004e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00,  2.7961e-01, -9.1319e-01, -2.5752e-01, -9.5664e-01,\n",
            "         -1.8500e-01,  6.7256e-01, -8.9229e-02,  6.6831e-02,  3.1886e-01,\n",
            "         -5.8698e-01, -1.2758e-01, -3.5190e-01,  1.5404e-01,  4.3318e-01,\n",
            "         -4.9557e-01, -6.9917e-02, -2.3247e-01, -1.1999e+00,  6.8551e-01,\n",
            "          1.8102e-01, -8.9760e-02,  1.0053e+00, -1.6367e+00, -5.2553e-01,\n",
            "         -1.6622e-01, -1.9662e-01,  6.1414e-01, -1.6790e-01,  1.3570e+00,\n",
            "          2.5763e-01,  6.1664e-01,  1.9395e-01,  3.6657e-01, -9.5195e-01,\n",
            "          4.3465e-01, -4.5620e-01,  6.3805e-02,  7.3189e-01, -1.5967e-01,\n",
            "          1.1051e-01, -5.7984e-01, -4.7816e-01,  6.4952e-01,  1.5682e-01,\n",
            "          4.4678e-01,  1.1395e+00, -1.1693e+00, -3.9067e-01,  1.8896e-01,\n",
            "          8.5693e-01,  1.0939e+00,  1.2407e+00,  1.1994e-01, -7.6440e-01,\n",
            "         -1.1102e+00, -1.1869e+00,  6.5454e-02,  2.2757e+00, -1.8983e-01,\n",
            "         -1.6258e-01,  7.3536e-01, -9.4124e-01, -9.4334e-01,  5.2775e-02,\n",
            "         -1.1247e+00, -7.4441e-01, -4.5449e-02, -1.5899e+00, -2.6093e-01,\n",
            "          8.9131e-01,  7.1178e-01,  1.0075e+00, -5.7871e-02, -1.4047e+00,\n",
            "          2.3398e-01, -1.9084e+00, -3.7811e-01, -9.3279e-01,  3.1324e-01,\n",
            "         -3.2478e-01, -2.2914e-01, -1.9046e-02, -9.5177e-01, -7.6401e-01,\n",
            "          7.5197e-01,  4.0819e-01,  1.4400e+00, -1.2227e+00, -6.9360e-01,\n",
            "          1.0985e+00, -4.7766e-01,  1.2876e+00,  2.3431e+00, -2.1629e-01,\n",
            "          8.3300e-01,  1.9901e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.2227e+00, -8.8468e-01,  8.2877e-01, -1.6363e+00,\n",
            "         -4.8450e-01,  2.1808e+00, -1.1387e-01, -5.1349e-01, -2.2178e-01,\n",
            "         -1.3956e+00,  3.2407e+00, -8.1253e-01,  3.6333e-01,  1.7293e+00,\n",
            "         -4.3001e-01,  1.1648e+00, -1.7877e+00, -9.7951e-01,  1.5907e+00,\n",
            "         -9.4778e-01,  3.4815e-01, -4.5740e-01,  9.3620e-01, -4.8181e-01,\n",
            "          2.0724e+00,  3.2354e+00,  5.1593e-01,  3.5050e-01, -3.0774e-01,\n",
            "          1.9097e+00, -1.4646e-01,  9.9858e-01,  1.2037e+00,  3.2097e-01,\n",
            "         -1.1940e+00, -8.3090e-01,  4.8763e-01,  8.9874e-02, -1.0155e+00,\n",
            "          4.4987e-01,  2.0103e+00, -1.5128e-01,  1.2240e+00, -5.6797e-01,\n",
            "         -1.4763e-01, -6.9311e-01, -5.7205e-01,  3.2634e-01,  1.4080e+00,\n",
            "         -1.1041e-03,  3.1912e+00, -1.2705e+00, -5.9581e-02,  3.5198e-01,\n",
            "          2.0855e-01, -8.1497e-01, -1.1951e-01, -9.5970e-01,  8.4964e-02,\n",
            "          5.0693e-02,  5.1039e-01, -1.8460e+00,  2.2626e-03,  7.9258e-01,\n",
            "          4.0870e-01,  4.7801e-01, -1.1253e-01,  1.8714e-01, -8.4109e-01,\n",
            "         -5.8024e-01, -4.2964e-01, -5.2693e-02, -4.5345e-01, -2.6408e-01,\n",
            "          1.5317e+00, -6.4089e-01, -5.0340e-01,  1.5669e+00, -1.1851e+00,\n",
            "         -1.4128e+00, -8.6769e-01, -9.0028e-01, -9.0437e-01, -9.7809e-02,\n",
            "         -3.3260e-01, -2.8013e-01, -3.6028e-01,  6.8478e-01, -1.6207e+00,\n",
            "          1.4512e+00, -7.0903e-01,  5.9956e-01,  4.1014e-01, -5.9289e-01,\n",
            "          2.5146e-02, -7.5376e-01]])\n",
            "Adjacency Matrix Tensor:\n",
            " tensor([[1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 1., 1., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1., 1., 0.],\n",
            "        [0., 0., 0., 1., 1., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 1.]])\n",
            "Label Tensor: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kHz21PyIj-xc"
      },
      "id": "kHz21PyIj-xc"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assume combined_features and adj_matrix_with_loops are predefined\n",
        "# Example label for the sentence (e.g., 1 for positive sentiment, 0 for negative)\n",
        "sentence_label = 1\n",
        "\n",
        "# Convert feature and adjacency data to PyTorch tensors\n",
        "node_features = torch.tensor(combined_features, dtype=torch.float32)  # Node features as float tensor\n",
        "adj_matrix = torch.tensor(adj_matrix_with_loops, dtype=torch.float32)  # Adjacency matrix as float tensor\n",
        "label = torch.tensor([sentence_label], dtype=torch.long)  # Sentence label as long tensor for classification\n",
        "\n",
        "# Display the converted tensors to confirm their structure\n",
        "print(\"Node Features Tensor:\\n\", node_features)\n",
        "print(\"Adjacency Matrix Tensor:\\n\", adj_matrix)\n",
        "print(\"Label Tensor:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr2zW0AQeUcY",
        "outputId": "d98b8c07-81c8-44e8-e4fc-e89c12cbd855"
      },
      "id": "pr2zW0AQeUcY",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features Tensor:\n",
            " tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0466e+00, -6.3125e-01, -5.6540e-01,  2.7119e+00,\n",
            "         -1.0801e+00, -4.9187e-02, -7.9210e-01,  6.1598e-02, -6.1989e-01,\n",
            "          1.6166e+00,  1.4493e+00,  1.3127e+00, -6.7903e-01, -1.2306e+00,\n",
            "         -7.8954e-01, -1.0821e+00, -8.0464e-01,  1.6262e+00, -8.7126e-01,\n",
            "          4.0537e-01, -1.1336e+00, -3.7326e-01, -6.6686e-01, -1.6324e+00,\n",
            "          1.8673e+00, -2.4132e-01,  1.0853e+00,  8.6994e-02, -9.4281e-02,\n",
            "          6.0370e-01,  1.2150e+00, -1.2031e+00,  9.7626e-01, -2.0013e+00,\n",
            "         -6.6515e-02,  9.5435e-01,  2.6909e-01, -7.1802e-01,  2.5988e-01,\n",
            "          3.8899e+00, -8.0076e-02,  1.2519e+00, -1.3616e+00,  9.7839e-01,\n",
            "         -9.9233e-01, -8.0711e-02, -4.8829e-01,  2.3329e+00,  1.2838e+00,\n",
            "          9.2897e-02, -9.7115e-01, -3.6849e-01,  5.5837e-01,  5.8041e-01,\n",
            "          8.4477e-01,  7.8262e-01, -1.1075e+00,  9.8441e-01, -9.1765e-01,\n",
            "         -1.3616e-01, -4.8621e-01,  3.5007e-01, -4.7011e-02, -4.8244e-01,\n",
            "          3.0105e-01, -2.5362e-01, -1.7427e-01, -9.8540e-01, -6.6830e-01,\n",
            "         -1.2227e+00,  4.2672e-02,  9.4135e-01,  3.6272e-01, -3.7862e-01,\n",
            "         -3.6757e-01, -1.0384e+00,  7.6296e-02, -8.0155e-01,  8.9522e-01,\n",
            "          1.2948e-01, -2.0569e-01, -1.2827e+00, -8.4814e-01, -1.5223e+00,\n",
            "          2.1006e+00,  2.6072e-01,  9.9722e-01, -5.7762e-01,  1.3976e-01,\n",
            "         -4.4610e-01, -1.7746e-01, -2.7478e-01,  2.4672e-02, -2.6006e-01,\n",
            "         -4.9468e-02, -2.7745e-01],\n",
            "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00, -2.7830e-01, -2.7215e-01, -5.6672e-01,  3.4908e-01,\n",
            "          7.5255e-02,  5.1887e-01,  2.7152e+00,  7.8152e-01, -3.2749e-02,\n",
            "         -3.9680e-01,  8.1274e-01, -5.9804e-01, -4.7571e-01, -2.1330e-01,\n",
            "         -5.7066e-01,  3.9056e-01,  7.6172e-01, -7.9528e-01, -6.4704e-02,\n",
            "          4.0032e-01,  8.5267e-01, -2.1082e-03,  8.4364e-01, -1.6039e-01,\n",
            "         -4.7434e-03,  1.6328e-01,  6.0075e-01, -3.8785e-01,  1.2790e+00,\n",
            "          1.1133e-01, -5.3935e-01, -1.3522e+00, -8.3967e-01, -7.1345e-01,\n",
            "         -4.0346e-01, -1.0706e+00, -3.3082e-01, -6.6639e-01,  4.3903e-01,\n",
            "          4.0830e-01, -1.7506e+00,  8.1692e-01,  3.7977e-01,  1.6986e+00,\n",
            "         -7.0632e-01,  6.5648e-02,  1.1635e+00,  2.4660e+00,  1.7616e-02,\n",
            "          3.6189e-01, -7.5553e-01, -5.2672e-01, -2.1548e-01, -4.7824e-01,\n",
            "          9.8119e-01, -2.1651e-01,  1.2059e+00,  8.5868e-02, -2.8156e-02,\n",
            "         -2.0614e-01, -8.5902e-02, -5.1092e-01, -1.1201e+00,  3.2073e-01,\n",
            "          3.4802e-01, -9.2195e-01, -6.7805e-01,  7.4530e-03,  2.3161e-01,\n",
            "         -1.2330e+00,  6.2556e-01, -3.5276e-01,  1.0257e+00, -1.7551e+00,\n",
            "          3.2152e-01, -8.6125e-01, -4.0212e-01, -1.2579e+00,  7.9955e-01,\n",
            "         -1.9703e+00,  1.3464e-01,  8.3858e-01, -1.5642e+00, -5.9977e-01,\n",
            "          2.4085e-01, -1.6819e-01, -2.7681e-01, -1.4349e-01, -6.4928e-01,\n",
            "          4.7014e-01, -5.3835e-02,  5.7702e-01,  1.7431e+00, -1.4278e-01,\n",
            "          3.7690e-01,  1.7775e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.2089e-01,  9.6797e-01,  5.7002e-01, -8.6154e-01,\n",
            "         -1.9329e-01, -9.4405e-01, -1.4777e+00, -8.4947e-01,  4.4858e-01,\n",
            "          4.7003e-01,  4.5836e-01,  5.4919e-01, -8.8852e-01, -8.4964e-01,\n",
            "         -5.4730e-01, -3.6774e-01, -6.1365e-01, -7.4975e-01,  2.6458e-01,\n",
            "          7.7260e-01, -3.0023e-01,  8.9829e-01, -5.4176e-01, -1.1204e+00,\n",
            "         -5.3247e-01, -5.5761e-01, -8.6956e-01,  6.0031e-01,  7.8129e-02,\n",
            "         -7.3234e-01,  2.2336e-02, -3.8570e-02,  4.4550e-02,  9.9966e-01,\n",
            "         -8.5930e-01,  4.0388e-02, -6.1088e-01, -3.6136e-01, -2.9084e-01,\n",
            "         -1.3732e+00,  1.8201e+00,  1.3455e+00,  8.5533e-01, -6.5450e-01,\n",
            "          3.9870e-01, -8.8511e-01,  1.0991e+00, -1.1684e+00,  6.6168e-01,\n",
            "          1.5853e+00, -1.1566e+00, -4.9199e-01, -1.1498e+00, -5.2573e-01,\n",
            "          8.4986e-01, -2.5071e-01,  1.2003e+00,  2.7931e-01, -5.7366e-01,\n",
            "          1.1968e-02, -1.1808e-01,  4.3878e-01, -7.3117e-01,  1.9779e+00,\n",
            "          1.0692e+00, -1.2015e-02, -2.3207e-01, -1.3452e+00,  1.0574e+00,\n",
            "          9.8239e-01,  6.4031e-01,  8.8139e-01,  2.2901e-01, -8.5030e-01,\n",
            "         -3.5013e-01,  4.8815e-01, -8.3415e-01,  1.4968e-01, -5.3698e-01,\n",
            "         -1.0501e+00, -6.4595e-02, -1.7392e-01, -4.6627e-01,  1.4185e+00,\n",
            "          1.4131e+00,  1.9416e-01,  5.4089e-01, -9.6600e-01, -8.6368e-01,\n",
            "         -4.1394e-01,  5.6898e-01, -6.6868e-01, -3.8467e-01,  8.4789e-01,\n",
            "          1.2682e+00,  1.4624e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  8.8363e-02,  6.8898e-01,  3.7087e-01, -9.9162e-01,\n",
            "          3.2143e-01, -1.0220e+00, -6.4119e-01, -1.8897e+00,  4.0421e-01,\n",
            "          8.5032e-01,  4.4113e-01, -8.0841e-01, -7.3891e-01, -8.9062e-01,\n",
            "          5.0821e-01, -4.8691e-01, -1.4978e+00,  1.3634e+00,  2.6757e-01,\n",
            "          6.5181e-01, -2.4141e-01, -6.2450e-01, -6.6104e-01, -5.8367e-01,\n",
            "          1.1655e+00, -5.1826e-01,  1.2206e+00, -6.9513e-01,  7.5096e-01,\n",
            "          3.2317e-01,  8.2030e-02, -3.7350e-01,  6.5122e-01, -3.7203e-01,\n",
            "         -4.9859e-01,  6.5745e-01,  9.1281e-01,  4.1721e-01,  8.8202e-01,\n",
            "         -1.0903e+00, -3.6643e-01,  2.7609e-01, -9.0137e-01, -1.2292e+00,\n",
            "          3.6166e-01,  5.3718e-01,  7.6810e-01, -4.2810e-01, -5.6994e-02,\n",
            "         -1.0932e+00, -1.1633e+00,  6.8617e-01,  8.0640e-01,  3.2472e-01,\n",
            "         -1.7163e+00, -2.2577e-01, -7.1181e-01, -5.4241e-01,  1.1366e+00,\n",
            "         -1.0909e+00, -1.1210e+00, -1.2996e+00, -3.4786e-01,  1.1831e+00,\n",
            "         -3.3749e-02,  1.0548e+00,  1.2703e+00, -6.4579e-01, -3.3163e-01,\n",
            "          1.5699e-01,  1.3454e+00, -1.9342e-01, -8.6390e-02, -2.4086e-02,\n",
            "         -8.2817e-01,  1.0954e+00, -4.9836e-01,  1.1588e+00,  5.7212e-02,\n",
            "         -5.6857e-01,  4.0182e-01,  5.2155e-01, -3.0204e-01,  7.9412e-01,\n",
            "          1.4695e+00,  1.4173e-01, -3.0142e-01,  1.4899e+00, -6.8626e-01,\n",
            "          8.5028e-01,  7.0571e-02, -2.8820e-01, -2.7844e-01,  5.8317e-01,\n",
            "         -8.9058e-01,  1.4469e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  1.0608e+00,  4.6738e-02, -8.4570e-01,  1.0409e+00,\n",
            "          8.8011e-02,  4.2018e-01, -9.2696e-01, -4.4648e-01,  4.9820e-01,\n",
            "          1.4338e+00,  2.8563e-01,  5.3766e-01,  1.3688e+00,  5.5870e-01,\n",
            "         -1.0579e+00, -1.2381e+00,  5.8224e-01,  1.2231e+00,  3.7855e-02,\n",
            "         -6.7967e-01, -1.4820e+00, -1.3614e-01, -9.6220e-01, -1.3247e+00,\n",
            "          1.0489e+00, -4.2452e-01,  2.9633e-01, -6.6203e-02,  9.4682e-01,\n",
            "          9.7303e-01,  7.3550e-01, -1.5553e-01,  1.5346e-01, -5.5300e-01,\n",
            "          1.2249e+00,  1.0922e-01,  1.5794e+00,  5.9524e-01, -1.0335e+00,\n",
            "         -2.2806e-01,  4.5175e-01,  1.2293e+00, -1.4437e+00,  8.4738e-01,\n",
            "         -8.7419e-01,  7.6511e-01, -2.4370e-01,  8.4341e-01, -8.4143e-01,\n",
            "         -2.0008e-01, -8.0523e-01,  6.9975e-01,  9.7540e-02,  1.3566e+00,\n",
            "         -6.9780e-02,  9.9242e-01, -1.0409e+00,  1.8707e+00, -7.3151e-01,\n",
            "         -9.2074e-01,  6.9700e-01, -1.3637e+00,  5.3379e-01, -1.9976e+00,\n",
            "          3.2352e-01, -5.3999e-01,  3.8370e-01,  1.5695e-01,  4.4637e-03,\n",
            "          1.1257e+00,  5.1451e-01,  1.0989e+00, -1.3787e+00,  2.1094e-01,\n",
            "         -8.3332e-01, -9.8499e-01, -2.7440e-01,  1.2838e-01, -5.8584e-01,\n",
            "          3.0970e-01, -6.3880e-01,  7.5544e-01, -4.7980e-01, -1.4346e+00,\n",
            "         -3.1373e-01, -2.9685e-01,  2.3855e+00, -5.9635e-01, -1.2849e+00,\n",
            "          8.6141e-01, -3.1208e-01, -4.5290e-02,  1.6822e-01, -6.9363e-01,\n",
            "         -1.7618e+00, -1.3004e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
            "          0.0000e+00,  2.7961e-01, -9.1319e-01, -2.5752e-01, -9.5664e-01,\n",
            "         -1.8500e-01,  6.7256e-01, -8.9229e-02,  6.6831e-02,  3.1886e-01,\n",
            "         -5.8698e-01, -1.2758e-01, -3.5190e-01,  1.5404e-01,  4.3318e-01,\n",
            "         -4.9557e-01, -6.9917e-02, -2.3247e-01, -1.1999e+00,  6.8551e-01,\n",
            "          1.8102e-01, -8.9760e-02,  1.0053e+00, -1.6367e+00, -5.2553e-01,\n",
            "         -1.6622e-01, -1.9662e-01,  6.1414e-01, -1.6790e-01,  1.3570e+00,\n",
            "          2.5763e-01,  6.1664e-01,  1.9395e-01,  3.6657e-01, -9.5195e-01,\n",
            "          4.3465e-01, -4.5620e-01,  6.3805e-02,  7.3189e-01, -1.5967e-01,\n",
            "          1.1051e-01, -5.7984e-01, -4.7816e-01,  6.4952e-01,  1.5682e-01,\n",
            "          4.4678e-01,  1.1395e+00, -1.1693e+00, -3.9067e-01,  1.8896e-01,\n",
            "          8.5693e-01,  1.0939e+00,  1.2407e+00,  1.1994e-01, -7.6440e-01,\n",
            "         -1.1102e+00, -1.1869e+00,  6.5454e-02,  2.2757e+00, -1.8983e-01,\n",
            "         -1.6258e-01,  7.3536e-01, -9.4124e-01, -9.4334e-01,  5.2775e-02,\n",
            "         -1.1247e+00, -7.4441e-01, -4.5449e-02, -1.5899e+00, -2.6093e-01,\n",
            "          8.9131e-01,  7.1178e-01,  1.0075e+00, -5.7871e-02, -1.4047e+00,\n",
            "          2.3398e-01, -1.9084e+00, -3.7811e-01, -9.3279e-01,  3.1324e-01,\n",
            "         -3.2478e-01, -2.2914e-01, -1.9046e-02, -9.5177e-01, -7.6401e-01,\n",
            "          7.5197e-01,  4.0819e-01,  1.4400e+00, -1.2227e+00, -6.9360e-01,\n",
            "          1.0985e+00, -4.7766e-01,  1.2876e+00,  2.3431e+00, -2.1629e-01,\n",
            "          8.3300e-01,  1.9901e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "          0.0000e+00, -1.2227e+00, -8.8468e-01,  8.2877e-01, -1.6363e+00,\n",
            "         -4.8450e-01,  2.1808e+00, -1.1387e-01, -5.1349e-01, -2.2178e-01,\n",
            "         -1.3956e+00,  3.2407e+00, -8.1253e-01,  3.6333e-01,  1.7293e+00,\n",
            "         -4.3001e-01,  1.1648e+00, -1.7877e+00, -9.7951e-01,  1.5907e+00,\n",
            "         -9.4778e-01,  3.4815e-01, -4.5740e-01,  9.3620e-01, -4.8181e-01,\n",
            "          2.0724e+00,  3.2354e+00,  5.1593e-01,  3.5050e-01, -3.0774e-01,\n",
            "          1.9097e+00, -1.4646e-01,  9.9858e-01,  1.2037e+00,  3.2097e-01,\n",
            "         -1.1940e+00, -8.3090e-01,  4.8763e-01,  8.9874e-02, -1.0155e+00,\n",
            "          4.4987e-01,  2.0103e+00, -1.5128e-01,  1.2240e+00, -5.6797e-01,\n",
            "         -1.4763e-01, -6.9311e-01, -5.7205e-01,  3.2634e-01,  1.4080e+00,\n",
            "         -1.1041e-03,  3.1912e+00, -1.2705e+00, -5.9581e-02,  3.5198e-01,\n",
            "          2.0855e-01, -8.1497e-01, -1.1951e-01, -9.5970e-01,  8.4964e-02,\n",
            "          5.0693e-02,  5.1039e-01, -1.8460e+00,  2.2626e-03,  7.9258e-01,\n",
            "          4.0870e-01,  4.7801e-01, -1.1253e-01,  1.8714e-01, -8.4109e-01,\n",
            "         -5.8024e-01, -4.2964e-01, -5.2693e-02, -4.5345e-01, -2.6408e-01,\n",
            "          1.5317e+00, -6.4089e-01, -5.0340e-01,  1.5669e+00, -1.1851e+00,\n",
            "         -1.4128e+00, -8.6769e-01, -9.0028e-01, -9.0437e-01, -9.7809e-02,\n",
            "         -3.3260e-01, -2.8013e-01, -3.6028e-01,  6.8478e-01, -1.6207e+00,\n",
            "          1.4512e+00, -7.0903e-01,  5.9956e-01,  4.1014e-01, -5.9289e-01,\n",
            "          2.5146e-02, -7.5376e-01]])\n",
            "Adjacency Matrix Tensor:\n",
            " tensor([[1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 1., 1., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1., 1., 0.],\n",
            "        [0., 0., 0., 1., 1., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 1.]])\n",
            "Label Tensor: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Training the GCN Model\n",
        "\n",
        "We’ll use a simple training loop to train the GCN model for the sentence classification task. The model will minimize the cross-entropy loss between predicted and true labels.\n"
      ],
      "metadata": {
        "id": "fp_nZPEs2edQ"
      },
      "id": "fp_nZPEs2edQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Training Loop\n"
      ],
      "metadata": {
        "id": "OqQ7BjPs2eel"
      },
      "id": "OqQ7BjPs2eel"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model, loss function, and optimizer\n",
        "input_dim = node_features.shape[1]  # Dimensionality of input node features\n",
        "hidden_dim = 8  # Hidden layer dimension for GCN\n",
        "output_dim = 2  # Output dimension (e.g., binary classification: positive and negative)\n",
        "\n",
        "# Initialize the GCN model with input, hidden, and output dimensions\n",
        "model = GCNModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define loss function for classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer for training the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Number of epochs for training\n",
        "epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: compute node-level outputs\n",
        "    node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "    # Aggregate node outputs by taking the mean to get a graph-level embedding\n",
        "    sentence_output = node_outputs.mean(dim=0, keepdim=True)  # Mean aggregation for sentence-level representation\n",
        "\n",
        "    # Compute the loss using the aggregated sentence embedding\n",
        "    loss = criterion(sentence_output, label)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()        # Backpropagate to compute gradients\n",
        "    optimizer.step()        # Update model parameters\n",
        "\n",
        "    # Print the loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO68pYq4fOp_",
        "outputId": "641c4762-9f84-4777-f8dc-f09459010917"
      },
      "id": "HO68pYq4fOp_",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6529\n",
            "Epoch 2/20, Loss: 0.5152\n",
            "Epoch 3/20, Loss: 0.4485\n",
            "Epoch 4/20, Loss: 0.3842\n",
            "Epoch 5/20, Loss: 0.3227\n",
            "Epoch 6/20, Loss: 0.2654\n",
            "Epoch 7/20, Loss: 0.2130\n",
            "Epoch 8/20, Loss: 0.1671\n",
            "Epoch 9/20, Loss: 0.1285\n",
            "Epoch 10/20, Loss: 0.0970\n",
            "Epoch 11/20, Loss: 0.0721\n",
            "Epoch 12/20, Loss: 0.0530\n",
            "Epoch 13/20, Loss: 0.0388\n",
            "Epoch 14/20, Loss: 0.0283\n",
            "Epoch 15/20, Loss: 0.0207\n",
            "Epoch 16/20, Loss: 0.0152\n",
            "Epoch 17/20, Loss: 0.0112\n",
            "Epoch 18/20, Loss: 0.0084\n",
            "Epoch 19/20, Loss: 0.0064\n",
            "Epoch 20/20, Loss: 0.0049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Evaluating the Model\n",
        "\n",
        "After training, we can evaluate the model by using it to predict labels on unseen sentences.\n"
      ],
      "metadata": {
        "id": "1tk69LXg2ega"
      },
      "id": "1tk69LXg2ega"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "bN8j38Wx2eji"
      },
      "id": "bN8j38Wx2eji"
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    \"\"\"\n",
        "    Evaluation phase where we pass the data through the model without calculating gradients.\n",
        "    This reduces memory usage and speeds up the forward pass.\n",
        "    \"\"\"\n",
        "    # Forward pass to get node-level outputs from the model\n",
        "    node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "    # Aggregate node outputs by taking the mean to get the sentence-level embedding\n",
        "    sentence_output = node_outputs.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Determine the predicted label by taking the class with the highest score\n",
        "    _, predicted = torch.max(sentence_output, dim=1)\n",
        "\n",
        "    # Display the predicted label and the true label\n",
        "    print(\"Predicted Label:\", predicted.item())\n",
        "    print(\"True Label:\", label.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Wt8wZLfXME",
        "outputId": "cb876e99-edbd-4c95-e281-5af4f1d0d80e"
      },
      "id": "l_Wt8wZLfXME",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: 1\n",
            "True Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6. Code Walkthrough: Full Pipeline\n",
        "\n",
        "Here’s the complete code that includes the GCN model, data preparation, training, and evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "gFwhLHQP2enP"
      },
      "id": "gFwhLHQP2enP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### Explanation:\n",
        "1. **GCNLayer**:\n",
        "   - Defines a single GCN layer that applies a linear transformation, aggregates neighbor information, normalizes by node degrees, and applies ReLU.\n",
        "\n",
        "2. **GCNModel**:\n",
        "   - Combines two `GCNLayer` instances, allowing the model to propagate information across two hops.\n",
        "\n",
        "3. **Data Preparation**:\n",
        "   - Converts `combined_features` (node features), `adj_matrix_with_loops` (adjacency matrix with self-loops), and `sentence_label` (target label) into PyTorch tensors.\n",
        "\n",
        "4. **Training Loop**:\n",
        "   - Runs for 20 epochs.\n",
        "   - For each epoch, performs a forward pass through the model, computes the mean of node embeddings for sentence classification, calculates the loss, backpropagates, and updates the model parameters.\n",
        "   \n",
        "5. **Evaluation**:\n",
        "   - Runs a forward pass without gradient computation, aggregates node embeddings, and uses `torch.max` to predict the class label.\n",
        "   - Prints the predicted and true labels for comparison.\n",
        "\n",
        "This code is designed for binary sentence classification using GCNs, where the sentence representation is derived by aggregating node-level features."
      ],
      "metadata": {
        "id": "FpSM3uRvfixR"
      },
      "id": "FpSM3uRvfixR"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Define GCN Layer\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Initialize a single GCN layer with a linear transformation.\n",
        "\n",
        "        Parameters:\n",
        "        - in_features (int): Number of input features for each node.\n",
        "        - out_features (int): Number of output features for each node.\n",
        "        \"\"\"\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        \"\"\"\n",
        "        Forward pass for the GCN layer.\n",
        "\n",
        "        Parameters:\n",
        "        - node_features (torch.Tensor): Node features matrix.\n",
        "        - adj_matrix (torch.Tensor): Adjacency matrix of the graph.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Activated, normalized node features.\n",
        "        \"\"\"\n",
        "        # Linear transformation on node features\n",
        "        transformed_features = self.linear(node_features)\n",
        "\n",
        "        # Aggregation step: multiply with adjacency matrix to aggregate neighbors\n",
        "        aggregated_features = torch.matmul(adj_matrix, transformed_features)\n",
        "\n",
        "        # Normalization by node degrees\n",
        "        degree_matrix = adj_matrix.sum(dim=1, keepdim=True)\n",
        "        normalized_features = aggregated_features / degree_matrix\n",
        "\n",
        "        # Apply ReLU non-linearity\n",
        "        return F.relu(normalized_features)\n",
        "\n",
        "# Define GCN Model\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        A two-layer GCN model for node feature transformation.\n",
        "\n",
        "        Parameters:\n",
        "        - input_dim (int): Dimensionality of input features.\n",
        "        - hidden_dim (int): Dimensionality of hidden layer.\n",
        "        - output_dim (int): Dimensionality of output layer (e.g., number of classes).\n",
        "        \"\"\"\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
        "        self.gcn2 = GCNLayer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        \"\"\"\n",
        "        Forward pass through the two-layer GCN model.\n",
        "\n",
        "        Parameters:\n",
        "        - node_features (torch.Tensor): Node features matrix.\n",
        "        - adj_matrix (torch.Tensor): Adjacency matrix of the graph.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Node-level outputs.\n",
        "        \"\"\"\n",
        "        # First GCN layer\n",
        "        x = self.gcn1(node_features, adj_matrix)\n",
        "        # Second GCN layer\n",
        "        x = self.gcn2(x, adj_matrix)\n",
        "        return x\n",
        "\n",
        "# Prepare node features, adjacency matrix, and label\n",
        "node_features = torch.tensor(combined_features, dtype=torch.float32)  # Node features tensor\n",
        "adj_matrix = torch.tensor(adj_matrix_with_loops, dtype=torch.float32)  # Adjacency matrix tensor\n",
        "label = torch.tensor([sentence_label], dtype=torch.long)  # Label tensor\n",
        "\n",
        "# Model setup\n",
        "input_dim = node_features.shape[1]  # Input feature dimension\n",
        "hidden_dim = 8                      # Hidden layer dimension\n",
        "output_dim = 2                      # Output dimension (e.g., binary classification)\n",
        "model = GCNModel(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()    # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Optimizer\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "    # Mean aggregation for sentence-level embedding\n",
        "    sentence_output = node_outputs.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(sentence_output, label)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    # Forward pass on evaluation\n",
        "    node_outputs = model(node_features, adj_matrix)\n",
        "    sentence_output = node_outputs.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Get the predicted label by finding the index with the max value\n",
        "    _, predicted = torch.max(sentence_output, dim=1)\n",
        "\n",
        "    # Print the predicted and true labels\n",
        "    print(\"Predicted Label:\", predicted.item())\n",
        "    print(\"True Label:\", label.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiICTi3WfcEk",
        "outputId": "edd0c872-0442-464d-91c0-af57ab51d0b8"
      },
      "id": "kiICTi3WfcEk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.7942\n",
            "Epoch 2/20, Loss: 0.7038\n",
            "Epoch 3/20, Loss: 0.6931\n",
            "Epoch 4/20, Loss: 0.6931\n",
            "Epoch 5/20, Loss: 0.6931\n",
            "Epoch 6/20, Loss: 0.6931\n",
            "Epoch 7/20, Loss: 0.6931\n",
            "Epoch 8/20, Loss: 0.6931\n",
            "Epoch 9/20, Loss: 0.6931\n",
            "Epoch 10/20, Loss: 0.6931\n",
            "Epoch 11/20, Loss: 0.6931\n",
            "Epoch 12/20, Loss: 0.6931\n",
            "Epoch 13/20, Loss: 0.6931\n",
            "Epoch 14/20, Loss: 0.6931\n",
            "Epoch 15/20, Loss: 0.6931\n",
            "Epoch 16/20, Loss: 0.6931\n",
            "Epoch 17/20, Loss: 0.6931\n",
            "Epoch 18/20, Loss: 0.6931\n",
            "Epoch 19/20, Loss: 0.6931\n",
            "Epoch 20/20, Loss: 0.6931\n",
            "Predicted Label: 0\n",
            "True Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ob: wrong prediction\n",
        "\n",
        "change epoch=40, it will tune more, and the prediction will be correct"
      ],
      "metadata": {
        "id": "2JjQNvaUft65"
      },
      "id": "2JjQNvaUft65"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary and Key Takeaways\n",
        "\n",
        "- **GCN Model Architecture**: Our model includes two GCN layers for sentence classification, allowing each node to aggregate information from its neighbors.\n",
        "- **Sentence-Level Classification**: We use mean aggregation of node features to obtain a sentence representation for classification.\n",
        "- **Training and Evaluation**: The model is trained using cross-entropy loss, and the output can be evaluated on unseen sentences.\n",
        "\n",
        "With this GCN model setup, you’re now equipped to perform sentence classification using graph-based techniques. In the next section, we’ll explore ways to experiment with model configurations, including using different aggregation methods and tuning hyperparameters to optimize performance."
      ],
      "metadata": {
        "id": "7JY2sY1i2erm"
      },
      "id": "7JY2sY1i2erm"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PPmNOFtC2evz"
      },
      "id": "PPmNOFtC2evz"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hsnesx-Z2eyN"
      },
      "id": "Hsnesx-Z2eyN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}