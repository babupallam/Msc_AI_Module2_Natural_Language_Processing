{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvKpGEEbeW26hlMiZrrzGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L10-Graph%20Convolutional%20Networks/Supporting_Documents/library_networkx/library_networkx_note__02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for neural network operations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "3aHY0L-w3tS6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 31: Graph Data as Input for Neural Networks\n",
        " To work with graphs in neural networks, we often need to convert graph data into matrices.\n"
      ],
      "metadata": {
        "id": "Gcy219FR0Yj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rmb_pgFP0Yj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510726ff-114d-4626-df2d-aaac757e5cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency Matrix:\n",
            "   (0, 1)\t1\n",
            "  (0, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 2)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 3)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 2)\t1\n",
            "Shape of Adjacency Matrix: (4, 4)\n",
            "Adjacency Matrix: after applying todense()\n",
            " [[0 1 0 1]\n",
            " [1 0 1 0]\n",
            " [0 1 0 1]\n",
            " [1 0 1 0]]\n",
            "Shape of Adjacency Matrix: (4, 4)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a sample graph G with 4 nodes\n",
        "G = nx.Graph()\n",
        "G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0)])\n",
        "\n",
        "# Convert the graph's adjacency matrix into a numpy array\n",
        "adj_matrix = nx.adjacency_matrix(G);\n",
        "print(\"Adjacency Matrix:\\n\", adj_matrix)\n",
        "print(\"Shape of Adjacency Matrix:\", adj_matrix.shape)\n",
        "\n",
        "adj_matrix = adj_matrix.todense()\n",
        "print(\"Adjacency Matrix: after applying todense()\\n\", adj_matrix)\n",
        "print(\"Shape of Adjacency Matrix:\", adj_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 32: Node Features in Graphs\n",
        " Node features are essential for training graph neural networks. Let's create some features for each node.\n"
      ],
      "metadata": {
        "id": "KlJ_tj-f0YlF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TwJdNbkz0YlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540c9f4b-b377-4ece-d2b3-66c9a06b7156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features:\n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]\n",
            " [7 8]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define node features as a simple numpy array (4 nodes, each with 2 features)\n",
        "node_features = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "print(\"Node Features:\\n\", node_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 33: Converting Node Features to Torch Tensors\n",
        " Neural networks in PyTorch use tensors, so we need to convert node features and adjacency matrices to tensors.\n"
      ],
      "metadata": {
        "id": "jgXRXiiL0YnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6Bva97o20YnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8351dd63-66f9-46de-8ccf-2ed4afd85160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features Tensor:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Adjacency Matrix Tensor:\n",
            " tensor([[0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Convert node features to a tensor\n",
        "node_features_tensor = torch.tensor(node_features, dtype=torch.float32)\n",
        "\n",
        "# Convert adjacency matrix to tensor\n",
        "adj_matrix_tensor = torch.tensor(adj_matrix, dtype=torch.float32)\n",
        "\n",
        "print(\"Node Features Tensor:\\n\", node_features_tensor)\n",
        "print(\"Adjacency Matrix Tensor:\\n\", adj_matrix_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 34: Building a Simple Graph Neural Network Layer\n",
        "Let's define a simple neural network layer that can perform operations on graph data.\n"
      ],
      "metadata": {
        "id": "nk2iF_fq0YoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0dw39bc70YoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649480e8-f3d7-4a21-d9b1-f33c90ed3fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple GNN Layer created.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple GNN layer class\n",
        "class SimpleGNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Graph Neural Network (GNN) layer that applies a linear transformation to aggregated node features from neighboring nodes in a graph.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Initialize the GNN layer with the input and output feature dimensions.\n",
        "\n",
        "        Parameters:\n",
        "        - in_features (int): The number of features each input node has.\n",
        "        - out_features (int): The number of features each output node will have after transformation.\n",
        "        \"\"\"\n",
        "        super(SimpleGNNLayer, self).__init__()\n",
        "\n",
        "        # Define a linear layer to apply transformations to the node features\n",
        "        # This layer will map from `in_features` to `out_features`.\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        print(\"Inside the forward function\")\n",
        "        \"\"\"\n",
        "        Forward pass of the GNN layer, which performs aggregation of neighboring node features followed by a linear transformation.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): Node feature matrix with shape (num_nodes, in_features).\n",
        "          Each row corresponds to the features of a node.\n",
        "        - adj (Tensor): Adjacency matrix with shape (num_nodes, num_nodes).\n",
        "          The matrix indicates connections between nodes in the graph.\n",
        "\n",
        "        Returns:\n",
        "        - out (Tensor): The transformed node feature matrix with shape (num_nodes, out_features).\n",
        "        \"\"\"\n",
        "        # Perform aggregation of neighbor features by multiplying adjacency matrix with the feature matrix.\n",
        "        # This effectively sums the features of neighboring nodes for each node.\n",
        "        out = torch.matmul(adj, x)\n",
        "        print(\"Aggregated Features:\\n\", out)\n",
        "        # Apply the linear transformation to the aggregated features.\n",
        "        # This step adjusts the features to the desired output dimension.\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Initialize a GNN layer with 2 input features and 4 output features.\n",
        "# This means each node's initial feature vector has 2 values, and after the layer, each will have 6 values.\n",
        "gnn_layer = SimpleGNNLayer(in_features=2, out_features=6)\n",
        "print(\"Simple GNN Layer created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 35: Forward Pass through GNN Layer\n",
        "We'll pass the node features and adjacency matrix through the GNN layer.\n"
      ],
      "metadata": {
        "id": "AFnwEFQA0Ypg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m8LjGsmw0Ypg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9a6c4b-edaa-43c6-9989-bd6988007813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features Tensor:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Adjacency Matrix Tensor:\n",
            " tensor([[0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.]])\n",
            "Inside the forward function\n",
            "Aggregated Features:\n",
            " tensor([[10., 12.],\n",
            "        [ 6.,  8.],\n",
            "        [10., 12.],\n",
            "        [ 6.,  8.]])\n",
            "Output after GNN layer:\n",
            " tensor([[12.0235, -3.1484,  1.5169,  9.4490,  3.8257, -3.0649],\n",
            "        [ 7.5461, -1.5947,  0.9551,  6.1475,  2.5892, -2.5219],\n",
            "        [12.0235, -3.1484,  1.5169,  9.4490,  3.8257, -3.0649],\n",
            "        [ 7.5461, -1.5947,  0.9551,  6.1475,  2.5892, -2.5219]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Node Features Tensor:\\n\", node_features_tensor)\n",
        "print(\"Adjacency Matrix Tensor:\\n\", adj_matrix_tensor)\n",
        "# Perform a forward pass\n",
        "output = gnn_layer(node_features_tensor, adj_matrix_tensor)\n",
        "print(\"Output after GNN layer:\\n\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation of how does it work?**\n",
        "\n",
        "\n",
        "Certainly! Here’s an enhanced explanation, incorporating details on the creation of weights and biases, the role of neurons, and additional observations about the operations.\n",
        "\n",
        "#### Inputs\n",
        "- **Node Features Tensor**: Each row represents a node, and each column represents a feature.\n",
        "  ```python\n",
        "  tensor([[1., 2.],\n",
        "          [3., 4.],\n",
        "          [5., 6.],\n",
        "          [7., 8.]])\n",
        "  ```\n",
        "  - We have 4 nodes, each with 2 features (e.g., values `[1., 2.]` for node 0). These initial features represent each node’s information, which could include properties or attributes relevant to the graph.\n",
        "\n",
        "- **Adjacency Matrix Tensor**: Represents the connections between nodes.\n",
        "  ```python\n",
        "  tensor([[0., 1., 0., 1.],\n",
        "          [1., 0., 1., 0.],\n",
        "          [0., 1., 0., 1.],\n",
        "          [1., 0., 1., 0.]])\n",
        "  ```\n",
        "  - This matrix indicates which nodes are connected:\n",
        "    - For example, node 0 is connected to nodes 1 and 3, and node 1 is connected to nodes 0 and 2.\n",
        "  - Since this is an undirected and unweighted adjacency matrix, it is symmetric, with values of `0` (no connection) or `1` (connection).\n",
        "\n",
        "#### Step-by-Step Explanation Inside the `forward` Function\n",
        "\n",
        "##### Step 1: Aggregating Features from Neighbors\n",
        "The `forward` function first aggregates features from neighboring nodes using matrix multiplication with the adjacency matrix:\n",
        "```python\n",
        "out = torch.matmul(adj, x)\n",
        "```\n",
        "Here’s how this aggregation works:\n",
        "\n",
        "1. **Matrix Multiplication (`torch.matmul(adj, x)`)**:\n",
        "   - Each row in the resulting matrix is a sum of the features of the neighboring nodes for that row’s node.\n",
        "   - For example:\n",
        "     - **Node 0** has neighbors 1 and 3. Its aggregated features are the sum of node 1’s and node 3’s features:\n",
        "       \\[\n",
        "       [3, 4] + [7, 8] = [10, 12]\n",
        "       \\]\n",
        "     - **Node 1** has neighbors 0 and 2:\n",
        "       \\[\n",
        "       [1, 2] + [5, 6] = [6, 8]\n",
        "       \\]\n",
        "     - Nodes 2 and 3 follow the same pattern, producing aggregated features:\n",
        "       ```python\n",
        "       tensor([[10., 12.],\n",
        "               [ 6.,  8.],\n",
        "               [10., 12.],\n",
        "               [ 6.,  8.]])\n",
        "       ```\n",
        "\n",
        "   - This aggregation step captures information from each node’s immediate neighbors, essential for graph-based learning tasks where node influence is based on connectivity.\n",
        "\n",
        "##### Step 2: Applying Linear Transformation\n",
        "Next, we pass the aggregated features through a linear layer:\n",
        "```python\n",
        "out = self.linear(out)\n",
        "```\n",
        "This linear transformation adjusts the feature dimensions from 2 to a higher dimensionality, with learned weights and biases defining the transformation. Let’s examine how this transformation works in detail.\n",
        "\n",
        "1. **Weight and Bias Creation**:\n",
        "   - The linear layer (`self.linear`) is defined in the initialization method (`__init__`):\n",
        "     ```python\n",
        "     self.linear = nn.Linear(in_features, out_features)\n",
        "     ```\n",
        "   - Here:\n",
        "     - **`in_features = 2`**: The input feature dimension, corresponding to the 2 features per node.\n",
        "     - **`out_features = 4`**: The output feature dimension, set to 4, meaning we want to expand each node’s feature representation to 4 dimensions.\n",
        "\n",
        "   - **Neurons and Parameters**:\n",
        "     - The `nn.Linear(in_features, out_features)` layer contains **4 neurons** (one for each output feature dimension), each with **2 weights** (for the two input features) and **1 bias**.\n",
        "     - Therefore, this layer has:\n",
        "       - **Weights**: A matrix of shape `(out_features, in_features)`, i.e., `(4, 2)`, resulting in \\( 4 \\times 2 = 8 \\) weight parameters.\n",
        "       - **Bias**: A vector of shape `(out_features,)`, i.e., `(4,)`, resulting in 4 bias parameters.\n",
        "     - **Total Parameters**: 8 weights + 4 biases = 12 parameters in total for this layer, which are learned during training.\n",
        "\n",
        "2. **Linear Transformation Operation**:\n",
        "   - The linear transformation in `self.linear(out)` performs the following steps:\n",
        "     - **Matrix Multiplication with Weights**:\n",
        "       - The aggregated feature matrix of shape `(4, 2)` (4 nodes, each with 2 features) is multiplied by the weight matrix of shape `(4, 2)` (transposed to `(2, 4)` for compatibility).\n",
        "       - This multiplication yields an output matrix of shape `(4, 4)`, representing each node’s transformed features in a higher-dimensional feature space.\n",
        "     - **Adding Bias**:\n",
        "       - A bias vector of shape `(4,)` is added to each row of the transformed matrix, shifting the values.\n",
        "\n",
        "   - The final output after the linear transformation is:\n",
        "     ```python\n",
        "     tensor([[12.0235, -3.1484,  1.5169,  9.4490],\n",
        "             [ 7.5461, -1.5947,  0.9551,  6.1475],\n",
        "             [12.0235, -3.1484,  1.5169,  9.4490],\n",
        "             [ 7.5461, -1.5947,  0.9551,  6.1475]],\n",
        "            grad_fn=<AddmmBackward0>)\n",
        "     ```\n",
        "   - **Explanation of Output**:\n",
        "     - Each row corresponds to a node, and each column corresponds to one of the 4 new feature dimensions produced by the linear transformation.\n",
        "     - These values represent the updated feature embeddings for each node, learned from the aggregation of neighbor features and then transformed by the linear layer.\n",
        "\n",
        "#### Summary of Each Step:\n",
        "1. **Neighbor Aggregation**: Summed the features of each node’s neighbors using matrix multiplication with the adjacency matrix.\n",
        "2. **Weight and Bias Creation**: The `nn.Linear` layer internally creates a weight matrix `(4, 2)` and bias vector `(4)`, resulting in a total of 12 parameters (8 weights and 4 biases) to learn.\n",
        "3. **Linear Transformation**: The linear layer (with 4 neurons) adjusts the feature dimensions from 2 to 4 by applying matrix multiplication with learned weights, followed by adding bias, yielding the final transformed node features in the output tensor.\n",
        "\n",
        "This process enables the GNN layer to learn complex, high-dimensional representations of each node by aggregating and transforming information from neighboring nodes, capturing structural and feature-based information from the graph structure.\n"
      ],
      "metadata": {
        "id": "duIpxdHQDM1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 36: Stacking GNN Layers\n",
        " GNNs often use multiple stacked layers to learn more complex patterns.\n"
      ],
      "metadata": {
        "id": "bqEcSuKA0Yq5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6lEGYjzG0Yq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544e5914-a7e7-467a-ff21-11a8360ff374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside the forward function\n",
            "Aggregated Features:\n",
            " tensor([[10., 12.],\n",
            "        [ 6.,  8.],\n",
            "        [10., 12.],\n",
            "        [ 6.,  8.]])\n",
            "Inside the forward function\n",
            "Aggregated Features:\n",
            " tensor([[ 0.0000, 10.7863, 12.1743,  9.5225,  0.0000,  6.3963],\n",
            "        [ 0.0000, 15.5433, 19.3663, 15.4491,  0.0000,  8.3092],\n",
            "        [ 0.0000, 10.7863, 12.1743,  9.5225,  0.0000,  6.3963],\n",
            "        [ 0.0000, 15.5433, 19.3663, 15.4491,  0.0000,  8.3092]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "Output after Two-Layer GNN:\n",
            " tensor([[-0.4022],\n",
            "        [-1.0680],\n",
            "        [-0.4022],\n",
            "        [-1.0680]], grad_fn=<AddmmBackward0>)\n",
            "Shape of outputs: torch.Size([4, 2])\n",
            "Shape of node_labels: torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class TwoLayerGNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoLayerGNN, self).__init__()\n",
        "        self.layer1 = SimpleGNNLayer(in_features=2, out_features=6)\n",
        "        self.layer2 = SimpleGNNLayer(in_features=6, out_features=1)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.layer1(x, adj)\n",
        "        x = torch.relu(x)  # Apply activation function\n",
        "        x = self.layer2(x, adj)\n",
        "        return x\n",
        "\n",
        "# Initialize and test a two-layer GNN\n",
        "two_layer_gnn = TwoLayerGNN()\n",
        "output = two_layer_gnn(node_features_tensor, adj_matrix_tensor)\n",
        "print(\"Output after Two-Layer GNN:\\n\", output)\n",
        "\n",
        "\n",
        "print(\"Shape of outputs:\", outputs.shape)\n",
        "print(\"Shape of node_labels:\", node_labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 37: Graph Classification Task Setup\n",
        " For a supervised learning task, we need labels for nodes or graphs.\n"
      ],
      "metadata": {
        "id": "KmvHTqry0Ys6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xOaI0H1k0Ys7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a8e014-4a63-4b2d-d279-ec78bff279fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Labels: tensor([0., 1., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a target label for each node (e.g., 0 or 1)\n",
        "node_labels = torch.tensor([0, 1, 0, 1], dtype=torch.float32)\n",
        "print(\"Node Labels:\", node_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 38: Defining Loss and Optimizer for the GNN Model\n",
        "We'll define a loss function and an optimizer to train the GNN on the node classification task.\n"
      ],
      "metadata": {
        "id": "-ahQYARA0YvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does it do?\n",
        "\n",
        "- Binary Cross-Entropy: This is a common loss function used for binary classification problems where you're trying to predict one of two classes (e.g., 0 or 1, True or False). It calculates the difference between the predicted probabilities and the actual labels (0 or 1).\n",
        "\n",
        "- With Logits: The \"logits\" part is important. It means that this loss function expects the raw output from your neural network (before applying a sigmoid or softmax function to get probabilities). It internally applies a sigmoid activation to the output before calculating the loss, which is numerically more stable."
      ],
      "metadata": {
        "id": "d14bXkOpESzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0Vl8hDIr0YvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb138a09-3e32-485a-c937-7bff28f88805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function defined.\n",
            "Loss function: BCEWithLogitsLoss()\n",
            "<generator object Module.parameters at 0x78a2e64b2ea0>\n",
            "Optimizer defined.\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Use binary cross-entropy loss for binary node classification\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(\"Loss function defined.\")\n",
        "print(\"Loss function:\", loss_fn)\n",
        "\n",
        "\n",
        "print(two_layer_gnn.parameters()) #<generator object Module.parameters at 0x78a2e64b3450>\n",
        "\n",
        "\n",
        "# Use Adam optimizer for training\n",
        "optimizer = optim.Adam(two_layer_gnn.parameters(), lr=0.01)\n",
        "print(\"Optimizer defined.\")\n",
        "print(\"Optimizer:\", optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 39: Training the GNN Model (Single Epoch)\n",
        "We’ll perform one forward and backward pass to illustrate the training process.\n"
      ],
      "metadata": {
        "id": "Wb5JHP0e0Yx2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ol3zUhrg0Yx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd89da18-9317-46ef-b4ee-ee15750e11d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside the forward function\n",
            "Aggregated Features:\n",
            " tensor([[10., 12.],\n",
            "        [ 6.,  8.],\n",
            "        [10., 12.],\n",
            "        [ 6.,  8.]])\n",
            "Inside the forward function\n",
            "Aggregated Features:\n",
            " tensor([[ 0.0000, 11.5861, 11.3093,  8.7095,  0.0000,  7.1753],\n",
            "        [ 0.0000, 16.7894, 18.0296, 14.1840,  0.0000,  9.5250],\n",
            "        [ 0.0000, 11.5861, 11.3093,  8.7095,  0.0000,  7.1753],\n",
            "        [ 0.0000, 16.7894, 18.0296, 14.1840,  0.0000,  9.5250]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "Outputs after forward pass:\n",
            " tensor([1.1054, 1.2325, 1.1054, 1.2325], grad_fn=<SqueezeBackward0>)\n",
            "Shape of outputs: torch.Size([4])\n",
            "Shape of node_labels: torch.Size([4])\n",
            "Loss after one training step: 0.8236291408538818\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "outputs = two_layer_gnn(node_features_tensor, adj_matrix_tensor).squeeze()\n",
        "print(\"Outputs after forward pass:\\n\", outputs)\n",
        "print(\"Shape of outputs:\", outputs.shape)\n",
        "print(\"Shape of node_labels:\", node_labels.shape)\n",
        "\n",
        "# Loss calculation\n",
        "loss = loss_fn(outputs, node_labels)  # This function has been defined in Demonstration 38.\n",
        "\n",
        "# Backward pass and optimization\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Loss after one training step:\", loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detailed Explanation\n",
        "\n",
        "##### 1. Forward Pass\n",
        "```python\n",
        "outputs = two_layer_gnn(node_features_tensor, adj_matrix_tensor).squeeze()\n",
        "```\n",
        "- **Purpose**: This line performs a forward pass through the `two_layer_gnn` model, generating the predicted outputs for each node based on the input features and adjacency matrix.\n",
        "- **Explanation**:\n",
        "  - `two_layer_gnn`: This is an instance of a neural network model defined earlier. It processes the input graph data in a series of graph neural network (GNN) layers.\n",
        "  - `node_features_tensor`: A tensor that represents the initial features for each node in the graph, typically of shape `(num_nodes, in_features)`, where each row corresponds to a node and each column to a feature.\n",
        "  - `adj_matrix_tensor`: The adjacency matrix representing connections between nodes. It is used by GNN layers to aggregate features from neighboring nodes.\n",
        "  - **`.squeeze()`**: This method removes any singleton dimensions from the tensor. For example, if `outputs` initially has a shape of `(4, 1)`, `.squeeze()` would make it `(4,)`. This is useful to ensure `outputs` has the same shape as `node_labels` for loss calculation.\n",
        "- **Output**:\n",
        "  - After this line, `outputs` contains the predictions from the GNN model for each node. The shape of `outputs` depends on the last layer of `two_layer_gnn`. If the last layer has a single output per node, `outputs` will have shape `(num_nodes,)` after `.squeeze()`.\n",
        "\n",
        "```python\n",
        "print(\"Outputs after forward pass:\\n\", outputs)\n",
        "print(\"Shape of outputs:\", outputs.shape)\n",
        "print(\"Shape of node_labels:\", node_labels.shape)\n",
        "```\n",
        "- **Purpose**: These lines print the predicted outputs after the forward pass and display the shapes of `outputs` and `node_labels`.\n",
        "- **Explanation**:\n",
        "  - `outputs.shape`: Displays the shape of the predicted outputs, which should match the shape of `node_labels` for compatibility in the loss calculation.\n",
        "  - `node_labels.shape`: Displays the shape of the ground-truth labels to verify that it matches the output shape.\n",
        "\n",
        "#3### 2. Loss Calculation\n",
        "```python\n",
        "loss = loss_fn(outputs, node_labels)  # This function has been defined in Demonstration 38.\n",
        "```\n",
        "- **Purpose**: This line calculates the loss, which measures the difference between the model's predictions (`outputs`) and the actual labels (`node_labels`).\n",
        "- **Explanation**:\n",
        "  - `loss_fn`: This is a loss function defined in an earlier demonstration. Based on your previous information, it’s likely `torch.nn.BCEWithLogitsLoss()` for binary classification.\n",
        "  - **How `BCEWithLogitsLoss` Works**:\n",
        "    - `BCEWithLogitsLoss` combines a sigmoid activation with binary cross-entropy loss, making it suitable for binary classification tasks. It takes raw logits (unactivated outputs) as input and applies a sigmoid internally to map predictions to probabilities between 0 and 1.\n",
        "    - It then computes the binary cross-entropy loss between the predicted probabilities and the true labels.\n",
        "  - **Expected Shape Match**:\n",
        "    - `outputs` and `node_labels` should have the same shape for this function to work correctly. If `outputs` has a shape of `(4,)`, `node_labels` should also have a shape of `(4,)`.\n",
        "\n",
        "##### 3. Backward Pass and Optimization\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "- **Purpose**: This line resets the gradients of all model parameters to zero before performing backpropagation.\n",
        "- **Explanation**:\n",
        "  - During backpropagation, gradients are accumulated for each parameter. If we don’t reset them to zero, gradients from previous iterations will be added to the current ones, which is typically not desired.\n",
        "  - `optimizer.zero_grad()` clears these gradients to ensure only the current step’s gradients are used.\n",
        "\n",
        "```python\n",
        "loss.backward()\n",
        "```\n",
        "- **Purpose**: This line performs backpropagation, calculating the gradients of the loss with respect to each parameter in the model.\n",
        "- **Explanation**:\n",
        "  - `loss.backward()` computes the derivative of the loss with respect to each parameter (weight and bias) in `two_layer_gnn` using the chain rule.\n",
        "  - These gradients are stored in each parameter's `.grad` attribute, allowing the optimizer to update the parameters in the next step.\n",
        "\n",
        "```python\n",
        "optimizer.step()\n",
        "```\n",
        "- **Purpose**: This line updates the model’s parameters based on the gradients calculated during backpropagation.\n",
        "- **Explanation**:\n",
        "  - The `optimizer` (e.g., `torch.optim.Adam` or `torch.optim.SGD`) takes the gradients from each parameter’s `.grad` attribute and adjusts the parameters according to the chosen optimization algorithm.\n",
        "  - This step reduces the loss by moving the model parameters in the direction that minimizes it, effectively “learning” from the data.\n",
        "\n",
        "##### 4. Printing the Loss\n",
        "```python\n",
        "print(\"Loss after one training step:\", loss.item())\n",
        "```\n",
        "- **Purpose**: This line outputs the loss value after one training step.\n",
        "- **Explanation**:\n",
        "  - `loss.item()`: Converts the PyTorch tensor containing the loss to a Python scalar, making it easier to display.\n",
        "  - This printed value helps monitor the model’s progress during training. A decreasing loss over successive training steps typically indicates that the model is learning.\n"
      ],
      "metadata": {
        "id": "AYlRb1aaYc4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 40: Evaluating Model Predictions\n",
        "After training, we need to evaluate the model predictions.\n"
      ],
      "metadata": {
        "id": "3bdDrSay0Yz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- outputs:\n",
        "  - This represents the raw output from your two-layer GNN model. These outputs are often called logits and are not probabilities. They can range from negative infinity to positive infinity.\n",
        "\n",
        "- torch.sigmoid(outputs):\n",
        "  - This applies the sigmoid activation function to the outputs. The sigmoid function squashes the logits into a range between 0 and 1, effectively converting them into probabilities.\n"
      ],
      "metadata": {
        "id": "A6oDgQs7Y88r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "lQGXoNuW0Yz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e18be5-3328-490b-8d39-a2ba6f6c147d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions after one epoch: tensor([1, 1, 1, 1], dtype=torch.int32)\n",
            "Actual labels: tensor([0, 1, 0, 1], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get predictions (output logits > 0.5 as class 1, else class 0)\n",
        "predictions = (torch.sigmoid(outputs) > 0.5).int()\n",
        "print(\"Predictions after one epoch:\", predictions)\n",
        "print(\"Actual labels:\", node_labels.int())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 41: Implementing Graph Convolutional Layer\n",
        "\n",
        "\n",
        "- In traditional Convolutional Neural Networks (CNNs), convolution is applied to grid-like data (e.g., images) using filters or kernels that slide over the data to capture local patterns.\n",
        "- However, graphs are non-Euclidean structures, meaning they don’t have a regular grid structure.\n",
        "- Instead, they have nodes (vertices) and edges, and each node can have a varying number of neighbors.\n",
        "- Therefore, convolution on graphs is defined differently, often involving an \"aggregation\" of information from a node's neighbors.\n",
        "\n",
        "### Graph Convolution: Key Concepts\n",
        "\n",
        "- The **graph convolution** operation involves updating each node’s feature representation by aggregating information from its neighbors, which allows the network to learn features based on the graph structure.\n",
        "- The process essentially smooths or propagates information across the graph, enabling nodes to gain insights from their neighbors.\n",
        "\n",
        "### Steps in Graph Convolution\n",
        "\n",
        "A graph convolution operation generally includes the following steps:\n",
        "\n",
        "1. **Self-Loop Addition**:\n",
        "   - In many graph convolution implementations, each node is connected to itself by adding self-loops in the adjacency matrix.\n",
        "   - This allows each node to retain its own information in addition to the aggregated information from its neighbors.\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Normalize the adjacency matrix to ensure that the feature aggregation doesn’t lead to exploding or vanishing values.\n",
        "   - A common approach is to use **symmetric normalization**, which divides each row by the square root of the degree of the corresponding node.\n",
        "   - The normalized adjacency matrix (often denoted as \\( \\hat{A} \\)) helps balance the contributions from nodes with different numbers of neighbors, making the learning process more stable.\n",
        "\n",
        "3. **Feature Aggregation**:\n",
        "   - Multiply the normalized adjacency matrix with the node feature matrix. This step aggregates the features of neighboring nodes for each node.\n",
        "   - The result is a new feature representation for each node that incorporates information from its local neighborhood.\n",
        "\n",
        "4. **Linear Transformation**:\n",
        "   - Apply a learnable linear transformation (via a fully connected layer) to the aggregated features.\n",
        "   - This step projects the features to a new dimension and allows the network to learn meaningful representations of the nodes.\n",
        "\n",
        "### Graph Convolution Implementation in Code\n"
      ],
      "metadata": {
        "id": "-HqaN6Mp0Y12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "kfeeqTaA0Y13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd26b46-d0d3-4955-866a-e81ea220e4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Convolutional Layer created.\n"
          ]
        }
      ],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Step 1: Self-Loop Addition\n",
        "        # Add self-loops to the adjacency matrix (by adding the identity matrix).\n",
        "        # This ensures each node's representation includes its own features.\n",
        "        adj_hat = adj + torch.eye(adj.size(0))  # Adds self-loops to the adjacency matrix\n",
        "\n",
        "        # Step 2: Normalization\n",
        "        # Calculate D^(-1/2) (inverse square root of the degree matrix).\n",
        "        # This will be used to normalize the adjacency matrix.\n",
        "        deg_inv_sqrt = torch.diag(torch.pow(adj_hat.sum(1), -0.5))\n",
        "\n",
        "        # Normalize the adjacency matrix using symmetric normalization:\n",
        "        # adj_norm = D^(-1/2) * adj_hat * D^(-1/2)\n",
        "        #     - `D^(-1/2)` is the inverse square root of the degree matrix (`deg_inv_sqrt`).\n",
        "        #     - `A_hat` is the adjacency matrix with self-loops (`adj_hat`).\n",
        "        #     - This formula ensures that the normalized adjacency matrix has rows and columns that sum to approximately 1, effectively scaling the aggregated features.\n",
        "        #  the @ symbol represents the matrix multiplication operator\n",
        "        adj_norm = deg_inv_sqrt @ adj_hat @ deg_inv_sqrt\n",
        "\n",
        "        # Step 3: Feature Aggregation\n",
        "        # Multiply the normalized adjacency matrix with the feature matrix.\n",
        "        # This aggregates features from each node's neighbors.\n",
        "        out = adj_norm @ x  # (num_nodes, num_nodes) @ (num_nodes, in_features) -> (num_nodes, in_features)\n",
        "\n",
        "        # Step 4: Linear Transformation\n",
        "        # Apply the linear layer to transform the aggregated features to the output dimension.\n",
        "        out = self.linear(out)  # (num_nodes, in_features) -> (num_nodes, out_features)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Initialize a graph convolutional layer\n",
        "# This layer will take in node features with 2 dimensions and output features with 4 dimensions.\n",
        "gcn_layer = GraphConvolution(in_features=2, out_features=4)\n",
        "print(\"Graph Convolutional Layer created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Detailed Explanation of the Code\n",
        "\n",
        "#### 1. Self-Loop Addition\n",
        "```python\n",
        "adj_hat = adj + torch.eye(adj.size(0))\n",
        "```\n",
        "- `adj + torch.eye(adj.size(0))` adds the identity matrix to the adjacency matrix `adj`, effectively adding self-loops. This means each node will aggregate its own features along with those of its neighbors.\n",
        "- `adj_hat` is the adjacency matrix with self-loops, represented as \\( \\hat{A} \\).\n",
        "\n",
        "#### 2. Normalization\n",
        "```python\n",
        "deg_inv_sqrt = torch.diag(torch.pow(adj_hat.sum(1), -0.5))\n",
        "adj_norm = deg_inv_sqrt @ adj_hat @ deg_inv_sqrt\n",
        "```\n",
        "- `adj_hat.sum(1)`: Computes the degree of each node (i.e., the number of edges for each node).\n",
        "- `torch.pow(..., -0.5)`: Takes the inverse square root of each degree. This gives us \\( D^{-1/2} \\), where \\( D \\) is the degree matrix.\n",
        "- `deg_inv_sqrt`: Creates a diagonal matrix with these values.\n",
        "- `adj_norm = deg_inv_sqrt @ adj_hat @ deg_inv_sqrt`: Performs symmetric normalization on `adj_hat`, yielding \\( D^{-1/2} \\hat{A} D^{-1/2} \\). This normalized adjacency matrix is used to stabilize the training process by balancing the contributions from nodes with varying degrees.\n",
        "\n",
        "#### 3. Feature Aggregation\n",
        "```python\n",
        "out = adj_norm @ x\n",
        "```\n",
        "- `adj_norm @ x`: Multiplies the normalized adjacency matrix with the feature matrix `x`.\n",
        "  - This operation aggregates the features of neighboring nodes for each node based on the structure of `adj_norm`.\n",
        "  - For each node, this sums up the features from its neighbors (as defined by `adj_norm`), giving a new feature representation that incorporates information from the neighborhood.\n",
        "\n",
        "#### 4. Linear Transformation\n",
        "```python\n",
        "out = self.linear(out)\n",
        "```\n",
        "- `self.linear(out)`: Applies a learnable linear transformation to the aggregated features.\n",
        "  - This step maps the aggregated features to a new dimensional space (`out_features`), allowing the model to learn complex representations based on the graph structure.\n",
        "  - The linear transformation has a weight matrix and a bias vector, both of which are learned during training. The weights adjust how much influence each feature has, and the bias shifts the overall feature values.\n",
        "\n",
        "### Summary of the Graph Convolution Operation\n",
        "1. **Self-Loop Addition**: Adds self-loops to the adjacency matrix to retain each node’s own features during aggregation.\n",
        "2. **Normalization**: Uses the degree matrix to normalize the adjacency matrix, ensuring that the contribution from each node's neighbors is balanced.\n",
        "3. **Feature Aggregation**: Aggregates features from neighboring nodes based on the normalized adjacency matrix, resulting in a new feature representation for each node that includes information from its local neighborhood.\n",
        "4. **Linear Transformation**: Applies a learnable linear transformation to the aggregated features, projecting them into a new feature space that can be used in subsequent layers or for prediction.\n",
        "\n",
        "This `GraphConvolution` layer is a key building block in Graph Convolutional Networks (GCNs), enabling each node to learn from its neighbors and capture local graph structure, ultimately allowing for effective learning on graph data."
      ],
      "metadata": {
        "id": "WXZIPVHIaTfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 42: Forward Pass through Graph Convolutional Layer\n",
        " Perform a forward pass through the GCN layer using our node features and adjacency matrix.\n"
      ],
      "metadata": {
        "id": "Nm80fyhK0Y36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "yFzJvboi0Y37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2c9ec5-3214-4c91-9cc0-66c6f66dd2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Adjacency Matrix:\n",
            " tensor([[0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.]])\n",
            "Output after GCN layer:\n",
            " tensor([[ 3.1062, -0.8937,  2.5748,  2.0371],\n",
            "        [ 2.6560, -0.8154,  2.2127,  1.6671],\n",
            "        [ 4.0065, -1.0504,  3.2990,  2.7770],\n",
            "        [ 3.5563, -0.9721,  2.9369,  2.4071]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Node Features:\\n\", node_features_tensor)\n",
        "print(\"Adjacency Matrix:\\n\", adj_matrix_tensor)\n",
        "# Pass node features and adjacency matrix through the GCN layer\n",
        "output_gcn = gcn_layer(node_features_tensor, adj_matrix_tensor)\n",
        "print(\"Output after GCN layer:\\n\", output_gcn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last output:\n",
        "\n",
        "- It is the output feature matrix after the GCN layer has processed the node features and adjacency matrix.\n",
        "- Each row represents a node, and each column represents one of the new features after the graph convolution.\n",
        "- In this case, each node now has 4 features instead of the original 2, because the GCN layer was designed to output 4 features per node (out_features=4).\n",
        "- Explanation of Each Step Leading to This Output:\n",
        "  - The GCN layer aggregates features from each node's neighbors based on the adjacency matrix.\n",
        "  - It then applies a linear transformation to map the aggregated features to a new feature space of dimension 4.\n",
        "- Each element in this output tensor is the result of:\n",
        "  - Neighbor Aggregation: Aggregating information from the neighbors of each node.\n",
        "  - Transformation: Applying a learned weight matrix and bias to transform the aggregated features."
      ],
      "metadata": {
        "id": "ASDKTwDjdFIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 43: Building a Two-Layer GCN Model\n",
        " Let's stack two GCN layers to build a simple GCN model.\n"
      ],
      "metadata": {
        "id": "kxKdPlTf0Y6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "GZUY3lZr0Y6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec9aadb-66c0-45c6-ae0d-7ba4eeebb49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-Layer GCN created.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class TwoLayerGCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoLayerGCN, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(in_features=2, out_features=4)\n",
        "        self.gcn2 = GraphConvolution(in_features=4, out_features=1)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.gcn1(x, adj)\n",
        "        x = torch.relu(x)  # Apply activation function\n",
        "        x = self.gcn2(x, adj)\n",
        "        return x\n",
        "\n",
        "# Initialize the two-layer GCN\n",
        "two_layer_gcn = TwoLayerGCN()\n",
        "print(\"Two-Layer GCN created.\")\n",
        "\n",
        "# out_features =1 is for binary classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 44: Training the Two-Layer GCN\n",
        " We’ll train this GCN model for a few epochs to see how it learns on a simple classification task.\n"
      ],
      "metadata": {
        "id": "ldo_Pgzf4Bgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "I_B-bIi54Bgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598891a2-b53b-4b4c-de39-1dc55e59b82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7165266275405884\n",
            "Epoch 2, Loss: 0.7076399326324463\n",
            "Epoch 3, Loss: 0.7002691030502319\n",
            "Epoch 4, Loss: 0.6944364309310913\n",
            "Epoch 5, Loss: 0.6901254653930664\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(two_layer_gcn.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop for 5 epochs\n",
        "for epoch in range(5):\n",
        "    outputs = two_layer_gcn(node_features_tensor, adj_matrix_tensor).squeeze()\n",
        "    loss = loss_fn(outputs, node_labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 45: Evaluating GCN Model Predictions\n",
        " After training, let's check the model's predictions on the node classification task.\n"
      ],
      "metadata": {
        "id": "MchjcsHo4Bog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "PpLipB2q4Boh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ced47e-27d2-4b04-c015-9fe753d6521f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Adjacency Matrix:\n",
            " tensor([[0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.]])\n",
            "Final Outputs after training:\n",
            " tensor([[0.6885],\n",
            "        [0.7246],\n",
            "        [0.7607],\n",
            "        [0.7968]], grad_fn=<AddmmBackward0>)\n",
            "Final Outputs after squeezing:\n",
            " tensor([0.6885, 0.7246, 0.7607, 0.7968], grad_fn=<SqueezeBackward0>)\n",
            "Final Predictions: tensor([1, 1, 1, 1], dtype=torch.int32)\n",
            "Actual Labels: tensor([0, 1, 0, 1], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "print(\"Node Features:\\n\", node_features_tensor)\n",
        "print(\"Adjacency Matrix:\\n\", adj_matrix_tensor)\n",
        "# Get final predictions after training\n",
        "outputs = two_layer_gcn(node_features_tensor, adj_matrix_tensor);\n",
        "print(\"Final Outputs after training:\\n\", outputs)\n",
        "outputs = outputs.squeeze()\n",
        "print(\"Final Outputs after squeezing:\\n\", outputs)\n",
        "\n",
        "\n",
        "predictions = (torch.sigmoid(outputs) > 0.5).int()\n",
        "print(\"Final Predictions:\", predictions)\n",
        "print(\"Actual Labels:\", node_labels.int())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 46: Visualizing Node Embeddings\n",
        " Node embeddings are the learned representations from the GCN, useful for visualization and downstream tasks.\n"
      ],
      "metadata": {
        "id": "QSA7Bspa4BtD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "za4YNU2O4BtE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ad965d1b-448d-4f26-d3f3-923115f6fb71"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABiEElEQVR4nO3deViU1dsH8O8zyI4gyiqiqLjmgkIStmhFkZpmWZEbhEuZa1KplIpbYotKmSuuaaVlapaKFalpkiaKSynuQf4YQA0QVBDmvH/4MjkCOvPMDDPDfD/X9VwXc+ZZ7jOoc3tWSQghQERERGSBFKYOgIiIiEguJjJERERksZjIEBERkcViIkNEREQWi4kMERERWSwmMkRERGSxmMgQERGRxWIiQ0RERBaLiQwRERFZLCYyREYQEBCAV1991dRh6Gz37t2QJAkbN240+rOmTZsGSZK0OleSJEybNk39evXq1ZAkCRcvXjROcERkMZjIUK1X8aXn4OCAS5cuVXq/e/fuaNeunQki01737t0hSVKVR+vWrU0dHhGRydQxdQBENaWkpARz5szBggULTB2KLI0aNUJCQkKlcjc3NxNEY1qDBw/GK6+8Ant7e1OHQkQmxkSGrEZQUBCSkpIQFxeHhg0bmjocnbm5uWHQoEGmDsMs2NjYwMbGxtRhEJEZYNcSWY13330X5eXlmDNnzn3PLSsrw8yZM9G8eXPY29sjICAA7777LkpKSjTOE0Jg1qxZaNSoEZycnPD444/jzz//rPKe+fn5ePPNN+Hv7w97e3sEBgbigw8+gEqlMkj9gP/GnZw+fRqDBg2Cm5sbPD09MWXKFAghkJWVheeeew6urq7w8fHB3Llzq7xPeXk53n33Xfj4+MDZ2Rl9+vRBVlZWpfMOHDiAZ555Bm5ubnByckK3bt3w22+/VTpv3759ePDBB+Hg4IDmzZtj6dKlVT63pKQE48ePh6enJ+rWrYs+ffrgn3/+qXReVWNkAgIC8Oyzz2Lfvn3o0qULHBwc0KxZM3z++eeVrj927Bi6desGR0dHNGrUCLNmzcKqVasq3fPQoUOIiIiAh4cHHB0d0bRpUwwZMqTK2InINNgiQ1ajadOmiIqKQlJSEiZNmnTPVplhw4ZhzZo1ePHFF/HWW2/hwIEDSEhIwMmTJ7F582b1eVOnTsWsWbPQs2dP9OzZE4cPH8bTTz+N0tJSjftdv34d3bp1w6VLl/D666+jcePG2L9/P+Li4pCdnY3ExMT7xl9eXo7Lly9XKnd0dISzs7NGWWRkJNq0aYM5c+Zg27ZtmDVrFurXr4+lS5fiiSeewAcffIAvvvgCb7/9Nh588EE89thjGte///77kCQJEydORG5uLhITExEeHo709HQ4OjoCAH755Rf06NEDwcHBiI+Ph0KhwKpVq/DEE09g79696NKlCwDg+PHjePrpp+Hp6Ylp06ahrKwM8fHx8Pb2rvJzX7duHQYMGICuXbvil19+Qa9eve772VQ4e/YsXnzxRQwdOhTR0dFYuXIlXn31VQQHB+OBBx4AAFy6dAmPP/44JElCXFwcnJ2dsXz58krdVLm5ueq4J02ahHr16uHixYvYtGmT1vEQUQ0QRLXcqlWrBADxxx9/iHPnzok6deqIsWPHqt/v1q2beOCBB9Sv09PTBQAxbNgwjfu8/fbbAoD45ZdfhBBC5ObmCjs7O9GrVy+hUqnU57377rsCgIiOjlaXzZw5Uzg7O4vTp09r3HPSpEnCxsZGZGZm3rMO3bp1EwCqPF5//XX1efHx8QKAeO2119RlZWVlolGjRkKSJDFnzhx1+b///iscHR014ty1a5cAIPz8/ERhYaG6/OuvvxYAxCeffCKEEEKlUokWLVqIiIgIjbpfv35dNG3aVDz11FPqsr59+woHBwfx999/q8v++usvYWNjI+78J6jicx85cqRG3QcMGCAAiPj4eHVZxe/0woUL6rImTZoIAOLXX39Vl+Xm5gp7e3vx1ltvqcvGjBkjJEkSR44cUZdduXJF1K9fX+OemzdvVv+5ISLzxa4lsirNmjXD4MGDsWzZMmRnZ1d5zvbt2wEAsbGxGuVvvfUWAGDbtm0AgJ9//hmlpaUYM2aMxjTiN998s9I9v/nmGzz66KNwd3fH5cuX1Ud4eDjKy8vx66+/3jf2gIAA/PTTT5WOqp43bNgw9c82NjYICQmBEAJDhw5Vl9erVw+tWrXC+fPnK10fFRWFunXrql+/+OKL8PX1VX826enpOHPmDAYMGIArV66o61NcXIwnn3wSv/76K1QqFcrLy7Fz50707dsXjRs3Vt+vTZs2iIiI0Hhmxb3Hjh2rUV5V/arTtm1bPProo+rXnp6eleqYnJyMsLAwBAUFqcvq16+PgQMHatyrXr16AIAffvgBt27d0joGIqpZ7FoiqzN58mSsXbsWc+bMwSeffFLp/b///hsKhQKBgYEa5T4+PqhXrx7+/vtv9XkA0KJFC43zPD094e7urlF25swZHDt2DJ6enlXGlJube9+4nZ2dER4eft/zAGgkDcDtgcIODg7w8PCoVH7lypVK199dJ0mSEBgYqB4/cubMGQBAdHR0tTEUFBSgpKQEN27cqHQ/AGjVqpU6eQH++9ybN29e6Txt3V1vAHB3d8e///6r8ZywsLBK5939++7WrRv69euH6dOnY/78+ejevTv69u2LAQMGcLYUkRlhIkNWp1mzZhg0aBCWLVuGSZMmVXuetou1aUOlUuGpp57ChAkTqny/ZcuWBnsWgCpn9FQ3y0cIofP9KwYof/TRRxotG3dycXGpNDja2AxZx4qFAX///Xd8//332LlzJ4YMGYK5c+fi999/h4uLi77hEpEBMJEhqzR58mSsW7cOH3zwQaX3mjRpApVKhTNnzqBNmzbq8pycHOTn56NJkybq84DbrRPNmjVTn5eXl6fRAgAAzZs3R1FRkdYtKqZW0eJSQQiBs2fPokOHDgCgbjVxdXW9Z508PT3h6OhY6X4AkJGRofG64nM/d+6cRivM3efpq0mTJjh79myl8qrKAOChhx7CQw89hPfffx9ffvklBg4ciPXr12t03xGR6XCMDFml5s2bY9CgQVi6dCmUSqXGez179gSASjOJ5s2bBwDqWTTh4eGwtbXFggULNP7HX9UMpJdffhmpqanYuXNnpffy8/NRVlamT3UM7vPPP8e1a9fUrzdu3Ijs7Gz06NEDABAcHIzmzZvj448/RlFRUaXr8/LyANxuIYmIiMCWLVuQmZmpfv/kyZOVPouKe3/66aca5drM6NJFREQEUlNTkZ6eri67evUqvvjiC43z/v3330otORWtTzXd0kRE1WOLDFmt9957D2vXrkVGRoZ6ai4AdOzYEdHR0Vi2bBny8/PRrVs3HDx4EGvWrEHfvn3x+OOPA7jd2vD2228jISEBzz77LHr27IkjR45gx44dlcaivPPOO9i6dSueffZZ9XTg4uJiHD9+HBs3bsTFixcrXXO3goICrFu3rsr3DL1QXv369fHII48gJiYGOTk5SExMRGBgIIYPHw4AUCgUWL58OXr06IEHHngAMTEx8PPzw6VLl7Br1y64urri+++/BwBMnz4dycnJePTRRzFy5EiUlZVhwYIFeOCBB3Ds2DH1M4OCgtC/f38sWrQIBQUF6Nq1K1JSUqptKZFrwoQJWLduHZ566imMGTNGPf26cePGuHr1qrpLcc2aNVi0aBGef/55NG/eHNeuXUNSUhJcXV3VyS4RmQFTTpkiqgl3Tr++W3R0tACgMf1aCCFu3bolpk+fLpo2bSpsbW2Fv7+/iIuLEzdv3tQ4r7y8XEyfPl34+voKR0dH0b17d3HixAnRpEkTjWnNQghx7do1ERcXJwIDA4WdnZ3w8PAQXbt2FR9//LEoLS29Zx3uNf36zr/GFdOv8/LyKtXT2dm5yvveWfeK6ddfffWViIuLE15eXsLR0VH06tVLY/p0hSNHjogXXnhBNGjQQNjb24smTZqIl19+WaSkpGict2fPHhEcHCzs7OxEs2bNxJIlS9Sx3unGjRti7NixokGDBsLZ2Vn07t1bZGVlaT39ulevXlXWsVu3bpXifvTRR4W9vb1o1KiRSEhIEJ9++qkAIJRKpRBCiMOHD4v+/fuLxo0bC3t7e+Hl5SWeffZZcejQoUrPICLTkYSQMQqOiKiWefPNN7F06VIUFRVx+wMiC8IxMkRkdW7cuKHx+sqVK1i7di0eeeQRJjFEFoZjZIjI6oSFhaF79+5o06YNcnJysGLFChQWFmLKlCmmDo2IdMREhoisTs+ePbFx40YsW7YMkiShc+fOWLFiRaU9p4jI/LFriYiszuzZs3H69Glcv34dxcXF2Lt3r8Ws8UNkSL/++it69+6Nhg0bQpIkbNmy5b7X7N69G507d4a9vT0CAwOxevVqo8d5L0xkiIiIrFRxcTE6duyIhQsXanX+hQsX0KtXLzz++ONIT0/Hm2++iWHDhlW5RlZN4awlIiIigiRJ2Lx5M/r27VvtORMnTsS2bdtw4sQJddkrr7yC/Px8JCcn10CUlVndGBmVSoX//e9/qFu3rkH30iEiotpHCIFr166hYcOGUCiM14lx8+ZNlJaW6n0fIUSl7zZ7e3uDbXSamppaqRs2IiJCp13qDc3qEpn//e9/8Pf3N3UYRERkQbKystCoUSOj3PvmzZto6OiCf1Gu971cXFwqbRsSHx+PadOm6X1vAFAqlfD29tYo8/b2RmFhIW7cuAFHR0eDPEcXVpfI1K1bF8DtP5Surq4mjoaIiMxZYWEh/P391d8dxlBaWop/UY7VNk3hpMfQ1etQ4dWiC5W+3wzVGmOurC6RqWhyc3V1ZSJDRERaqYmhCM62NnCS5C/IKIlyoNy4328+Pj7IycnRKMvJyYGrq6tJWmMAK0xkiIiIzJFUR4JCj4RJEsZPtsLCwrB9+3aNsp9++glhYWFGf3Z1mMgQERGZAclWAUmS37UkyZiEXFRUpLHD/IULF5Ceno769eujcePGiIuLw6VLl/D5558DAEaMGIHPPvsMEyZMwJAhQ/DLL7/g66+/xrZt22THrS+uI0NERGSlDh06hE6dOqFTp04AgNjYWHTq1AlTp04FAGRnZyMzM1N9ftOmTbFt2zb89NNP6NixI+bOnYvly5cjIiLCJPEDbJEhIiIyCwobCQqF/O4hhUr3a7t37457LSdX1aq93bt3x5EjR3R+lrEwkSEiIjIDkq0ESY9ERpKRyNQG7FoiIiIii8UWGSIiqjVOnbmGHb8okZtXAiEAjwZ2eKqbNzq0dTX71dwVdWq+a6k2YCJDREQW7/DxfHy2/BxOny+CjY2E8vLb4z5sbCRs2ZGNJv5OGPlqMzzcpYGJI60eu5bkYdcSERFZtJS9uXhz8lGcuXB7af6KJObOnzP/uY6JM09gy47/mSRGMh62yBARkcU6caoAM+aegkp17/MqJuZ8vOgMvD3tERZifi0zChsJChs9upbK2SJDRERkUVZ88fc9pw/fTZKApZ9fMGJE8kk2kt6HNWIiQ0REFulS9g38kf7vfVtj7iQEcPZCMU6eLjReYFSjmMgQEZFFStmbC4WMbzEbGwk/7ck1fEB6quha0uewRhwjQ0REFunKv6VQSBJU0G2PISEEruSXGikq+SSFnrOWamDTSHPERIaIiCySQiEBMr+7bfRIGIxFslFAstFj00gdE7ragl1LRERkkXw8HTSmWmtLggRvT3sjRESmwESGiIgs0lPdvKCQsVpvuUqgxxM+RohIPxwjIw8TGSIiskj13e3Q/WEPnbqJFAqgc/t6aNzIyYiRySNJknqcjKzDzLdgMBYmMkREZLGGDgyAvYNCq9lLkgTUsZEwckgz4wdGNYaJDBERWazGfk6YN709HBxsYGNT/Xk2CsDWVoGEye3QOrBuzQWoA8lGv+4l6R71r81MnsgsXLgQAQEBcHBwQGhoKA4ePFjtubdu3cKMGTPQvHlzODg4oGPHjkhOTq7BaImIyNy0a+2GFfM7I/wxL9SxkdQtLxU/KxTAY2GeWPZxJ4R2rm/qcKvFlX3lMen06w0bNiA2NhZLlixBaGgoEhMTERERgYyMDHh5eVU6f/LkyVi3bh2SkpLQunVr7Ny5E88//zz279+PTp06maAGRERkDvwbOmFKbBuMGRqIX/blIvdyCYQAGtS3wxOPeMKjPmcp1VaS0GWTCgMLDQ3Fgw8+iM8++wwAoFKp4O/vjzFjxmDSpEmVzm/YsCHee+89jBo1Sl3Wr18/ODo6Yt26dVo9s7CwEG5ubigoKICrq6thKkJERLVSTXxnVDxj9yNd4FJHfvtCUVkZuu87aHXfbyZrkSktLUVaWhri4uLUZQqFAuHh4UhNTa3ympKSEjg4OGiUOTo6Yt++fdU+p6SkBCUlJerXhYXcX4OIiMyP3iv7muEifzXBZGNkLl++jPLycnh7e2uUe3t7Q6lUVnlNREQE5s2bhzNnzkClUuGnn37Cpk2bkJ2dXe1zEhIS4Obmpj78/f0NWg8iIiIyHZMP9tXFJ598ghYtWqB169aws7PD6NGjERMTA8U95t3FxcWhoKBAfWRlZdVgxERERNrhgnjymCyR8fDwgI2NDXJycjTKc3Jy4ONT9YqLnp6e2LJlC4qLi/H333/j1KlTcHFxQbNm1a8JYG9vD1dXV42DiIjI3Oi1GJ6e3VKWzGSJjJ2dHYKDg5GSkqIuU6lUSElJQVhY2D2vdXBwgJ+fH8rKyvDtt9/iueeeM3a4RERERiVJCkgKPQ7JojpZDMak069jY2MRHR2NkJAQdOnSBYmJiSguLkZMTAwAICoqCn5+fkhISAAAHDhwAJcuXUJQUBAuXbqEadOmQaVSYcKECaasBhEREZmISROZyMhI5OXlYerUqVAqlQgKCkJycrJ6AHBmZqbG+JebN29i8uTJOH/+PFxcXNCzZ0+sXbsW9erVM1ENiIiIDIOzluQx6ToypsB1ZIiISFs1uY7M7z0ehYutHuvI3CrDQzv2Wt33m3V2qBEREVGtYNKuJSIiIrqNXUvyMJEhIiIyAxWzj/S53hpZZ62JiIioVmCLDBERkRlg15I8TGSIiIjMABMZedi1RERERBaLLTJERERmgC0y8jCRISIiMgO3Exl9Zi0xkSEiIiITkRQSFDZ6tMiUW2ciwzEyREREZLHYIkNERGQGOEZGHiYyREREZoAr+8pjnbUmIiKiWoEtMkRERGaAXUvyMJEhIiIyA0xk5GHXEhEREVkstsgQERGZAQ72lYeJDBERkRlg15I81pm+ERERUa3AFhkiIiIzwK4leZjIEBERmQNJun3oc70VYiJDRERkBiRJzzEyVprIWGc7FBEREdUKbJEhIiIyAxwjIw8TGSIiIjPA6dfyWGf6RkRERLUCW2SIiIjMALuW5GEiQ0REZAYkhX7dQ5J15jHsWiIiIiLLxRYZIiIiM8DBvvIwkSEiIjIHCsXtQ5/rrZB11pqIiIhqBbbIEBERmQFJkvTaZsBatyhgIkNERGQGOP1aHiYyREREZoCDfeWxzvSNiIiIAAALFy5EQEAAHBwcEBoaioMHD97z/MTERLRq1QqOjo7w9/fH+PHjcfPmzRqKtjK2yBAREZkDSc9ZSzJWxNuwYQNiY2OxZMkShIaGIjExEREREcjIyICXl1el87/88ktMmjQJK1euRNeuXXH69Gm8+uqrkCQJ8+bNkx+7HtgiQ0REZA7+v2tJ7gEZXUvz5s3D8OHDERMTg7Zt22LJkiVwcnLCypUrqzx///79ePjhhzFgwAAEBATg6aefRv/+/e/bimNMTGSIiIhqkcLCQo2jpKSkyvNKS0uRlpaG8PBwdZlCoUB4eDhSU1OrvKZr165IS0tTJy7nz5/H9u3b0bNnT8NXREvsWiIiIjIDkqSApMeGSRXX+vv7a5THx8dj2rRplc6/fPkyysvL4e3trVHu7e2NU6dOVfmMAQMG4PLly3jkkUcghEBZWRlGjBiBd999V3bc+mIiQ0REZA5kdg9pXA8gKysLrq6u6mJ7e3t9I1PbvXs3Zs+ejUWLFiE0NBRnz57FuHHjMHPmTEyZMsVgz9EFExkiIqJaxNXVVSORqY6HhwdsbGyQk5OjUZ6TkwMfH58qr5kyZQoGDx6MYcOGAQDat2+P4uJivPbaa3jvvfegMMFaNiYfI2Pp076IiIgMoWJBPH0OXdjZ2SE4OBgpKSnqMpVKhZSUFISFhVV5zfXr1yslKzY2NgAAIYSONTYMk7bI1IZpX0RERIZgigXxYmNjER0djZCQEHTp0gWJiYkoLi5GTEwMACAqKgp+fn5ISEgAAPTu3Rvz5s1Dp06d1F1LU6ZMQe/evdUJTU0zaSJz57QvAFiyZAm2bduGlStXYtKkSZXOv3PaFwAEBASgf//+OHDgQI3GTUREVBtERkYiLy8PU6dOhVKpRFBQEJKTk9UDgDMzMzVaYCZPngxJkjB58mRcunQJnp6e6N27N95//31TVcF0iUzFtK+4uDh1mTbTvtatW4eDBw+iS5cu6mlfgwcPrvY5JSUlGlPPCgsLDVcJIiIiQ5EkWYvaaVwvw+jRozF69Ogq39u9e7fG6zp16iA+Ph7x8fGynmUMJktkamraV0JCAqZPn27Q2ImIiAyNey3JY/LBvrq4c9rX4cOHsWnTJmzbtg0zZ86s9pq4uDgUFBSoj6ysrBqMmIiISEsKhf6HFTJZi0xNTfuyt7c36Bx6IiIiMh8mS99qy7QvIiIiQ5AkSe/DGpl01lJtmPZFRERkECbY/bo2MGkiUxumfREREZHpSMLK+mQKCwvh5uaGgoICrZZwJiIi61UT3xkVz7j00Vi4Osof01l4owR+73xqdd9v3GuJiIjIHEgKPdeRsc6uJeusNREREdUKbJEhIiIyBwrp9qHP9VaIiQwREZEZkCQFJD26h/S51pJZZ62JiIioVmCLDBERkTlg15IsTGSIiIjMgKRQQNJjQTx9rrVkTGSIiIjMgSTdPvS53gpZZ/pGREREtQJbZIiIiMyBQtJvryWOkSEiIiKTYdeSLOxaIiIiIovFFhkiIiIzwFlL8jCRISIiMgfcNFIWnWp99OhRzJo1C4sWLcLly5c13issLMSQIUMMGhwRERHRvWidyPz444/o0qUL1q9fjw8++ACtW7fGrl271O/fuHEDa9asMUqQREREtZ4k/be6r5yDg33vbdq0aXj77bdx4sQJXLx4ERMmTECfPn2QnJxszPiIiIisQsWmkfoc1kjrMTJ//vkn1q5dCwCQJAkTJkxAo0aN8OKLL2L9+vV48MEHjRYkERERUVW0TmTs7e2Rn5+vUTZgwAAoFApERkZi7ty5ho6NiIjIenDTSFm0TmSCgoKwa9cuBAcHa5S/8sorEEIgOjra4MERERFZDc5akkXrROaNN97Ar7/+WuV7/fv3hxACSUlJBguMiIjIqnBlX1m0TmSef/55PP/889W+P2DAAAwYMMAgQRERERFpgwviERERmQOFQs9NI9m1RERERKbCMTKyWGetiYiIqFZgiwwREZE54PRrWZjIEBERmQNJ0rNriYmMVsrLy7F69WqkpKQgNzcXKpVK4/1ffvnFYMERERER3YvOicy4ceOwevVq9OrVC+3atYNkpRkgERGRQXEdGVl0TmTWr1+Pr7/+Gj179jRGPERERNaJ069l0bnWdnZ2CAwMNEYsRERERDrROZF566238Mknn0AIYYx4iIiIrFNF15I+hxXSuWtp37592LVrF3bs2IEHHngAtra2Gu9v2rTJYMERERFZDS6IJ4vOiUy9evXuuecSEZG5EuXlyN2+G8otP6P08lUo7O3gHNgE/jEvwrlFgKnDI2sn6TlGhomMdlatWmWMOIiIjEYIgcykDTgzayFKsnMh1bGBKCsHJAmSQoFzHyXB48muaDvvPdRtyzGARJZE9oJ4eXl5yMjIAAC0atUKnp6eBguKiMhQhBD4650EXPxkzX9lZeUVb0KU3/75yu4D+O3hlxCavBruoR1NESpZO06/lkXndqji4mIMGTIEvr6+eOyxx/DYY4+hYcOGGDp0KK5fv26MGImIZLv46RqNJKY6orwc5Tdu4uCzw3Aj8381EBnRXSrGyOhzWCGdax0bG4s9e/bg+++/R35+PvLz8/Hdd99hz549eOutt4wRIxGRLOU3buL09AU6XKBC+bViXPj0/okPEZkHnROZb7/9FitWrECPHj3g6uoKV1dX9OzZE0lJSdi4caMxYiQikiX7mx0ou1ak0zWivBxZK79B+fUbRoqKqBqcfi2LzonM9evX4e3tXancy8uLXUtEZFYufblV1iyQsmvFyPtxnxEiIrqHipV99TmskM61DgsLQ3x8PG7evKkuu3HjBqZPn46wsDCDBkdEpI+bl5TAXRvbakWSUKLMM3xARGRwOs9a+uSTTxAREYFGjRqhY8fbI/uPHj0KBwcH7Ny50+ABEhHJJdnYyLtQCPnXEskkJAlCj+4hfa61ZDonMu3atcOZM2fwxRdf4NSpUwCA/v37Y+DAgXB0dDR4gEREcjk180fRqfPqKda6cGjsa4SIiO5BkvRc2dc6ExlZn5iTkxOGDx+OuXPnYu7cuRg2bJheSczChQsREBAABwcHhIaG4uDBg9We2717d0iSVOno1auX7OcTUe3U6NV+spIYO28PeDzZ1QgREZGhadUis3XrVvTo0QO2trbYunXrPc/t06ePTgFs2LABsbGxWLJkCUJDQ5GYmIiIiAhkZGTAy8ur0vmbNm1CaWmp+vWVK1fQsWNHvPTSSzo9l4hqP6+e3WHv46nbeBeFAgFvDICijuz1Qonk4V5LskhCi22sFQoFlEolvLy8oLjHqGhJklCu4/9+QkND8eCDD+Kzzz4DAKhUKvj7+2PMmDGYNGnSfa9PTEzE1KlTkZ2dDWdn5/ueX1hYCDc3NxQUFMDV1VWnWInI8mRv2onDkWO1OleqYwPHxg3xyO/fwtbdzciRkSWoie+Mimdkb1sBV2cn+fcpvg7fXkOt7vtNq/RNpVKpW0dUKlW1h65JTGlpKdLS0hAeHv5fQAoFwsPDkZqaqtU9VqxYgVdeeaXaJKakpASFhYUaBxFZD98XItBu4fTb4wds7vEfMRsbOPh5IzR5FZMYMg2u7CuLQWqdn58v67rLly+jvLy80ro03t7eUCqV973+4MGDOHHiBIYNG1btOQkJCXBzc1Mf/v7+smIlIsvV5LVXELpzFRo81uV2gY0Ckm0dSLa3u49sXJzQZORAPJz6LZya8t8IIkuicyfwBx98gICAAERGRgIAXnrpJXz77bfw9fXF9u3b1VOya8KKFSvQvn17dOnSpdpz4uLiEBsbq35dWFjIZIbICnk8HgaPx8NQdPoCcr77CaVX8qGws4VzYAB8+kWgjh5N+kQGwU0jZdE5kVmyZAm++OILAMBPP/2En3/+GcnJyfj666/xzjvv4Mcff9T6Xh4eHrCxsUFOTo5GeU5ODnx8fO55bXFxMdavX48ZM2bc8zx7e3vY29trHRMR1W4uLZvC5Z3XTB0GUWX6rs7LlX21o1Qq1S0aP/zwA15++WU8/fTTmDBhAv744w+d7mVnZ4fg4GCkpKSoy1QqFVJSUu67SvA333yDkpISDBo0SNcqEBERUS2hcyLj7u6OrKwsAEBycrJ6oK4QQufBvsDt3bSTkpKwZs0anDx5Em+88QaKi4sRExMDAIiKikJcXFyl61asWIG+ffuiQYMGOj+TiIjI3FSs7KvPYY107lp64YUXMGDAALRo0QJXrlxBjx49AABHjhxBYGCgzgFERkYiLy8PU6dOhVKpRFBQEJKTk9UDgDMzMytN+c7IyMC+fft06sYiIiIya1xHRhadE5n58+cjICAAWVlZ+PDDD+Hi4gIAyM7OxsiRI2UFMXr0aIwePbrK93bv3l2prFWrVtBi+RsiIiKq5XROZGxtbfH2229XKh8/frxBAiIiIrJGQlJA6NGqos+1lkzWGtxnzpzBrl27kJubC5VKpfHe1KlTDRIYERGRVeH0a1l0Tt+SkpLQpk0bTJ06FRs3bsTmzZvVx5YtW4wQIhERUe0noFC3ysg6ZK5xq8vGzcDtRXBHjRoFX19f2Nvbo2XLlti+fbtWzzp8+DCOHz+ufv3dd9+hb9++ePfddzX2UdSFzrWeNWsW3n//fSiVSqSnp+PIkSPq4/Dhw7KCICIioppXsXFzfHw8Dh8+jI4dOyIiIgK5ublVnl9aWoqnnnoKFy9exMaNG5GRkYGkpCT4+flp9bzXX38dp0+fBgCcP38er7zyCpycnPDNN99gwoQJsuqgcyLz77//cqdpIiIiQ6voWtLn0NG8efMwfPhwxMTEoG3btliyZAmcnJywcuXKKs9fuXIlrl69ii1btuDhhx9GQEAAunXrpvWq/qdPn0ZQUBCA2+vBPfbYY/jyyy+xevVqfPvttzrHD8hIZF566SVOeyYiIjI0SdJz08jbiczdGyWXlJRU+Tg5Gzdv3boVYWFhGDVqFLy9vdGuXTvMnj1b63XkhBDqsbU///wzevbsCQDw9/fH5cuXtf6o7qTzYN/AwEBMmTIFv//+O9q3bw9bW1uN98eOHSsrECIiItLf3fsJxsfHY9q0aZXOu9fGzadOnary3ufPn8cvv/yCgQMHYvv27Th79ixGjhyJW7duIT4+/r6xhYSEYNasWQgPD8eePXuwePFiAMCFCxcqxaEtnROZZcuWwcXFBXv27MGePXs03pMkiYkMERGRDPquzltxbVZWFlxdXdXlhtxvUKVSwcvLC8uWLYONjQ2Cg4Nx6dIlfPTRR1olMomJiRg4cCC2bNmC9957T72Q7saNG9G1a1dZMemcyFy4cEHWg4iIiOgeDLSyr6urq0YiUx05Gzf7+vrC1tYWNjY26rI2bdpAqVSitLQUdnZ293xmhw4dNGYtVfjoo4807qkL2Z9YaWkpMjIyUFZWJvcWREREZCJyNm5++OGHcfbsWY015E6fPg1fX9/7JjH34uDgUGmoirZ0TmSuX7+OoUOHwsnJCQ888AAyMzMBAGPGjMGcOXNkBUFERGTtBCS9D13punHzG2+8gatXr2LcuHE4ffo0tm3bhtmzZ2PUqFHVPsPd3R3169fX6pBD566luLg4HD16FLt378YzzzyjLg8PD8e0adMwadIkWYEQERFZM1NsUaDrxs3+/v7YuXMnxo8fjw4dOsDPzw/jxo3DxIkTq31GYmKiznHpQhI67r7YpEkTbNiwAQ899BDq1q2Lo0ePolmzZjh79iw6d+6MwsJCY8VqEIWFhXBzc0NBQYFWfYhERGS9auI7o+IZf/+6Fa4uzvLvU1SMJo/1sbrvN53Tt7y8PHh5eVUqLy4uhmSl+zwQERHpTa81ZPQcKFyDzp07h8mTJ6N///7qFYR37NiBP//8U9b9dK51SEgItm3bpn5dkbwsX7682sFBREREdG8V06/1Oczdnj170L59exw4cACbNm1CUVERAODo0aNaTd+uis5jZGbPno0ePXrgr7/+QllZGT755BP89ddf2L9/f6V1ZYiIiEg7phgjU9MmTZqEWbNmITY2FnXr1lWXP/HEE/jss89k3VPnWj/yyCNIT09HWVkZ2rdvjx9//BFeXl5ITU1FcHCwrCCIiIio9jt+/Dief/75SuVeXl41t0UBADRv3hxJSUmyHkhERERVkLnxo8b1Zq5evXrIzs5G06ZNNcqPHDmi9Q7ad5OVyABAbm4ucnNzNRbFAW6v2kdEREQ60rNryRIG+77yyiuYOHEivvnmG0iSBJVKhd9++w1vv/02oqKiZN1T50QmLS0N0dHROHnyJO6euS1JktY7YBIREZF1qVg8z9/fH+Xl5Wjbti3Ky8sxYMAATJ48WdY9dU5khgwZgpYtW2LFihXw9vbmlGsiIiIDkLs6753Xmzs7OzskJSVhypQpOHHiBIqKitCpUye0aNFC9j11TmTOnz+Pb7/9Vr1jJREREenPGmYtVWjcuDH8/f0BQO8GEZ1r/eSTT+Lo0aN6PZSIiIis04oVK9CuXTs4ODjAwcEB7dq1w/Lly2XfT+cWmeXLlyM6OhonTpxAu3btKu1W2adPH9nBEBERWS0Jes5aMlgkRjN16lTMmzcPY8aMUS+im5qaivHjxyMzMxMzZszQ+Z46JzKpqan47bffsGPHjkrvcbAvERGRPAIKCN07SjSuN3eLFy9GUlIS+vfvry7r06cPOnTogDFjxshKZHSu9ZgxYzBo0CBkZ2dDpVJpHExiiIiIqDq3bt1CSEhIpfLg4GCUlZXJuqfOicyVK1cwfvx49RbfREREpD9r2Gtp8ODBWLx4caXyZcuWYeDAgbLuqXPX0gsvvIBdu3ahefPmsh5IREREldXWWUuxsbHqnyVJwvLly/Hjjz/ioYceAgAcOHAAmZmZNbcgXsuWLREXF4d9+/ahffv2lQb7jh07VlYgRERE1qy2riNz5MgRjdcV+zKeO3cOAODh4QEPDw/8+eefsu4vibuX572Pu/dH0LiZJOH8+fOyAqkphYWFcHNzQ0FBAVxdXU0dDhERmbGa+M6oeEbGH3tR18VF9n2uFRWh1YOPWt33m84tMhcuXDBGHERERFattnYtGZvsTSOJiIjIcPQdsGsJg30B4NChQ/j666+RmZmJ0tJSjfc2bdqk8/20SmRiY2Mxc+ZMODs7awzaqcq8efN0DoKIiIhqv/Xr1yMqKgoRERH48ccf8fTTT+P06dPIycnB888/L+ueWiUyR44cwa1bt9Q/V4cbSBIREclTWwf73mn27NmYP38+Ro0ahbp16+KTTz5B06ZN8frrr8PX11fWPbVKZHbt2lXlz0RERGQY1jBG5ty5c+jVqxeA2zthFxcXQ5IkjB8/Hk888QSmT5+u8z1l1VoIgcuXL+PKlStyLiciIiIr5O7ujmvXrgEA/Pz8cOLECQBAfn4+rl+/LuueOiUySqUSUVFRcHd3h7e3N7y8vODu7o4hQ4YgJydHVgBERET0X9eSPoe5e+yxx/DTTz8BAF566SWMGzcOw4cPR//+/fHkk0/KuqfWs5YKCwvRtWtXFBUVISYmBq1bt4YQAn/99Re++uor7Nu3D4cPH4aLHnPgiYiIrJWAnl1LFrBp5GeffYabN28CAN577z3Y2tpi//796NevHyZPnizrnlonMp988glsbGzw559/wtPTU+O9yZMn4+GHH8ann36Kd999V1YgREREVLvVr19f/bNCocCkSZMAANevX0d6ejq6du2q8z21Tt+2bduGd999t1ISAwBeXl6Ii4vD999/r3MAREREZB1dS9U5c+YMHn30UVnXap3InD59+p6ZUteuXZGRkSErCCIiImt3e0E8hR6H5SYy+tBpjEy9evWqfb9evXooLCw0RExERERWxxrWkTEGrVtkhBBQKKo/XZIk6Lj/JBEREZFetG6REUKgZcuW1a7eKzeJWbhwIT766CMolUp07NgRCxYsQJcuXao9Pz8/H++99x42bdqEq1evokmTJkhMTETPnj1lPZ+IiMgc1Oa9lrZu3XrP9/XZkFrrRGbVqlWyH1KdDRs2IDY2FkuWLEFoaCgSExMRERGBjIwMeHl5VTq/tLQUTz31FLy8vLBx40b4+fnh77//vmeXFxERkSUQQoIQeiQyelxrbH379r3vOXK3OdI6kYmOjpb1gHuZN28ehg8fjpiYGADAkiVLsG3bNqxcuVI9JetOK1euxNWrV7F//37Y2toCAAICAgweFxERERmOSqUy2r1NtnpOaWkp0tLSEB4e/l8wCgXCw8ORmppa5TVbt25FWFgYRo0aBW9vb7Rr1w6zZ89GeXl5tc8pKSlBYWGhxkFERGR+FLcXxZN5mPAr3aRMVuvLly+jvLwc3t7eGuXe3t5QKpVVXnP+/Hls3LgR5eXl2L59O6ZMmYK5c+di1qxZ1T4nISEBbm5u6sPf39+g9SAiIjIEa15HRh8Wlb6pVCp4eXlh2bJlCA4ORmRkJN577z0sWbKk2mvi4uJQUFCgPrKysmowYiIiIjImrcfIGJqHhwdsbGwqbTaZk5MDHx+fKq/x9fWFra0tbGxs1GVt2rSBUqlEaWkp7OzsKl1jb28Pe3t7wwZPRERkYFxHRh6TtcjY2dkhODgYKSkp6jKVSoWUlBSEhYVVec3DDz+Ms2fPagwaOn36NHx9fatMYoiIiCwFu5bk0blFJjY2tspySZLg4OCAwMBAPPfccxobQ93rXtHR0QgJCUGXLl2QmJiI4uJi9SymqKgo+Pn5ISEhAQDwxhtv4LPPPsO4ceMwZswYnDlzBrNnz8bYsWN1rQYRERGZQH5+PjZu3Ihz587hnXfeQf369XH48GF4e3vDz89P5/vpnMgcOXIEhw8fRnl5OVq1agXgdquIjY0NWrdujUWLFuGtt97Cvn370LZt23veKzIyEnl5eZg6dSqUSiWCgoKQnJysHgCcmZmpsZqwv78/du7cifHjx6NDhw7w8/PDuHHjMHHiRF2rQUREZFasoWvp2LFjCA8Ph5ubGy5evIjhw4ejfv362LRpEzIzM/H555/rfE9J6Lgkb2JiIvbu3YtVq1bB1dUVAFBQUIBhw4bhkUcewfDhwzFgwADcuHEDO3fu1DkgYyssLISbmxsKCgrU8RMREVWlJr4zKp5x4PApuNStK/s+RdeuIbRza7P+fgsPD0fnzp3x4Ycfom7dujh69CiaNWuG/fv3Y8CAAbh48aLO99R5jMxHH32EmTNnanxIbm5umDZtGj788EM4OTlh6tSpSEtL0zkYIiIia2UNY2T++OMPvP7665XK/fz8ql165X50TmQKCgqQm5tbqTwvL0+92Fy9evVQWloqKyAiIiKqnezt7atcmPb06dPw9PSUdU+dE5nnnnsOQ4YMwebNm/HPP//gn3/+webNmzF06FD1XgoHDx5Ey5YtZQVERERkjayhRaZPnz6YMWMGbt26BeD2RKHMzExMnDgR/fr1k3VPnROZpUuX4sknn8Qrr7yCJk2aoEmTJnjllVfw5JNPqhema926NZYvXy4rICIiImtkDYnM3LlzUVRUBC8vL9y4cQPdunVDYGAg6tati/fff1/WPXUe7FuhqKgI58+fBwA0a9YMLi4usgKoaRzsS0RE2qrJwb77D5/Re7Bv184tLOL7bd++fTh27BiKiorQuXNnjX0XdSV7ZV8XFxd06NBB9oOJiIjoPwIShKjd068rPPLII3jkkUcMci+dE5ni4mLMmTMHKSkpyM3NrbQ1d0UrDREREWlPBQkqPZIRfa41pk8//VTrc+UscKtzIjNs2DDs2bMHgwcPhq+vLyTJPD84IiIiMr358+drvM7Ly8P169dRr149ALdX+nVycoKXl1fNJDI7duzAtm3b8PDDD+v8MCIiIqpabV3Z98KFC+qfv/zySyxatAgrVqxQ7w6QkZGB4cOHV7m+jDZ0nrXk7u6u1T5KREREpD0hJL0PczdlyhQsWLBAncQAQKtWrTB//nxMnjxZ1j11TmRmzpyJqVOn4vr167IeSERERNYpOzsbZWVllcrLy8uRk5Mj6546dy3NnTsX586dg7e3NwICAmBra6vx/uHDh2UFQkREZM0E9OsekrWWSg178skn8frrr2P58uXo3LkzACAtLQ1vvPGG7CnYOicyFav3EhERkeHo2z1kCV1LK1euRHR0NEJCQtQNIWVlZYiIiJC9kK7OiUx8fLysBxEREVH1autg3zt5enpi+/btOH36NE6ePAlJktC6dWu9tjWSvSAeERERkRwtW7ZEixYtAEDvZVy0Guxbv359XL58GcB/s5aqO4iIiEh31jBrCQA+//xztG/fHo6OjnB0dESHDh2wdu1a2ffTqkVm/vz5qPv/+z8kJibKfhgRERFVTQBQ3fese19v7ubNm4cpU6Zg9OjR6vXo9u3bhxEjRuDy5csYP368zveUvWmkpeKmkUREpK2a3DTy5z8y4ewi/xnFRYUIf7CxWX+/NW3aFNOnT0dUVJRG+Zo1azBt2jSNxfO0pVWLTGFhodY3NNcPj4iIyJxZw6yl7OxsdO3atVJ5165dkZ2dLeueWiUy9erV03owTnl5uaxAiIiIrJk1zFoKDAzE119/jXfffVejfMOGDerBv7rSKpHZtWuX+ueLFy9i0qRJePXVVxEWFgYASE1NxZo1a5CQkCArCCIiIqr9pk+fjsjISPz666/qMTK//fYbUlJS8PXXX8u6p1aJTLdu3dQ/z5gxA/PmzUP//v3VZX369EH79u2xbNkyREdHywqEiIjImllD11K/fv1w4MABzJ8/H1u2bAEAtGnTBgcPHkSnTp1k3VPnvZZSU1MREhJSqTwkJAQHDx6UFQQREZG1q+ha0ueQY+HChQgICICDgwNCQ0O1/i5fv349JEnSecX/4OBgrFu3DmlpaUhLS8O6detkJzGAjETG398fSUlJlcqXL18Of39/2YEQERFRzdqwYQNiY2MRHx+Pw4cPo2PHjoiIiEBubu49r7t48SLefvttPProozUUafV0Xtl3/vz56NevH3bs2IHQ0FAAwMGDB3HmzBl8++23Bg+QiIjIGqjE7UOf63U1b948DB8+HDExMQCAJUuWYNu2bVi5ciUmTZpU5TXl5eUYOHAgpk+fjr179yI/P/++z1EoFPedNCRJUpU7Y9+PzolMz549cfr0aSxevBinTp0CAPTu3RsjRoxgiwwREZFMhpq1dPeSKfb29rC3t690fmlpKdLS0hAXF6cuUygUCA8PR2pqarXPmTFjBry8vDB06FDs3btXq9g2b95c7Xupqan49NNPoVLJWw5Q1l5L/v7+mD17tqwHEhERUWWGGux7d6NCfHw8pk2bVun8y5cvo7y8HN7e3hrl3t7e6oaKu+3btw8rVqxAenq6TrE999xzlcoyMjIwadIkfP/99xg4cCBmzJih0z0raJXIHDt2TOsbdujQQVYgREREpL+srCyNxWmrao2R49q1axg8eDCSkpLg4eEh+z7/+9//EB8fjzVr1iAiIgLp6elo166d7PtplcgEBQVBkiQIITT6uCp2N7izjAviERER6U6I24c+1wO3V9jXZpV9Dw8P2NjYICcnR6M8JycHPj4+lc4/d+4cLl68iN69e6vLKrqD6tSpg4yMDDRv3rza5xUUFGD27NlYsGABgoKCkJKSYpDBwlrNWrpw4QLOnz+PCxcu4Ntvv0XTpk2xaNEipKenIz09HYsWLULz5s052JeIiEgmFSS9D13Y2dkhODgYKSkp/8WgUiElJUW94O2dWrdujePHj6u/+9PT09GnTx88/vjjSE9Pv+c42Q8//BDNmjXDDz/8gK+++gr79+832IwnrVpkmjRpov75pZdewqeffoqePXuqyzp06AB/f39MmTJF5/nkREREZBqxsbGIjo5GSEgIunTpgsTERBQXF6tnMUVFRcHPzw8JCQlwcHCo1AVUr149ALhv19CkSZPg6OiIwMBArFmzBmvWrKnyvE2bNulcB50H+x4/fhxNmzatVN60aVP89ddfOgdAREREplnZNzIyEnl5eZg6dSqUSiWCgoKQnJysHgCcmZkJhULnJecqiYqK0nrPRl1JQujWI9e5c2e0a9cOy5cvh52dHYDbU7iGDRuGEydO4PDhw0YJ1FBqYkt2IiKqHWriO6PiGZv35cLZRf4ziosK8fwjXlb3/aZzi8ySJUvQu3dvNGrUSD1D6dixY5AkCd9//73BAyQiIiKqjs6JTJcuXXD+/Hl88cUX6nnmkZGRGDBgAJydnQ0eIBERkTUw1IJ41kbWgnjOzs547bXXDB0LERGR1TLFFgW1gawRPGvXrsUjjzyChg0b4u+//wZwew+m7777zqDBEREREd2Lzi0yixcvxtSpU/Hmm29i1qxZ6gXw3N3dkZiYWOUyxESW4vqNcvy4OwfpJwpQfL0Mjg42aNuqLno+6QPXuramDo+IajM9Zy1Bn2stmM6JzIIFC5CUlIS+fftizpw56vKQkBC8/fbbBg2OqKaUlKqwbO0FfLfjf7hZooJCAahUgEICdv2WhyVrLiDicW+MHtIcdV1k9cgSEd2ToVb2tTY6/4t84cIFdOrUqVK5vb09iouLDRIUUU26cbMcsVOP4cSpQvU/BBWbsFb0OZeVCexIUeL4XwVY+EEQ3N3sTBMsEdVaclbnvft6a6TzGJmmTZtWuetlcnIy2rRpY4iYiGrU9I9P4s+Mwvv+b0alAi5l38A700+grNxK/+tDRGRmdG6RiY2NxahRo3Dz5k0IIXDw4EF89dVXSEhIwPLly40RI5HRnDpzDfsOXNH6/HLV7Wv2H7yCx8Lk7/5KRHQ3di3Jo3MiM2zYMDg6OmLy5Mm4fv06BgwYgIYNG+KTTz7BK6+8YowYiYxm8/ZLsLGRUK5DC4tCAXy77RITGSIyKFNsUVAbyJp+PXDgQJw5cwZFRUVQKpX4559/MHToUNlBLFy4EAEBAXBwcEBoaCgOHjxY7bmrV6+GJEkah4ODg+xnk/USQuCnPXk6JTHA7S6mtKP5yC+4ZaTIiIhIW7KnX+Tm5iIjIwMAIEkSPD09Zd1nw4YNiI2NxZIlSxAaGorExEREREQgIyMDXl5eVV7j6uqqfnbF84l0VXy9HKW3VLKvv5pfinpunJJNRIbBBfHk0blF5tq1axg8eDAaNmyIbt26oVu3bmjYsCEGDRqEgoICnQOYN28ehg8fjpiYGLRt2xZLliyBk5MTVq5cWe01kiTBx8dHfVTs0kmkC4VCvwTYRs/riYjuVDFGRp/DGumcyAwbNgwHDhzAtm3bkJ+fj/z8fPzwww84dOgQXn/9dZ3uVVpairS0NISHh/8XkEKB8PBwpKamVntdUVERmjRpAn9/fzz33HP4888/qz23pKQEhYWFGgcRADg6KODsZCPrWoUCaFCfU7CJiExN50Tmhx9+wMqVKxEREQFXV1e4uroiIiICSUlJOu9+ffnyZZSXl1dqUfH29oZSqazymlatWmHlypX47rvvsG7dOqhUKnTt2hX//PNPlecnJCTAzc1Nffj7++sUI9VekiTh2ad9odDxb4GNAugW5gkXZy6MR0SGU7FppD6HNdI5kWnQoAHc3Nwqlbu5ucHd3d0gQd1LWFgYoqKiEBQUhG7dumHTpk3w9PTE0qVLqzw/Li4OBQUF6iMrK8voMZLl6NvDV734nbbKVcALvRoaJyAisloq/DdORtZh6gqYiM6JzOTJkxEbG6vRYqJUKvHOO+9gypQpOt3Lw8MDNjY2yMnJ0SjPycmBj4+PVvewtbVFp06dcPbs2Srft7e3V7ccVRxEFfwbOuHlPn5an6+QgG5hHghqVzmZJyKimqdV23inTp00ZgadOXMGjRs3RuPGjQEAmZmZsLe3R15enk7jZOzs7BAcHIyUlBT07dsXAKBSqZCSkoLRo0drdY/y8nIcP34cPXv21Pq5RHcaNaQ5CovKkPxLDiSp+gFzkgQEB7lj6lutOVOOiAyOC+LJo1UiU5FkGENsbCyio6MREhKCLl26IDExEcXFxYiJiQEAREVFwc/PDwkJCQCAGTNm4KGHHkJgYCDy8/Px0Ucf4e+//8awYcOMFiPVbjY2Et57sxU6tHXD+s1ZyLx0AzY2kjqpKS8X8PKwx4u9/fDyc41Qx4ZJDBEZHhMZebRKZOLj440WQGRkJPLy8jB16lQolUoEBQUhOTlZPQA4MzMTijtGY/77778YPnw4lEol3N3dERwcjP3796Nt27ZGi5FqP0mS0CfCF72f9kH6iQKkn8hH8fVyODraoG3LuujSqT5smMAQkRGphASVHqvz6nOtJZOEkJ/DFRUVQXXXSElzH4NSWFgINzc3FBQUmH2sRERkWjXxnVHxjBU/5sPJWf4zrhcXYujT9azu+03nwb4XLlxAr1694OzsrJ6p5O7ujnr16tXIrCUiIqLaiAviyaPzQhiDBg2CEAIrV66Et7c3Bz0SEREZAMfIyKNzInP06FGkpaWhVatWxoiHiIiISGs6dy09+OCDXFSOiIjIwIQ+i+Gxa0l7y5cvx4gRI3Dp0iW0a9cOtraau/926NDBYMERERFZCyEkCD1mHulzrSXTOZHJy8vDuXPn1Ou8ALenrgohIEkSysvLDRogERERUXV0TmSGDBmCTp064auvvuJgXyIiIgPhYF95dE5k/v77b2zduhWBgYHGiIeIiMgqVYx10ed6a6TzYN8nnngCR48eNUYsRERERDrRuUWmd+/eGD9+PI4fP4727dtXGuzbp08fgwVHRERkLdi1JI/OicyIESMA3N688W4c7EtERCQPExl5dE5k7t5biYiIiPTHMTLy6DxGhoiIiMhcaJ3I9OzZEwUFBerXc+bMQX5+vvr1lStX0LZtW4MGR0REZC24aaQ8WicyO3fuRElJifr17NmzcfXqVfXrsrIyZGRkGDY6IiIiK6FS6X9YI60TGXFXqnf3ayIiIqKapvNgXyIiIjI8zlqSR+tERpKkStsRcHsCIiIiw2AiI4/WiYwQAq+++irs7e0BADdv3sSIESPg7OwMABrjZ4iIiIhqgtaJTHR0tMbrQYMGVTonKipK/4iIiIiskAp6riNjsEgsi9aJzKpVq4wZBxERkVUTQug1kcZaJ+FwQTwiIiKyWJy1REREZAY42FceJjJERERmQOi5qJ2w0kEyTGSIiIjMAFtk5OEYGSIiIrJYbJEhIiIyAyqh5/RrK22RYSJDRERkBti1JA+7loiIiMhisUWGiIjIDAiVgNCjf0ifay0ZExkiIiIzwDEy8rBriYiIiCwWW2SIiIjMAAf7ysNEhoiIyAyoVAIqPfqH9LnWkrFriYiIiCwWW2SIiIjMALuW5GEiQ0REZAaYyMjDRIaIiMgMqISASo9sRJ9rLRnHyBAREZHFYosMERGRGRCq24c+11sjJjJERERmQEBA6NE9JMCuJSIiIiKLwkSGiIjIDAgVoNLjkNu1tHDhQgQEBMDBwQGhoaE4ePBgtecmJSXh0Ucfhbu7O9zd3REeHn7P82uCWSQyunyId1q/fj0kSULfvn2NGyAREZGRCSH0PnS1YcMGxMbGIj4+HocPH0bHjh0RERGB3NzcKs/fvXs3+vfvj127diE1NRX+/v54+umncenSJX2rL5vJExldP8QKFy9exNtvv41HH320hiIlIiKqXebNm4fhw4cjJiYGbdu2xZIlS+Dk5ISVK1dWef4XX3yBkSNHIigoCK1bt8by5cuhUqmQkpJSw5H/x+SJjK4fIgCUl5dj4MCBmD59Opo1a1aD0RIRERmHSuh/AEBhYaHGUVJSUuXzSktLkZaWhvDwcHWZQqFAeHg4UlNTtYr5+vXruHXrFurXr693/eUyaSIj90OcMWMGvLy8MHTo0Ps+o6SkpNIvlYiIyNwIldD7AAB/f3+4ubmpj4SEhCqfd/nyZZSXl8Pb21uj3NvbG0qlUquYJ06ciIYNG2p8j9c0k06/vteHeOrUqSqv2bdvH1asWIH09HStnpGQkIDp06frGyoREZFFyMrKgqurq/q1vb29UZ4zZ84crF+/Hrt374aDg4NRnqENk3ct6eLatWsYPHgwkpKS4OHhodU1cXFxKCgoUB9ZWVlGjpKIiEh3FXst6XMAgKurq8ZRXSLj4eEBGxsb5OTkaJTn5OTAx8fnnrF+/PHHmDNnDn788Ud06NDBIPWXy6QtMrp+iOfOncPFixfRu3dvdZlKdXu+WZ06dZCRkYHmzZtrXGNvb2+0bJSIiMhQVCoBVcVAF5nX68LOzg7BwcFISUlRz/6tGLg7evToaq/78MMP8f7772Pnzp0ICQmRHa+hmLRF5s4PsULFhxgWFlbp/NatW+P48eNIT09XH3369MHjjz+O9PR0+Pv712T4REREBmOK6dexsbFISkrCmjVrcPLkSbzxxhsoLi5GTEwMACAqKgpxcXHq8z/44ANMmTIFK1euREBAAJRKJZRKJYqKigz2OejK5FsUxMbGIjo6GiEhIejSpQsSExMrfYh+fn5ISEiAg4MD2rVrp3F9vXr1AKBSOREREd1bZGQk8vLyMHXqVCiVSgQFBSE5OVk9djUzMxMKxX9tHosXL0ZpaSlefPFFjfvEx8dj2rRpNRm6mskTGV0/RCIiotrIVJtGjh49utqupN27d2u8vnjxoryHGJEk9NmhygIVFhbCzc0NBQUFGqO6iYiI7lYT3xkVzxg9Nxv2jvKfUXKjEJ+95Wt1329s6iAiIiKLZfKuJSIiIoLsAbt3Xm+NmMgQERGZgZqefl1bsGuJiIiILBZbZIiIiMzAnavzyr3eGjGRISIiMgNC/Lfxo9zrrRG7loiIiMhisUWGiIjIDAghoOKsJZ0xkSEiIjIDQqVn15KVzlpiIkNERGQGmMjIwzEyREREZLHYIkNERGQGVOL2oc/11oiJDBERkRlg15I87FoiIiIii8UWGSIiIjPATSPlYSJDRERkBlQq/TZ+VKkMGIwFYdcSERERWSy2yBAREZkBdi3Jw0SGiIjIDHDWkjzsWiIiIiKLxRYZIiIiM8AWGXmYyBAREZkBFfTb/VoFJjJERERkImyRkYdjZIiIiMhisUWGiIjIDHD6tTxMZIiIiMyAUAm9VvZl1xIRERGRhWGLDBERkRngYF95mMgQERGZAY6RkYddS0RERGSx2CJDRERkBoRKBaFS6XW9NWIiQ0REZAZUes5a0udaS8auJSIiIrJYbJEhIiIyAxzsKw8TGSIiIjPA6dfyMJEhIiIyA0xk5OEYGSIiIrJYbJEhIiIyAyqooBLyp1CrwOnXREREZCJCpV/3kB45kEVj1xIRERFZLLbIEBERmQEO9pWHiYyObvyjxD+rN6LwxGmUX78JO3c3eD7zGHxeiICNvZ2pwyMiIgvFdWTkYSKjpRv/KPHn+FnI2ZoCSABUAhACko0NLn25FbZvzkSz8UPQfMJrkBTssSMiIqoJZvGNu3DhQgQEBMDBwQGhoaE4ePBgtedu2rQJISEhqFevHpydnREUFIS1a9caNb6i0xew76F+yP3hF0ClAspVwP9nvqK8HABw62oBMqbMx5GBseoyIiIibalUKr0Pa2TyRGbDhg2IjY1FfHw8Dh8+jI4dOyIiIgK5ublVnl+/fn289957SE1NxbFjxxATE4OYmBjs3LnTKPHdKriGA8/E4NblfyHK7p+gZH+bjJOTPjRKLEREVHtVjJHR57BGJk9k5s2bh+HDhyMmJgZt27bFkiVL4OTkhJUrV1Z5fvfu3fH888+jTZs2aN68OcaNG4cOHTpg3759Rokva+VG3PxHqX0rixC4sOBz3FTmGSUeIiIi+o9JE5nS0lKkpaUhPDxcXaZQKBAeHo7U1NT7Xi+EQEpKCjIyMvDYY49VeU5JSQkKCws1Dm0JlQoXF63T+vz/LgSyVm3U/ToiIrJaQqj0PqyRSROZy5cvo7y8HN7e3hrl3t7eUCqV1V5XUFAAFxcX2NnZoVevXliwYAGeeuqpKs9NSEiAm5ub+vD399c6voLDf+LGxX/U42G0plLh0totul1DRERWjV1L8pi8a0mOunXrIj09HX/88Qfef/99xMbGYvfu3VWeGxcXh4KCAvWRlZWl9XNK9OgeKsm5LPtaIiKyQvomMVaayJh0+rWHhwdsbGyQk5OjUZ6TkwMfH59qr1MoFAgMDAQABAUF4eTJk0hISED37t0rnWtvbw97e3tZ8Uk2NrKu+/8g5V9LREREWjHpt62dnR2Cg4ORkpKiLlOpVEhJSUFYWJjW91GpVCgpKTF4fI6NG8q7UJLkX0tERFZJJVR6H9bI5AvixcbGIjo6GiEhIejSpQsSExNRXFyMmJgYAEBUVBT8/PyQkJAA4PaYl5CQEDRv3hwlJSXYvn071q5di8WLFxs8troPtIBrUBsUHsu4vX6MDhoPfdng8RARUe3FLQrkMXkiExkZiby8PEydOhVKpRJBQUFITk5WDwDOzMyE4o5umuLiYowcORL//PMPHB0d0bp1a6xbtw6RkZFGiS9g1GAcG/6uTtco7O3gN+g5o8RDRERE/5GElW3OUFhYCDc3NxQUFMDV1fW+55eXlCL18YEoPPyn1mvJtJ33HpqOidI3VCIiMjFdvzP0eUb3l3ajjq2L7PuU3SrC7m+6GzVWc8QRqfdhY2+HLluXwuWBwHsP4P3/91pMHsUkhoiIdMbp1/IwkdGCnUd9dN3zFQInvgbb+m4AAMm2zu3j/2c2uQU/gOBvPkPL+LGmDJWIiMiqmHyMjKWo4+KMVjPGo8XkUVB+9zOu/XkGqhs3UaeeK7ye6Qa3Tm1NHSIREVkwfVfntdaVfZnI6EhhZ4eGL/UEXjJ1JEREVJuoVIBKj+4hK938ml1LREREZLnYIkNERGQGhEoFoUezij7XWjImMkRERGaAC+LJw64lIiIiM1Ax2FefQ46FCxciICAADg4OCA0NxcGDB+95/jfffIPWrVvDwcEB7du3x/bt22U911CYyBAREVmpDRs2IDY2FvHx8Th8+DA6duyIiIgI5ObmVnn+/v370b9/fwwdOhRHjhxB37590bdvX5w4caKGI/8PV/YlIiKqRk2u7PtQjx2oY+ss+z5lt4rx+44eOsUaGhqKBx98EJ999hmA25sw+/v7Y8yYMZg0aVKl8yMjI1FcXIwffvhBXfbQQw8hKCgIS5YskR27PtgiQ0REZAYqBvvqc+iitLQUaWlpCA8PV5cpFAqEh4cjNTW1ymtSU1M1zgeAiIiIas+vCVY32LeiAaqwsNDEkRARkbmr+K6oic6L8rJig1x/9/ebvb097O3tK51/+fJllJeXqzdpruDt7Y1Tp05V+QylUlnl+UqlUp/Q9WJ1icy1a9cAAP7+/iaOhIiILMW1a9fg5uZmlHvb2dnBx8cHh1Je1vteLi4ulb7f4uPjMW3aNL3vba6sLpFp2LAhsrKyULduXUiSZOpwjKawsBD+/v7IysqyqrFA1lhva6wzwHpbU71NWWchBK5du4aGDRsa7RkODg64cOECSktL9b6XEKLSd1tVrTEA4OHhARsbG+Tk5GiU5+TkwMfHp8prfHx8dDq/JlhdIqNQKNCoUSNTh1FjXF1dreYfuztZY72tsc4A621NTFVnY7XE3MnBwQEODg5Gf86d7OzsEBwcjJSUFPTt2xfA7cG+KSkpGD16dJXXhIWFISUlBW+++aa67KeffkJYWFgNRFw1q0tkiIiI6LbY2FhER0cjJCQEXbp0QWJiIoqLixETEwMAiIqKgp+fHxISEgAA48aNQ7du3TB37lz06tUL69evx6FDh7Bs2TKT1YGJDBERkZWKjIxEXl4epk6dCqVSiaCgICQnJ6sH9GZmZkKh+G+Cc9euXfHll19i8uTJePfdd9GiRQts2bIF7dq1M1UVmMjUVvb29oiPj6+2b7S2ssZ6W2OdAdbbmuptjXWuSaNHj662K2n37t2Vyl566SW89NJLRo5Ke1a3IB4RERHVHlwQj4iIiCwWExkiIiKyWExkiIiIyGIxkSEiIiKLxUTGQl29ehUDBw6Eq6sr6tWrh6FDh6KoqOie17z++uto3rw5HB0d4enpieeee67SfhqZmZno1asXnJyc4OXlhXfeeQdlZWXGrIpOdK331atXMWbMGLRq1QqOjo5o3Lgxxo4di4KCAo3zJEmqdKxfv97Y1dGaseptzr9vOX/Gly1bhu7du8PV1RWSJCE/P7/SOQEBAZV+13PmzDFSLXRnrHrLuW9NkhPfzZs3MWrUKDRo0AAuLi7o169fpVVnzf3vNhmAIIv0zDPPiI4dO4rff/9d7N27VwQGBor+/fvf85qlS5eKPXv2iAsXLoi0tDTRu3dv4e/vL8rKyoQQQpSVlYl27dqJ8PBwceTIEbF9+3bh4eEh4uLiaqJKWtG13sePHxcvvPCC2Lp1qzh79qxISUkRLVq0EP369dM4D4BYtWqVyM7OVh83btwwdnW0Zox6m/vvW86f8fnz54uEhASRkJAgAIh///230jlNmjQRM2bM0PhdFxUVGakWujNWveXctybJiW/EiBHC399fpKSkiEOHDomHHnpIdO3aVeMcc/+7TfpjImOB/vrrLwFA/PHHH+qyHTt2CEmSxKVLl7S+z9GjRwUAcfbsWSGEENu3bxcKhUIolUr1OYsXLxaurq6ipKTEcBWQyVD1/vrrr4WdnZ24deuWugyA2Lx5syHDNRhj1ducf9/61nnXrl33TGTmz59vwGgNx1j1NtSfIWORE19+fr6wtbUV33zzjbrs5MmTAoBITU1Vl5nz320yDHYtWaDU1FTUq1cPISEh6rLw8HAoFAocOHBAq3sUFxdj1apVaNq0qXqn1NTUVLRv315ji/aIiAgUFhbizz//NGwlZDBEvQGgoKAArq6uqFNHcz3IUaNGwcPDA126dMHKlSshzGSJJWPV25x/34aqc3XmzJmDBg0aoFOnTvjoo4/MpjvNWPU29uepLznxpaWl4datWwgPD1eXtW7dGo0bN0ZqaqrGueb6d5sMgyv7WiClUgkvLy+Nsjp16qB+/fpQKpX3vHbRokWYMGECiouL0apVK/z000+ws7NT3/fOLzUA6tf3u29N0KfeFS5fvoyZM2fitdde0yifMWMGnnjiCTg5OeHHH3/EyJEjUVRUhLFjxxosfrmMVW9z/n0bos7VGTt2LDp37oz69etj//79iIuLQ3Z2NubNm6fXfQ3BWPU25udpCHLiUyqVsLOzQ7169TTKvb29Na4x57/bZBhskTEjkyZNqnJg2p3H3YNzdTVw4EAcOXIEe/bsQcuWLfHyyy/j5s2bBqqBPDVRbwAoLCxEr1690LZtW0ybNk3jvSlTpuDhhx9Gp06dMHHiREyYMAEfffSR3s+8F3Ood02rqTrfS2xsLLp3744OHTpgxIgRmDt3LhYsWICSkhKjPdMc6m0K5lBvU/zdpprFFhkz8tZbb+HVV1+95znNmjWDj48PcnNzNcrLyspw9epV+Pj43PN6Nzc3uLm5oUWLFnjooYfg7u6OzZs3o3///vDx8cHBgwc1zq+YAXC/++qjJup97do1PPPMM6hbty42b94MW1vbe54fGhqKmTNnoqSkxGj7u5i63qb4fddEnXUVGhqKsrIyXLx4Ea1atTLovSuYut41+XneyZj19vHxQWlpKfLz8zVaZXJycu5Zp5r4u001zNSDdEh3FQPjDh06pC7buXOnzgP3bt68KRwdHcWqVauEEP8N/szJyVGfs3TpUuHq6ipu3rxpsPjlklvvgoIC8dBDD4lu3bqJ4uJirZ41a9Ys4e7urnfMhmCsepvz71vfP+P3Gux7t3Xr1gmFQiGuXr2qT8gGYax6G+rfDGORE1/FYN+NGzeqy06dOlVpsO/dzOnvNhkGExkL9cwzz4hOnTqJAwcOiH379okWLVpoTFX8559/RKtWrcSBAweEEEKcO3dOzJ49Wxw6dEj8/fff4rfffhO9e/cW9evXV3+RVUzHffrpp0V6erpITk4Wnp6eZjMdVwjd611QUCBCQ0NF+/btxdmzZzWmYFZMO9+6datISkoSx48fF2fOnBGLFi0STk5OYurUqSapY1WMUW9z/33rWmchhMjOzhZHjhwRSUlJAoD49ddfxZEjR8SVK1eEEELs379fzJ8/X6Snp4tz586JdevWCU9PTxEVFVXj9auOMeqtzX1NTU69R4wYIRo3bix++eUXcejQIREWFibCwsLU71vC323SHxMZC3XlyhXRv39/4eLiIlxdXUVMTIy4du2a+v0LFy4IAGLXrl1CCCEuXbokevToIby8vIStra1o1KiRGDBggDh16pTGfS9evCh69OghHB0dhYeHh3jrrbc0pimbmq71rvgfalXHhQsXhBC3p3kGBQUJFxcX4ezsLDp27CiWLFkiysvLTVDDqhmj3kKY9+9b1zoLIUR8fHyVda5odUxLSxOhoaHCzc1NODg4iDZt2ojZs2ebvAXqTsaotzb3NTU59b5x44YYOXKkcHd3F05OTuL5558X2dnZ6vct4e826U8SgvPQiIiIyDJx1hIRERFZLCYyREREZLGYyBAREZHFYiJDREREFouJDBEREVksJjJERERksZjIEBERkcViIkNUhWnTpiEoKMjg97148SIkSUJ6enq15+zevRuSJCE/Px8AsHr16ko7/Jpa9+7d8eabb5o6jPuSJAlbtmwxdRhEZERMZMiivfrqq1XuqPvMM8+YOjSDiYyMxOnTp43+nNWrV6s/PxsbG7i7uyM0NBQzZsxAQUGBxrmbNm3CzJkzjR6TvrKzs9GjRw+jP2PAgAFo2bIlFAqFRSR4RLUJd78mi/fMM89g1apVGmW1aVdbR0dHODo61sizXF1dkZGRASEE8vPzsX//fiQkJGDVqlX47bff0LBhQwBA/fr1ayQefRlzZ+cKJSUl8PT0xOTJkzF//nyjP4+INLFFhiyevb09fHx8NA53d3f1+5IkYenSpXj22Wfh5OSENm3aIDU1FWfPnkX37t3h7OyMrl274ty5c5XuvXTpUvj7+8PJyQkvv/xypZaJ5cuXo02bNnBwcEDr1q2xaNEijfcPHjyITp06wcHBASEhIThy5EilZ2zfvh0tW7aEo6MjHn/8cVy8eFHj/bu7liq6vdauXYuAgAC4ubnhlVdewbVr19TnXLt2DQMHDoSzszN8fX0xf/58rbqDJEmCj48PfH190aZNGwwdOhT79+9HUVERJkyYoD7v7nsFBARg1qxZiIqKgouLC5o0aYKtW7ciLy8Pzz33HFxcXNChQwccOnRI43n79u3Do48+CkdHR/j7+2Ps2LEoLi7WuO/s2bMxZMgQ1K1bF40bN8ayZcvU75eWlmL06NHw9fWFg4MDmjRpgoSEBI363Nm1dPz4cTzxxBNwdHREgwYN8Nprr6GoqEj9/quvvoq+ffvi448/hq+vLxo0aIBRo0bh1q1b1X5mAQEB+OSTTxAVFQU3N7d7fr5EZHhMZMgqzJw5E1FRUUhPT0fr1q0xYMAAvP7664iLi8OhQ4cghMDo0aM1rjl79iy+/vprfP/990hOTsaRI0cwcuRI9ftffPEFpk6divfffx8nT57E7NmzMWXKFKxZswYAUFRUhGeffRZt27ZFWloapk2bhrffflvjGVlZWXjhhRfQu3dvpKenY9iwYZg0adJ963Pu3Dls2bIFP/zwA3744Qfs2bMHc+bMUb8fGxuL3377DVu3bsVPP/2EvXv34vDhw7I+Oy8vLwwcOBBbt25FeXl5tefNnz8fDz/8MI4cOYJevXph8ODBiIqKwqBBg3D48GE0b94cUVFRqNje7dy5c3jmmWfQr18/HDt2DBs2bMC+ffsq/R7mzp2rTgJHjhyJN954AxkZGQCATz/9FFu3bsXXX3+NjIwMfPHFFwgICKgyvuLiYkRERMDd3R1//PEHvvnmG/z888+Vnrdr1y6cO3cOu3btwpo1a7B69WqsXr1a1mdHRDXApFtWEukpOjpa2NjYCGdnZ43j/fffV58DQEyePFn9OjU1VQAQK1asUJd99dVXwsHBQf06Pj5e2NjYiH/++UddtmPHDqFQKNS76zZv3lx8+eWXGvHMnDlThIWFCSGEWLp0qWjQoIG4ceOG+v3FixcLAOLIkSNCCCHi4uJE27ZtNe4xceJEAUD8+++/QgghVq1aJdzc3DRic3JyEoWFheqyd955R4SGhgohhCgsLBS2trbim2++Ub+fn58vnJycxLhx46r9LO9+zp0q4s7JyRFCCNGtWzeNezVp0kQMGjRI/To7O1sAEFOmTFGXVXzuFZ/f0KFDxWuvvabxnL179wqFQqH+zO6+r0qlEl5eXmLx4sVCCCHGjBkjnnjiCaFSqaqMG4DYvHmzEEKIZcuWCXd3d1FUVKR+f9u2bUKhUAilUimEuP3nqUmTJqKsrEx9zksvvSQiIyOrvP/d7v5ciMj4OEaGLN7jjz+OxYsXa5TdPYajQ4cO6p+9vb0BAO3bt9cou3nzJgoLC+Hq6goAaNy4Mfz8/NTnhIWFQaVSISMjA3Xr1sW5c+cwdOhQDB8+XH1OWVmZunvh5MmT6NChAxwcHDTucaeTJ08iNDRUo+zuc6oSEBCAunXrql/7+voiNzcXAHD+/HncunULXbp0Ub/v5uaGVq1a3fe+1RH/34oiSVK152jzGQNAbm4ufHx8cPToURw7dgxffPGFxnNUKhUuXLiANm3aVLpvRddXRV1fffVVPPXUU2jVqhWeeeYZPPvss3j66aerjO/kyZPo2LEjnJ2d1WUPP/yw+ndaEd8DDzwAGxsb9Tm+vr44fvz4vT4eIjIhJjJk8ZydnREYGHjPc2xtbdU/V3wZV1WmUqm0embFuIqkpKRKicidX4LGcmfswO34tY1djpMnT8LV1RUNGjTQKiZtPuOioiK8/vrrGDt2bKV7NW7cuMr7Vtyn4h6dO3fGhQsXsGPHDvz88894+eWXER4ejo0bN+paRa2eR0Tmh2NkiKqRmZmJ//3vf+rXv//+OxQKBVq1agVvb280bNgQ58+fR2BgoMbRtGlTAECbNm1w7Ngx3Lx5U+Med2rTpg0OHjyoUXb3Obpq1qwZbG1t8ccff6jLCgoKZE/hzs3NxZdffom+fftCoTDcPxmdO3fGX3/9VenzCwwMhJ2dndb3cXV1RWRkJJKSkrBhwwZ8++23uHr1aqXz2rRpg6NHj2oMJv7tt9/Uv1MiskxMZMjilZSUQKlUahyXL1/W+74ODg6Ijo7G0aNHsXfvXowdOxYvv/yyekrv9OnTkZCQgE8//RSnT5/G8ePHsWrVKsybNw8AMGDAAEiShOHDh+Ovv/7C9u3b8fHHH2s8Y8SIEThz5gzeeecdZGRk4Msvv9R7YGndunURHR2Nd955B7t27cKff/6JoUOHQqFQ3LNrCLjdtaNUKpGdnY2TJ09i5cqV6Nq1K9zc3DQGExvCxIkTsX//fowePRrp6ek4c+YMvvvuu0qDb+9l3rx5+Oqrr3Dq1CmcPn0a33zzDXx8fKpcQHDgwIHq3+mJEyewa9cujBkzBoMHD1Z3K8mVnp6O9PR0FBUVIS8vD+np6fjrr7/0uicRaYddS2TxkpOT4evrq1HWqlUrnDp1Sq/7BgYG4oUXXkDPnj1x9epVPPvssxrTq4cNGwYnJyd89NFHeOedd+Ds7Iz27durpyW7uLjg+++/x4gRI9CpUye0bdsWH3zwAfr166e+R+PGjfHtt99i/PjxWLBgAbp06aKebqyPefPmYcSIEXj22Wfh6uqKCRMmICsrS2O8TlUKCwvh6+sLSZLg6uqKVq1aITo6GuPGjVOPHTKUDh06YM+ePXjvvffw6KOPQgiB5s2bIzIyUut71K1bFx9++CHOnDkDGxsbPPjgg9i+fXuVLUdOTk7YuXMnxo0bhwcffBBOTk7o16+fOvHUR6dOndQ/p6Wl4csvv0STJk0qTaUnIsOTRMUoPiKqtYqLi+Hn54e5c+di6NChpg6HiMhg2CJDVAsdOXIEp06dQpcuXVBQUIAZM2YAAJ577jkTR0ZEZFhMZIhqqY8//hgZGRmws7NDcHAw9u7dCw8PD1OHRURkUOxaIiIiIovFWUtERERksZjIEBERkcViIkNEREQWi4kMERERWSwmMkRERGSxmMgQERGRxWIiQ0RERBaLiQwRERFZLCYyREREZLH+Dwf8t3NcZAHOAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get node embeddings (output of the first GCN layer)\n",
        "# We use torch.no_grad() to disable gradient tracking for this part of the code.\n",
        "# This is because we are not updating the model weights here; we only need the output for visualization.\n",
        "with torch.no_grad():\n",
        "    # Pass the input features and adjacency matrix through the first GCN layer of the two-layer GCN model.\n",
        "    # This will generate the node embeddings after the first graph convolution operation.\n",
        "    embeddings = two_layer_gcn.gcn1(node_features_tensor, adj_matrix_tensor)\n",
        "\n",
        "    # Convert the embeddings tensor to a numpy array for easier plotting with matplotlib.\n",
        "    embeddings = embeddings.numpy()\n",
        "\n",
        "# Visualize the embeddings using a scatter plot\n",
        "# Each node will be represented as a point in a 2D space, with colors representing their labels.\n",
        "plt.scatter(embeddings[:, 0], embeddings[:, 1], c=node_labels, cmap=\"coolwarm\", s=100)\n",
        "\n",
        "# Set plot title and axis labels\n",
        "plt.title(\"Node Embeddings\")  # Title of the plot\n",
        "plt.xlabel(\"Embedding Dimension 1\")  # Label for the x-axis (first dimension of the embeddings)\n",
        "plt.ylabel(\"Embedding Dimension 2\")  # Label for the y-axis (second dimension of the embeddings)\n",
        "\n",
        "# Add a color bar to indicate node labels\n",
        "# The color bar will show the mapping between colors and node labels (if labels are provided).\n",
        "plt.colorbar(label=\"Node Labels\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "1. **Embedding Separation**:\n",
        "   - The nodes in the graph are mapped to a 2D space where nodes with similar labels (as indicated by colors) tend to be closer together.\n",
        "   - This suggests that the GCN layer has successfully learned to distinguish nodes based on their label information, indicating that it captures some meaningful patterns in the graph structure.\n",
        "\n",
        "2. **Clusters Based on Labels**:\n",
        "   - We observe two main color groups in the plot: blue (label close to 0) and red (label close to 1).\n",
        "   - The nodes labeled in blue occupy the left side of the plot, while the nodes labeled in red occupy the right side.\n",
        "   - This clear separation implies that the GCN is able to group nodes with similar characteristics, which is promising for tasks like classification, where we want nodes with similar labels to be close in the embedding space.\n",
        "\n",
        "3. **Distinctness of Groups**:\n",
        "   - The embeddings are spread out along both the x and y axes, which indicates that each dimension is contributing to distinguishing the nodes.\n",
        "   - The presence of both distinct clusters and intermediate points (e.g., nodes that are not perfectly clustered) suggests that there may be some overlap or shared characteristics between nodes of different labels.\n",
        "\n",
        "### Insights\n",
        "1. **Effectiveness of GCN in Capturing Local Graph Structure**:\n",
        "   - The GCN layer effectively learns to differentiate nodes by aggregating information from neighbors. The separation of blue and red clusters implies that nodes of similar types or labels influence each other through the graph structure.\n",
        "\n",
        "2. **Potential for Node Classification**:\n",
        "   - Since the GCN is clustering nodes with the same labels, this embedding could be used as a basis for a classification model. Nodes with unknown labels could potentially be classified by checking which cluster they are closest to in this embedding space.\n",
        "\n",
        "3. **Areas for Improvement**:\n",
        "   - While there is some separation, there is also a degree of overlap or intermediate points. Additional layers, more training, or hyperparameter tuning might further improve separation between classes.\n",
        "   - Alternatively, a higher-dimensional embedding could be used if further separation is desired, especially in more complex or densely connected graphs.\n"
      ],
      "metadata": {
        "id": "sE8goUlafGqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 48: Graph Pooling with Mean Pooling\n",
        "Mean pooling is another common pooling method that averages the node features.\n"
      ],
      "metadata": {
        "id": "lLNIjc6r4B2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "QfDFP0oR4B2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcb0b93-8ca4-44d2-ccfa-a26612078bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Features:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Graph-level Representation (Mean Pooling): tensor([4., 5.])\n"
          ]
        }
      ],
      "source": [
        "# Display the initial node features\n",
        "print(\"Node Features:\\n\", node_features_tensor)\n",
        "\n",
        "# Mean Pooling: Calculate the average feature vector across all nodes\n",
        "# This approach is often used to create a single, unified representation for the entire graph.\n",
        "# By averaging the feature vectors of all nodes, we capture a general representation that summarizes the graph as a whole.\n",
        "\n",
        "# Calculate the mean of node features along the 0th dimension (rows)\n",
        "# This means we're taking the average value for each feature across all nodes.\n",
        "graph_representation = node_features_tensor.mean(dim=0)\n",
        "\n",
        "# Print the graph-level representation obtained through mean pooling\n",
        "print(\"Graph-level Representation (Mean Pooling):\", graph_representation)\n",
        "# Example output (if the mean was calculated):\n",
        "# tensor([4., 5.])  # This represents the average features for the graph as a whole\n",
        "\n",
        "# Explanation:\n",
        "# - Here, `dim=0` specifies that we want to calculate the mean across rows (i.e., across nodes),\n",
        "#   which gives us a single feature vector that represents the entire graph.\n",
        "# - If `node_features_tensor` has shape `(num_nodes, num_features)`, the result of mean pooling\n",
        "#   will be a tensor of shape `(num_features,)`, providing one averaged feature value for each feature dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Insight:\n",
        " - This graph-level representation is useful for graph classification tasks, where we need to classify the entire graph instead of individual nodes.\n",
        " - By averaging the features of all nodes, we obtain a compact summary that can be used as input for a classification model.\n",
        "\n",
        "#### Practical Example:\n",
        " - Imagine each node represents a molecule's atom, and each feature represents some property of the atom (like charge).\n",
        " - Mean pooling allows us to create a single vector summarizing the whole molecule's properties, which we could use  to classify the molecule as toxic, reactive, etc."
      ],
      "metadata": {
        "id": "6RWGKpxygJl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 49: Graph Classification Task Setup\n",
        " For graph classification, we can use pooling layers to aggregate node-level embeddings into a graph-level representation.\n"
      ],
      "metadata": {
        "id": "3r0YRqYt4B7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "nhJr7_lt4B7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8524c538-b475-44de-f363-22b6ca57bd41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Classifier Model created.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# After training a GNN on node-level embeddings, apply sum or mean pooling for a graph-level task.\n",
        "class GraphClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphClassifier, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(2, 4)\n",
        "        self.gcn2 = GraphConvolution(4, 2)\n",
        "        self.fc = nn.Linear(2, 1)  # Final layer for binary classification\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.gcn1(x, adj)\n",
        "        x = torch.relu(x)\n",
        "        x = self.gcn2(x, adj)\n",
        "        x = x.mean(dim=0)  # Mean pooling for graph-level representation\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the graph classifier\n",
        "graph_classifier = GraphClassifier()\n",
        "print(\"Graph Classifier Model created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration 50: Training Graph Classifier Model\n",
        " We’ll train the graph classifier model on a simple binary classification task.\n"
      ],
      "metadata": {
        "id": "YezxJGfV4B_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "otoloJXg4B_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e72062d-2541-4739-fc5f-2eb39de63ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5365514159202576\n",
            "Epoch 2, Loss: 0.5244860649108887\n",
            "Epoch 3, Loss: 0.5124693512916565\n",
            "Epoch 4, Loss: 0.5005095601081848\n",
            "Epoch 5, Loss: 0.4886152744293213\n",
            "Epoch 6, Loss: 0.47679460048675537\n",
            "Epoch 7, Loss: 0.46505600214004517\n",
            "Epoch 8, Loss: 0.45340776443481445\n",
            "Epoch 9, Loss: 0.4418579339981079\n",
            "Epoch 10, Loss: 0.4304148852825165\n"
          ]
        }
      ],
      "source": [
        "# Define label for the graph (e.g., 1 for \"positive class\")\n",
        "# The label should be a tensor with shape [1] to match the output of the model\n",
        "graph_label = torch.tensor([1.0])  # Wrap the label in a list to create a tensor with shape [1]\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(graph_classifier.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop for 10 epochs\n",
        "for epoch in range(10):\n",
        "    # Forward pass through the graph classifier\n",
        "    output = graph_classifier(node_features_tensor, adj_matrix_tensor)\n",
        "\n",
        "    # Calculate loss between the output and graph label\n",
        "    loss = loss_fn(output, graph_label)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBBIuCL-gdrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}