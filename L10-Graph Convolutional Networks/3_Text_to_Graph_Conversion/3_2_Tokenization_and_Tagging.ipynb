{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3.2: Tokenization and Tagging\n",
        "\n",
        "In this section, we will explore the process of tokenization and POS (Part-of-Speech) tagging as foundational steps for creating GCN-ready NLP data. Tokenization splits text into words or phrases (tokens), while POS tagging assigns syntactic roles to each token. Both of these steps are crucial for constructing syntactic trees and graphs, which serve as input to GCN models in NLP.\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "1. **Understanding Tokenization in NLP**\n",
        "2. **POS Tagging for Syntactic Information**\n",
        "3. **Implementing Tokenization and POS Tagging with NLTK**\n",
        "4. **Using spaCy for Advanced Parsing**\n",
        "5. **Code Walkthrough**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "T6JMzohNKNDE"
      },
      "id": "T6JMzohNKNDE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Understanding Tokenization in NLP\n",
        "\n",
        "- **Definition**: Tokenization is the process of splitting text into smaller units, typically words or subwords, known as tokens.\n",
        "- **Importance**: Tokenization is the first step in text preprocessing, as it breaks down the text for further analysis, including POS tagging and dependency parsing.\n",
        "- **Types of Tokenization**:\n",
        "  - **Word Tokenization**: Splits text into individual words.\n",
        "  - **Subword Tokenization**: Breaks down words into smaller meaningful units (used in transformer models).\n",
        "  - **Sentence Tokenization**: Splits text into sentences.\n",
        "\n",
        "**Example**:\n",
        "  - Text: “The cat sat on the mat.”\n",
        "  - Tokens: `[‘The’, ‘cat’, ‘sat’, ‘on’, ‘the’, ‘mat’]`\n"
      ],
      "metadata": {
        "id": "jfHTkOxIKNFu"
      },
      "id": "jfHTkOxIKNFu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Basic Tokenization with NLTK\n",
        "\n"
      ],
      "metadata": {
        "id": "parMfO6TKNIf"
      },
      "id": "parMfO6TKNIf"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# Sample text for tokenization\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize the sentence into individual words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Display the tokenized words\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4FE-1XsKqYa",
        "outputId": "a3b4d155-f5c4-49f0-e0d1-81213b770f88"
      },
      "id": "f4FE-1XsKqYa",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "- Tokenization: This code splits the text into individual words, known as tokens, which are the basic units for further analysis such as POS tagging, parsing, or feature extraction. Tokenizing at the word level is essential in NLP tasks as it allows each word to be processed individually, laying the foundation for text analysis."
      ],
      "metadata": {
        "id": "J8SERh4tKtv9"
      },
      "id": "J8SERh4tKtv9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. POS Tagging for Syntactic Information\n",
        "\n",
        "- **Definition**: POS tagging assigns syntactic roles to each token in a sentence (e.g., noun, verb, adjective).\n",
        "- **Purpose**: POS tags provide grammatical information that is essential for dependency parsing and building a syntactic tree for GCNs.\n",
        "- **Common POS Tags**:\n",
        "  - **NN**: Noun (e.g., cat, dog)\n",
        "  - **VB**: Verb (e.g., run, sit)\n",
        "  - **JJ**: Adjective (e.g., big, small)\n",
        "\n",
        "**Example**:\n",
        "  - Tokens: `[‘The’, ‘cat’, ‘sat’, ‘on’, ‘the’, ‘mat’]`\n",
        "  - POS Tags: `[('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]`\n"
      ],
      "metadata": {
        "id": "RNI1_vtHKNLS"
      },
      "id": "RNI1_vtHKNLS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: POS Tagging with NLTK\n",
        "\n"
      ],
      "metadata": {
        "id": "3aZJ1qetKNOM"
      },
      "id": "3aZJ1qetKNOM"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "# Perform POS tagging on the tokenized words\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Display each word with its corresponding POS tag\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAy5tYquKzSr",
        "outputId": "69354986-1ed7-4366-c96e-d0b7b28d0f3a"
      },
      "id": "WAy5tYquKzSr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **POS Tagging**: Assigns a part-of-speech (POS) tag to each token, indicating its grammatical role (e.g., noun, verb).\n",
        "- **Usage**: POS tags help in understanding sentence structure, which is essential for dependency parsing, entity recognition, and building dependency relations for GCN processing in NLP."
      ],
      "metadata": {
        "id": "0VsQ6I80Kzsz"
      },
      "id": "0VsQ6I80Kzsz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Implementing Tokenization and POS Tagging with NLTK\n",
        "\n",
        "We can combine tokenization and POS tagging to prepare text data for graph-based representation.\n"
      ],
      "metadata": {
        "id": "lsIh_m_bKNQ3"
      },
      "id": "lsIh_m_bKNQ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Tokenization and POS Tagging Pipeline\n"
      ],
      "metadata": {
        "id": "6KW7ei42KNTu"
      },
      "id": "6KW7ei42KNTu"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample sentence for tokenization and POS tagging\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence into individual words\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)  # Display tokenized words\n",
        "\n",
        "# Step 2: Perform POS Tagging to assign a syntactic role to each token\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)  # Display each word with its POS tag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-GBhfy4K_ip",
        "outputId": "f3b8ae13-58c0-4cc3-91d7-90b3e5dbca23"
      },
      "id": "Y-GBhfy4K_ip",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
            "POS Tags: [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Tokenization**: Breaks the sentence into words, preparing each token for further analysis.\n",
        "- **POS Tagging**: Assigns a syntactic role (e.g., noun, verb) to each token, providing information on grammatical structure.\n",
        "- **Usage**: The POS tags help in syntactic parsing, facilitating the creation of an adjacency matrix in future steps by clarifying relationships between tokens for dependency analysis."
      ],
      "metadata": {
        "id": "eS1D-2GJK_4O"
      },
      "id": "eS1D-2GJK_4O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Using spaCy for Advanced Parsing\n",
        "\n",
        "While NLTK provides basic tokenization and POS tagging, **spaCy** is more advanced and includes built-in dependency parsing. Dependency parsing identifies syntactic relationships, such as subjects and objects, which is crucial for building adjacency matrices for GCNs.\n"
      ],
      "metadata": {
        "id": "fBqc-mf4KNWh"
      },
      "id": "fBqc-mf4KNWh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Install spaCy**:\n"
      ],
      "metadata": {
        "id": "WIpza-viKNZd"
      },
      "id": "WIpza-viKNZd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvBSGQMgLLqH",
        "outputId": "c542b061-a1a2-405b-bf84-bdc3eb35ef0f"
      },
      "id": "AvBSGQMgLLqH",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Using spaCy for Tokenization, POS Tagging, and Dependency Parsing**:\n"
      ],
      "metadata": {
        "id": "GdmGFhA6KNcN"
      },
      "id": "GdmGFhA6KNcN"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample sentence for analysis\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Perform tokenization, POS tagging, and dependency parsing\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}, Head: {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp6syK4HLBwx",
        "outputId": "14bfaf8d-0a03-4f3a-b2b7-54f6aeffb02b"
      },
      "id": "fp6syK4HLBwx",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The, POS: DET, Dependency: det, Head: cat\n",
            "Token: cat, POS: NOUN, Dependency: nsubj, Head: sat\n",
            "Token: sat, POS: VERB, Dependency: ROOT, Head: sat\n",
            "Token: on, POS: ADP, Dependency: prep, Head: sat\n",
            "Token: the, POS: DET, Dependency: det, Head: mat\n",
            "Token: mat, POS: NOUN, Dependency: pobj, Head: on\n",
            "Token: ., POS: PUNCT, Dependency: punct, Head: sat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Token**: Each individual word or phrase in the sentence, separated for analysis.\n",
        "- **POS (Part of Speech)**: The grammatical role of each token (e.g., noun, verb), helping identify the function of each word.\n",
        "- **Dependency**: The syntactic role of the token, such as subject or object, indicating how it relates to other words in the sentence.\n",
        "- **Head**: Identifies the \"parent\" word each token depends on, enabling the creation of dependency graphs, which are essential for visualizing syntactic structure in NLP."
      ],
      "metadata": {
        "id": "92SE9CR0LBYk"
      },
      "id": "92SE9CR0LBYk"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LX-ooJvBLCKC"
      },
      "id": "LX-ooJvBLCKC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2YsbgPxLCfS"
      },
      "id": "Q2YsbgPxLCfS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FL5ctFswLDIL"
      },
      "id": "FL5ctFswLDIL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Advantages of Using spaCy**:\n",
        "- **Built-in Dependency Parsing**: Provides syntactic relations for constructing adjacency matrices.\n",
        "- **Efficiency**: Faster and more accurate than NLTK for complex parsing tasks.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "wENsVVC5KNe_"
      },
      "id": "wENsVVC5KNe_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Code Walkthrough: Tokenization and Tagging Pipeline with spaCy\n",
        "\n",
        "Let’s combine the steps from tokenization, POS tagging, and dependency parsing using spaCy to prepare data for GCN processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ey6F2Q0KNh-"
      },
      "id": "1Ey6F2Q0KNh-"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a sentence for processing\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract information: tokens, POS tags, and dependency relationships\n",
        "tokens = [token.text for token in doc]\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "# Display the extracted information\n",
        "print(\"Tokens:\", tokens)               # List of tokens in the sentence\n",
        "print(\"POS Tags:\", pos_tags)            # List of tuples with token and POS tag\n",
        "print(\"Dependencies:\", dependencies)    # List of tuples with token, dependency role, and head word\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAlxK3NmLb8d",
        "outputId": "f49da42d-f316-484c-8969-edeaa97511cc"
      },
      "id": "wAlxK3NmLb8d",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
            "POS Tags: [('The', 'DET'), ('cat', 'NOUN'), ('sat', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('mat', 'NOUN'), ('.', 'PUNCT')]\n",
            "Dependencies: [('The', 'det', 'cat'), ('cat', 'nsubj', 'sat'), ('sat', 'ROOT', 'sat'), ('on', 'prep', 'sat'), ('the', 'det', 'mat'), ('mat', 'pobj', 'on'), ('.', 'punct', 'sat')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Tokens**: Extracted words that form the basic units of the sentence.\n",
        "- **POS Tags**: Each token’s syntactic role, which can serve as features for the GCN.\n",
        "- **Dependencies**: Pairs of related words (along with dependency type), essential for building an adjacency matrix in later steps.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RLNfbMVpKNlk"
      },
      "id": "RLNfbMVpKNlk"
    },
    {
      "metadata": {
        "id": "46c9790754b5adfe"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary and Key Takeaways\n",
        "\n",
        "- **Tokenization and POS Tagging**: Key steps in preparing text data, helping segment sentences and assign syntactic roles.\n",
        "- **Dependency Parsing with spaCy**: spaCy’s advanced parsing provides hierarchical syntactic structures, enabling the construction of dependency-based adjacency matrices.\n",
        "- **GCN-Ready NLP Data**: Tokenized and tagged data is now ready for further processing, like building adjacency matrices and extracting feature vectors for GCNs.\n",
        "\n",
        "With tokenization, tagging, and dependency parsing completed, we’re ready to proceed to the next step—creating adjacency matrices and feature representations that GCNs can process for NLP tasks."
      ],
      "id": "46c9790754b5adfe"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}