{
  "cells": [
    {
      "metadata": {
        "id": "853c4399a79b47e1"
      },
      "cell_type": "markdown",
      "source": [
        "### Section 3.1: NLP Tree Representation with GCNs\n",
        "\n",
        "In this section, we will explore how to represent natural language data as graphs that can be processed by Graph Convolutional Networks (GCNs). This involves converting sentences into tree structures, extracting features, and setting up adjacency matrices. We’ll use libraries like **NLTK** and **spaCy** for parsing and tree generation, which are essential steps for applying GCNs to NLP tasks.\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "1. **Introduction to NLP Graph Representation**\n",
        "2. **Converting Sentences to Dependency Trees**\n",
        "3. **Extracting Features from Parsed Trees**\n",
        "4. **Building the Adjacency Matrix**\n",
        "5. **Code Walkthrough**\n",
        "\n",
        "---\n"
      ],
      "id": "853c4399a79b47e1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Introduction to NLP Graph Representation\n",
        "\n",
        "- **Goal**: Represent text as a graph where each word or phrase becomes a node, and syntactic dependencies form the edges.\n",
        "- **Why Use Trees?**: NLP data has an inherent structure, such as dependency relations between words. Trees capture hierarchical relationships, which are useful for tasks like sentiment analysis, translation, and information extraction.\n",
        "\n",
        "**Example**: In the sentence, “The cat sat on the mat,”:\n",
        "  - **Nodes**: Each word is a node (e.g., “The,” “cat,” “sat”).\n",
        "  - **Edges**: Edges represent grammatical relationships, such as the subject (cat) linked to the verb (sat).\n"
      ],
      "metadata": {
        "id": "FoDDjLvK2c6M"
      },
      "id": "FoDDjLvK2c6M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Converting Sentences to Dependency Trees\n",
        "\n",
        "We’ll use **NLTK** for tokenizing and parsing sentences into syntactic trees.\n",
        "\n",
        "#### Steps:\n",
        "1. **Tokenize** the sentence to split it into words.\n",
        "2. **POS Tagging**: Apply Part-of-Speech (POS) tagging to each token.\n",
        "3. **Chunking**: Use chunking (phrase structure parsing) to convert the tagged tokens into a tree structure.\n",
        "4. **Dependency Parsing**: Establish dependency relations to build a dependency tree.\n"
      ],
      "metadata": {
        "id": "SnQVtfio2eI-"
      },
      "id": "SnQVtfio2eI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Sentence to Tree Representation using NLTK\n"
      ],
      "metadata": {
        "id": "EuYL_QtD2eNW"
      },
      "id": "EuYL_QtD2eNW"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Example sentence for processing\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence into individual words\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)  # Output each token for verification\n",
        "\n",
        "# Step 2: Perform POS Tagging to assign each word a syntactic role\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)  # Display the word and its associated POS tag\n",
        "\n",
        "# Step 3: Named Entity Chunking to group words into meaningful entities\n",
        "# This creates a tree structure with entities based on their POS tags\n",
        "tree = ne_chunk(pos_tags)\n",
        "print(\"Tree Structure:\", tree)  # Shows the hierarchical structure of entities\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962-sNWlJIBf",
        "outputId": "0c2059f7-2fd8-4ea9-8d51-bf9b70f76472"
      },
      "id": "962-sNWlJIBf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
            "POS Tags: [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n",
            "Tree Structure: (S The/DT cat/NN sat/VBD on/IN the/DT mat/NN ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "1. **POS Tagging**:\n",
        "   - Assigns a syntactic role to each word, which is essential for understanding relationships and dependencies.\n",
        "   \n",
        "2. **Named Entity Chunking**:\n",
        "   - Organizes words into a hierarchical tree based on POS tags, identifying entities or groups of related words.\n",
        "   - This step is particularly valuable for GCNs in NLP, as it captures dependency structures useful for relational understanding."
      ],
      "metadata": {
        "id": "sS78RFgPJIXh"
      },
      "id": "sS78RFgPJIXh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Extracting Features from Parsed Trees\n",
        "\n",
        "After converting the sentence to a tree structure, we need to extract features that represent each node (word).\n"
      ],
      "metadata": {
        "id": "RYhDN_c82eRj"
      },
      "id": "RYhDN_c82eRj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Node Features:\n",
        "- **One-Hot Encoding**: Each word is represented by a unique vector where only one element is “1” (e.g., the word “cat” as `[0, 1, 0, ...]`).\n",
        "- **POS Tags**: Use POS tags as features for each word.\n",
        "- **Word Embeddings**: Optional, using pre-trained embeddings (like Word2Vec or GloVe) for richer representations.\n"
      ],
      "metadata": {
        "id": "S8uDqo912eWI"
      },
      "id": "S8uDqo912eWI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Feature Extraction\n"
      ],
      "metadata": {
        "id": "yQSbfUdB2ebK"
      },
      "id": "yQSbfUdB2ebK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple vocabulary for one-hot encoding\n",
        "vocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"] # Add the period to the vocabulary\n",
        "# Create a dictionary that maps each word to a unique index\n",
        "vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# One-hot encode tokens based on the vocabulary\n",
        "one_hot_features = []\n",
        "for token in tokens:\n",
        "    # Initialize a zero vector with the length of the vocabulary\n",
        "    feature = [0] * len(vocab)\n",
        "    # Set the index corresponding to the token to 1\n",
        "    # Check if the token exists in the vocab before accessing it\n",
        "    if token in vocab_dict:\n",
        "        feature[vocab_dict[token]] = 1\n",
        "    # Append the one-hot encoded feature vector for each token\n",
        "    one_hot_features.append(feature)\n",
        "\n",
        "# Print the one-hot encoded features for each token in the sentence\n",
        "print(\"One-Hot Features:\", one_hot_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS7mwAKHJUIr",
        "outputId": "c1cd4f8e-f211-4a1f-a621-2b51426455b3"
      },
      "id": "xS7mwAKHJUIr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Features: [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Explanation:\n",
        "1. **Vocabulary Dictionary**:\n",
        "   - Maps each unique word to an index for one-hot encoding.\n",
        "   \n",
        "2. **One-Hot Encoding**:\n",
        "   - Creates a vector where only the index of the current token is set to `1`.\n",
        "   - Each token in the sentence is transformed into a one-hot encoded vector based on the predefined vocabulary.\n",
        "\n",
        "3. **Output**:\n",
        "   - Displays the one-hot encoded representation for each word in `tokens`, allowing numerical processing in models."
      ],
      "metadata": {
        "id": "Vp6Zx6CdJiXL"
      },
      "id": "Vp6Zx6CdJiXL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Building the Adjacency Matrix\n",
        "\n",
        "The **adjacency matrix** represents relationships between words in the tree. Each entry \\( A_{ij} \\) is “1” if there’s a connection between word \\( i \\) and word \\( j \\) (e.g., based on dependencies).\n"
      ],
      "metadata": {
        "id": "fp_nZPEs2edQ"
      },
      "id": "fp_nZPEs2edQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Steps to Create the Adjacency Matrix:\n",
        "1. Use dependency parsing to determine relationships between words.\n",
        "2. Create a square matrix where rows and columns correspond to words.\n",
        "3. Populate the matrix based on dependency relations from the parsed tree.\n"
      ],
      "metadata": {
        "id": "OqQ7BjPs2eel"
      },
      "id": "OqQ7BjPs2eel"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Building an Adjacency Matrix\n"
      ],
      "metadata": {
        "id": "1tk69LXg2ega"
      },
      "id": "1tk69LXg2ega"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize an empty adjacency matrix for the sentence with 6 tokens\n",
        "adj_matrix = np.zeros((len(tokens), len(tokens)), dtype=int)\n",
        "\n",
        "# Define dependencies manually as (start_node, end_node) pairs\n",
        "# Each pair represents a dependency relationship between tokens (e.g., (0, 1) implies a link between token 0 and token 1)\n",
        "dependencies = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]  # Example dependencies for the sentence\n",
        "\n",
        "# Populate the adjacency matrix based on defined dependencies\n",
        "for i, j in dependencies:\n",
        "    adj_matrix[i][j] = 1  # Create a directed edge from node i to node j\n",
        "    adj_matrix[j][i] = 1  # Create a bidirectional edge for undirected graph representation\n",
        "\n",
        "# Print the adjacency matrix to visualize token dependencies\n",
        "print(\"Adjacency Matrix:\")\n",
        "print(adj_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAkSzKbfJlEv",
        "outputId": "2d948e86-1aa7-4ed9-8d8a-838694a5f9d2"
      },
      "id": "ZAkSzKbfJlEv",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency Matrix:\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 0 0 1 0 1 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Explanation:\n",
        "1. **Adjacency Matrix**:\n",
        "   - Initializes a zero matrix where each token is treated as a graph node.\n",
        "   \n",
        "2. **Dependencies**:\n",
        "   - Manually defines dependencies to simulate a syntactic structure, with each tuple `(i, j)` representing a connection between tokens.\n",
        "   \n",
        "3. **Bidirectional Edges**:\n",
        "   - Sets both `A[i][j]` and `A[j][i]` to 1 for undirected relationships, commonly used for symmetrical dependency representation.\n",
        "\n",
        "4. **Output**:\n",
        "   - Prints the adjacency matrix, displaying the token-to-token dependencies in matrix form."
      ],
      "metadata": {
        "id": "WX1n-iI2Jla-"
      },
      "id": "WX1n-iI2Jla-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Code Walkthrough: Putting It All Together\n",
        "\n",
        "Let’s combine the steps above into a full pipeline that takes a sentence, generates a tree, extracts features, and builds an adjacency matrix.\n"
      ],
      "metadata": {
        "id": "bN8j38Wx2eji"
      },
      "id": "bN8j38Wx2eji"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK resources (only needs to be run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def process_sentence(sentence):\n",
        "    # Step 1: Tokenize the sentence into individual words\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Step 2: Perform POS tagging to assign each word a syntactic role\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Step 3: Named Entity Chunking to group words into meaningful entities\n",
        "    tree = ne_chunk(pos_tags)\n",
        "\n",
        "    # Step 4: Create One-Hot Features for each token based on unique vocabulary\n",
        "    vocab = list(set(tokens))  # Identify unique words in the sentence\n",
        "    vocab_dict = {word: i for i, word in enumerate(vocab)}  # Map each word to a unique index\n",
        "    one_hot_features = []\n",
        "    for token in tokens:\n",
        "        # Create a one-hot encoded vector for each token\n",
        "        feature = [0] * len(vocab)\n",
        "        feature[vocab_dict[token]] = 1\n",
        "        one_hot_features.append(feature)\n",
        "\n",
        "    # Step 5: Define an Adjacency Matrix based on example dependencies\n",
        "    adj_matrix = np.zeros((len(tokens), len(tokens)), dtype=int)  # Initialize an empty matrix\n",
        "    dependencies = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]  # Manually define dependencies\n",
        "    for i, j in dependencies:\n",
        "        adj_matrix[i][j] = 1  # Set edge from node i to node j\n",
        "        adj_matrix[j][i] = 1  # Symmetric edge for undirected graph representation\n",
        "\n",
        "    # Output the processed components\n",
        "    return tokens, one_hot_features, adj_matrix\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "tokens, features, adj_matrix = process_sentence(sentence)\n",
        "\n",
        "# Display the results\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"One-Hot Features:\", features)\n",
        "print(\"Adjacency Matrix:\\n\", adj_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw-4T_ubJt1e",
        "outputId": "6d15bd16-8b2d-4ae8-d5d5-f094a33b357d"
      },
      "id": "Qw-4T_ubJt1e",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
            "One-Hot Features: [[0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0]]\n",
            "Adjacency Matrix:\n",
            " [[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 0 0 1 0 1 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary and Key Takeaways\n",
        "\n",
        "- **Tree Structure Representation**: Transforming sentences into syntactic trees captures linguistic relationships that can be leveraged by GCNs.\n",
        "- **Node Features**: Basic one-hot encoding is simple, but POS tags or word embeddings can enhance the feature space.\n",
        "- **Adjacency Matrix**: Constructed based on dependency relations, this matrix defines the connections that GCNs use for message passing.\n",
        "- **GCN-Ready Data**: After creating the feature matrix and adjacency matrix, the data is ready for GCN processing in subsequent sections.\n",
        "\n",
        "In the next section, we’ll apply the generated trees and adjacency matrices in a GCN model to explore how it can perform NLP tasks like sentence classification or translation."
      ],
      "metadata": {
        "id": "gFwhLHQP2enP"
      },
      "id": "gFwhLHQP2enP"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7JY2sY1i2erm"
      },
      "id": "7JY2sY1i2erm"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PPmNOFtC2evz"
      },
      "id": "PPmNOFtC2evz"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hsnesx-Z2eyN"
      },
      "id": "Hsnesx-Z2eyN"
    },
    {
      "metadata": {
        "id": "9e77d3c4e48862bb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "9e77d3c4e48862bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}