{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 6.2: Hyperparameter Tuning for Optimizing GCN Performance\n",
        "\n",
        "Hyperparameter tuning is essential for maximizing the performance of GCN models in NLP tasks. Given the variety of hyperparameters that affect GCN performance—such as learning rate, hidden dimension size, dropout rate, and number of layers—systematically experimenting with these parameters can lead to improved accuracy, faster convergence, and better generalization.\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "1. **Key Hyperparameters to Tune in GCNs**\n",
        "2. **Setting Up a Grid Search for Hyperparameter Tuning**\n",
        "3. **Using Random Search for Efficient Tuning**\n",
        "4. **Automated Tuning with Bayesian Optimization**\n",
        "5. **Analyzing Results and Selecting the Best Model**\n",
        "6. **Code Walkthrough**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FoDDjLvK2c6M"
      },
      "id": "FoDDjLvK2c6M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Key Hyperparameters to Tune in GCNs\n",
        "\n",
        "Here are the most important hyperparameters to tune when optimizing GCN models:\n",
        "\n",
        "- **Learning Rate**: Controls the step size for each update during optimization. A lower learning rate often leads to more stable training but requires more epochs.\n",
        "- **Hidden Dimension Size**: Determines the feature size in hidden layers, affecting the model’s capacity to capture information.\n",
        "- **Dropout Rate**: A regularization parameter that randomly deactivates nodes during training, helping to prevent overfitting.\n",
        "- **Number of Layers**: Defines the depth of the model. More layers allow nodes to aggregate information from further neighbors but may lead to over-smoothing if too many layers are used.\n",
        "- **Batch Size**: Relevant for larger datasets, where mini-batch training can speed up optimization and smooth gradients.\n"
      ],
      "metadata": {
        "id": "SnQVtfio2eI-"
      },
      "id": "SnQVtfio2eI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Setting Up a Grid Search for Hyperparameter Tuning\n",
        "\n",
        "A **grid search** is a systematic way to search through predefined values for each hyperparameter and evaluate each possible combination.\n"
      ],
      "metadata": {
        "id": "EuYL_QtD2eNW"
      },
      "id": "EuYL_QtD2eNW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code taken from the section 5.2, pre for this section codes"
      ],
      "metadata": {
        "id": "ItgXEv5O-ceH"
      },
      "id": "ItgXEv5O-ceH"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model for POS tagging and embeddings\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define vocabulary and one-hot encoding function\n",
        "vocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "def one_hot_encode(sentence_tokens, vocab_dict):\n",
        "    features = []\n",
        "    for token in sentence_tokens:\n",
        "        one_hot = [0] * len(vocab_dict)\n",
        "        if token in vocab_dict:\n",
        "            one_hot[vocab_dict[token]] = 1\n",
        "        features.append(one_hot)\n",
        "    return np.array(features)\n",
        "\n",
        "def pos_tag_features(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    unique_tags = list(set(pos_tags))\n",
        "    pos_dict = {tag: i for i, tag in enumerate(unique_tags)}\n",
        "    features = []\n",
        "    for tag in pos_tags:\n",
        "        one_hot = [0] * len(pos_dict)\n",
        "        one_hot[pos_dict[tag]] = 1\n",
        "        features.append(one_hot)\n",
        "    return np.array(features)\n",
        "\n",
        "def word_embedding_features(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    features = [token.vector for token in doc]\n",
        "    return np.array(features)\n",
        "\n",
        "def create_combined_features(sentence, vocab_dict):\n",
        "    doc = nlp(sentence)\n",
        "    sentence_tokens = [token.text for token in doc]\n",
        "    one_hot_feats = one_hot_encode(sentence_tokens, vocab_dict)\n",
        "    pos_feats = pos_tag_features(sentence)\n",
        "    embedding_feats = word_embedding_features(sentence)\n",
        "    combined_feats = np.concatenate((one_hot_feats, pos_feats, embedding_feats), axis=1)\n",
        "    return combined_feats\n",
        "\n",
        "\n",
        "# Create an adjacency matrix with self-loops for the sentence\n",
        "def create_adjacency_matrix_with_loops(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    num_tokens = len(doc)\n",
        "    adj_matrix = np.zeros((num_tokens, num_tokens), dtype=int)\n",
        "    for token in doc:\n",
        "        adj_matrix[token.i][token.head.i] = 1\n",
        "        adj_matrix[token.head.i][token.i] = 1\n",
        "    np.fill_diagonal(adj_matrix, 1)\n",
        "    return adj_matrix\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "combined_features = create_combined_features(sentence, vocab_dict)\n",
        "\n",
        "\n",
        "adj_matrix_with_loops = create_adjacency_matrix_with_loops(sentence)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "node_features = torch.tensor(combined_features, dtype=torch.float32)\n",
        "adj_matrix = torch.tensor(adj_matrix_with_loops, dtype=torch.float32)\n",
        "label = torch.tensor([1], dtype=torch.long)  # Example label (1 for positive, 0 for negative)\n",
        "\n",
        "def aggregate_nodes(node_outputs, method=\"mean\"):\n",
        "    if method == \"mean\":\n",
        "        # Mean pooling: averages all node embeddings, providing a balanced representation\n",
        "        return node_outputs.mean(dim=0, keepdim=True)\n",
        "    elif method == \"sum\":\n",
        "        # Sum pooling: sums all node embeddings, which can give more weight to longer sentences\n",
        "        return node_outputs.sum(dim=0, keepdim=True)\n",
        "    elif method == \"max\":\n",
        "        # Max pooling: selects the maximum value for each feature across all nodes\n",
        "        # Max returns a tuple (values, indices), so we take the values\n",
        "        return node_outputs.max(dim=0, keepdim=True)[0]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported aggregation method. Choose 'mean', 'sum', or 'max'.\")\n",
        "\n",
        "\n",
        "\n",
        "# Define a custom GCN Layer\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        # Apply linear transformation to node features\n",
        "        transformed_features = self.linear(node_features)\n",
        "\n",
        "        # Aggregate features from neighboring nodes using adjacency matrix\n",
        "        aggregated_features = torch.matmul(adj_matrix, transformed_features)\n",
        "\n",
        "        # Normalize by node degrees to handle varying numbers of neighbors\n",
        "        degree_matrix = adj_matrix.sum(dim=1, keepdim=True)\n",
        "        normalized_features = aggregated_features / degree_matrix\n",
        "\n",
        "        # Apply non-linearity\n",
        "        return F.relu(normalized_features)\n",
        "\n",
        "# Define the DeepGCNModel with variable layers\n",
        "class DeepGCNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        Initializes a GCN model with a configurable number of layers and dropout.\n",
        "\n",
        "        Parameters:\n",
        "        - input_dim (int): Dimension of input features for each node.\n",
        "        - hidden_dim (int): Dimension of hidden layer.\n",
        "        - output_dim (int): Dimension of the output layer (number of classes).\n",
        "        - num_layers (int): Number of GCN layers in the model.\n",
        "        - dropout_rate (float): Dropout rate to apply after each GCN layer.\n",
        "        \"\"\"\n",
        "        super(DeepGCNModel, self).__init__()\n",
        "\n",
        "        # Initialize the list of layers\n",
        "        layers = [GCNLayer(input_dim, hidden_dim)]\n",
        "\n",
        "        # Add intermediate layers based on num_layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers.append(GCNLayer(hidden_dim, hidden_dim))\n",
        "\n",
        "        # Final layer maps hidden_dim to output_dim for classification\n",
        "        layers.append(GCNLayer(hidden_dim, output_dim))\n",
        "\n",
        "        # Convert list of layers to nn.ModuleList for PyTorch compatibility\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, node_features, adj_matrix):\n",
        "        \"\"\"\n",
        "        Forward pass through the GCN model.\n",
        "\n",
        "        Parameters:\n",
        "        - node_features (Tensor): Input feature matrix for nodes.\n",
        "        - adj_matrix (Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "        - Tensor: Output embeddings for nodes.\n",
        "        \"\"\"\n",
        "        x = node_features\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x, adj_matrix)     # Pass through each layer\n",
        "            x = self.dropout(x)          # Apply dropout after each layer\n",
        "\n",
        "        # Final layer without dropout, as it's typically the output layer\n",
        "        x = self.layers[-1](x, adj_matrix)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define input and output dimensions based on your data\n",
        "input_dim = node_features.shape[1]  # Number of features per node\n",
        "output_dim = 2  # Number of output classes, adjust this based on your task\n",
        "\n"
      ],
      "metadata": {
        "id": "z45vOHYN-hYW"
      },
      "id": "z45vOHYN-hYW",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Setting Up a Grid Search\n",
        "\n"
      ],
      "metadata": {
        "id": "RYhDN_c82eRj"
      },
      "id": "RYhDN_c82eRj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()    # Loss function\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1],  # Different learning rates to test\n",
        "    \"hidden_dim\": [8, 16, 32],            # Different hidden dimensions for the model\n",
        "    \"dropout_rate\": [0.0, 0.3, 0.5],      # Different dropout rates to prevent overfitting\n",
        "    \"num_layers\": [2, 3]                  # Number of GCN layers in the model\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store results for each hyperparameter combination\n",
        "grid_results = {}\n",
        "\n",
        "# Generate all possible combinations of hyperparameters using ParameterGrid\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"\\nTesting configuration: {params}\")\n",
        "\n",
        "    # Step 1: Define the model based on the current set of parameters\n",
        "    # We use the specified hidden dimension and number of layers for the GCN model\n",
        "    model = DeepGCNModel(input_dim, params['hidden_dim'], output_dim, num_layers=params['num_layers'])\n",
        "\n",
        "    # Step 2: Set up the optimizer with the specified learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # Step 3: Apply the dropout rate to the model (if implemented in the model definition)\n",
        "    model.dropout_rate = params['dropout_rate']\n",
        "\n",
        "    epochs = 10  # Number of training epochs\n",
        "    # Step 4: Train the model (simplified training loop for demonstration)\n",
        "    # The training loop iterates over a fixed number of epochs to minimize loss\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass: Get node embeddings from the GCN model\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "        # Aggregate node embeddings to get a sentence-level representation\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Compute the loss using the criterion (e.g., CrossEntropyLoss)\n",
        "        loss = criterion(sentence_output, label)\n",
        "\n",
        "        # Backward pass: Perform backpropagation and optimization step\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    # Step 5: Evaluate the model after training\n",
        "    # Perform a forward pass to get the final outputs after training\n",
        "    with torch.no_grad():\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Get the predicted label (class with the highest score)\n",
        "        _, predicted = torch.max(sentence_output, dim=1)\n",
        "\n",
        "        # Store the loss and prediction in the results dictionary\n",
        "        # The hyperparameter combination is used as the key\n",
        "        grid_results[tuple(params.items())] = {\"Loss\": loss.item(), \"Prediction\": predicted.item()}\n",
        "\n",
        "        # Print the loss and predicted label for this configuration\n",
        "        print(f\"Loss: {loss.item():.4f}, Prediction: {predicted.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah5YU2ex9T4Y",
        "outputId": "1385add2-18c2-4c5d-fa55-33be1c0be7a7"
      },
      "id": "ah5YU2ex9T4Y",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.6853, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6940, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.1041, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.5571, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.6420, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.0210, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.3989, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6733, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.0006, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.0, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.6093, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.1636, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.0461, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.5804, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.0011, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.5301, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.5872, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.0001, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.3, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.5851, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.7702, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 8, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.6236, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.0220, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.1538, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 16, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 2}\n",
            "Loss: 0.5849, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.001, 'num_layers': 3}\n",
            "Loss: 0.5977, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 2}\n",
            "Loss: 0.0051, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.01, 'num_layers': 3}\n",
            "Loss: 0.0159, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 2}\n",
            "Loss: 0.0000, Prediction: 1\n",
            "\n",
            "Testing configuration: {'dropout_rate': 0.5, 'hidden_dim': 32, 'learning_rate': 0.1, 'num_layers': 3}\n",
            "Loss: 0.0000, Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Using Random Search for Efficient Tuning\n",
        "\n",
        "Random search samples random combinations of hyperparameters from the search space, providing a faster alternative to grid search, especially when the parameter space is large.\n"
      ],
      "metadata": {
        "id": "S8uDqo912eWI"
      },
      "id": "S8uDqo912eWI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Random Search\n"
      ],
      "metadata": {
        "id": "yQSbfUdB2ebK"
      },
      "id": "yQSbfUdB2ebK"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define ranges for each hyperparameter\n",
        "param_ranges = {\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1],  # Possible learning rates to explore\n",
        "    \"hidden_dim\": [8, 16, 32],            # Possible dimensions for hidden layers\n",
        "    \"dropout_rate\": [0.0, 0.3, 0.5],      # Different dropout rates to reduce overfitting\n",
        "    \"num_layers\": [2, 3]                  # Number of GCN layers to experiment with\n",
        "}\n",
        "\n",
        "# Specify the number of random configurations to sample\n",
        "n_samples = 5\n",
        "\n",
        "# Dictionary to store results for each sampled configuration\n",
        "random_results = {}\n",
        "\n",
        "# Loop to generate and test random configurations\n",
        "for _ in range(n_samples):\n",
        "    # Randomly select a value for each hyperparameter\n",
        "    params = {key: random.choice(values) for key, values in param_ranges.items()}\n",
        "    print(f\"\\nTesting configuration: {params}\")\n",
        "\n",
        "    # Step 1: Define the model with the randomly sampled parameters\n",
        "    model = DeepGCNModel(input_dim, params['hidden_dim'], output_dim, num_layers=params['num_layers'])\n",
        "\n",
        "    # Step 2: Set up the optimizer using the selected learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # Step 3: Apply the dropout rate to the model if it has a dropout layer\n",
        "    model.dropout_rate = params['dropout_rate']\n",
        "\n",
        "    # Step 4: Train the model (simplified training loop)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass: Calculate node embeddings using the GCN model\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "        # Aggregate node embeddings to create a sentence-level representation\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Compute the loss for the sentence representation\n",
        "        loss = criterion(sentence_output, label)\n",
        "\n",
        "        # Backward pass: Perform backpropagation and update the model parameters\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "    # Step 5: Evaluate the model after training\n",
        "    with torch.no_grad():\n",
        "        # Forward pass to get the final node embeddings after training\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Get the predicted label by finding the class with the highest score\n",
        "        _, predicted = torch.max(sentence_output, dim=1)\n",
        "\n",
        "        # Store the loss and prediction for this configuration in random_results\n",
        "        random_results[tuple(params.items())] = {\"Loss\": loss.item(), \"Prediction\": predicted.item()}\n",
        "\n",
        "        # Print the loss and predicted label for this configuration\n",
        "        print(f\"Loss: {loss.item():.4f}, Prediction: {predicted.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu5Tq54I9lw_",
        "outputId": "99c50db5-6cb7-43e6-b4ea-0272bc2a19be"
      },
      "id": "vu5Tq54I9lw_",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing configuration: {'learning_rate': 0.1, 'hidden_dim': 8, 'dropout_rate': 0.0, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'learning_rate': 0.001, 'hidden_dim': 16, 'dropout_rate': 0.3, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'learning_rate': 0.01, 'hidden_dim': 8, 'dropout_rate': 0.5, 'num_layers': 2}\n",
            "Loss: 0.0748, Prediction: 1\n",
            "\n",
            "Testing configuration: {'learning_rate': 0.1, 'hidden_dim': 16, 'dropout_rate': 0.0, 'num_layers': 2}\n",
            "Loss: 0.6931, Prediction: 0\n",
            "\n",
            "Testing configuration: {'learning_rate': 0.01, 'hidden_dim': 32, 'dropout_rate': 0.5, 'num_layers': 3}\n",
            "Loss: 0.6931, Prediction: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 4. Automated Tuning with Bayesian Optimization\n",
        "\n",
        "Bayesian Optimization is a more sophisticated method that models the relationship between hyperparameters and model performance to intelligently explore the search space. Libraries such as **Hyperopt** or **Optuna** are commonly used for this approach."
      ],
      "metadata": {
        "id": "fp_nZPEs2edQ"
      },
      "id": "fp_nZPEs2edQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Code Example: Hyperparameter Tuning with Optuna (if library is installed)\n"
      ],
      "metadata": {
        "id": "OqQ7BjPs2eel"
      },
      "id": "OqQ7BjPs2eel"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGglR0oZAJxt",
        "outputId": "f4affece-3399-4bb6-c4d0-05c22447c8b6"
      },
      "id": "BGglR0oZAJxt",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# Define the objective function that Optuna will use to optimize the hyperparameters\n",
        "def objective(trial):\n",
        "    # Step 1: Suggest values for each hyperparameter from specified ranges\n",
        "    # Log-uniform sampling for learning rate: values between 1e-4 and 1e-1\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
        "\n",
        "    # Categorical choice for hidden dimensions: selects either 8, 16, or 32\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [8, 16, 32])\n",
        "\n",
        "    # Categorical choice for dropout rate: selects either 0.0, 0.3, or 0.5\n",
        "    dropout_rate = trial.suggest_categorical('dropout_rate', [0.0, 0.3, 0.5])\n",
        "\n",
        "    # Integer choice for the number of GCN layers: selects either 2 or 3\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
        "\n",
        "    # Step 2: Define the model with the suggested hyperparameters\n",
        "    model = DeepGCNModel(input_dim, hidden_dim, output_dim, num_layers=num_layers)\n",
        "\n",
        "    # Define the optimizer with the suggested learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Apply the suggested dropout rate to the model\n",
        "    model.dropout_rate = dropout_rate\n",
        "\n",
        "    # Step 3: Training loop (simplified version for demonstration)\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass: Generate node embeddings\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "        # Aggregate node embeddings to get a sentence-level representation\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Compute the loss for the sentence representation\n",
        "        loss = criterion(sentence_output, label)\n",
        "\n",
        "        # Backward pass: Calculate gradients and update model parameters\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Calculate gradients\n",
        "        optimizer.step()       # Update parameters\n",
        "\n",
        "    # Step 4: Return the final loss for this configuration as the metric to minimize\n",
        "    return loss.item()\n",
        "\n",
        "# Run the optimization process with Optuna\n",
        "# \"direction='minimize'\" tells Optuna to minimize the objective function (i.e., minimize the loss)\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize the objective function across a specified number of trials\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print out the best hyperparameters and the corresponding loss found during the optimization\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "print(\"Best loss:\", study.best_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIq-8ra9nkc",
        "outputId": "b85f97e7-4759-488a-93db-427797fda21e"
      },
      "id": "bYIq-8ra9nkc",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-06 01:04:57,257] A new study created in memory with name: no-name-1d174447-4c80-4f77-b08e-5da2e3ef7742\n",
            "<ipython-input-17-2842c8c02b84>:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)\n",
            "[I 2024-11-06 01:04:57,288] Trial 0 finished with value: 3.814689989667386e-06 and parameters: {'learning_rate': 0.05843969270250119, 'hidden_dim': 8, 'dropout_rate': 0.0, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,360] Trial 1 finished with value: 0.7060726881027222 and parameters: {'learning_rate': 0.0009953093439217103, 'hidden_dim': 8, 'dropout_rate': 0.3, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,440] Trial 2 finished with value: 0.00036221143091097474 and parameters: {'learning_rate': 0.01764639128352245, 'hidden_dim': 16, 'dropout_rate': 0.3, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,503] Trial 3 finished with value: 0.6931471824645996 and parameters: {'learning_rate': 0.003366712157811969, 'hidden_dim': 32, 'dropout_rate': 0.0, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,537] Trial 4 finished with value: 0.5575941205024719 and parameters: {'learning_rate': 0.0023631513169337483, 'hidden_dim': 8, 'dropout_rate': 0.5, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,581] Trial 5 finished with value: 0.6826927661895752 and parameters: {'learning_rate': 0.00010893470779865641, 'hidden_dim': 32, 'dropout_rate': 0.5, 'num_layers': 3}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,623] Trial 6 finished with value: 0.00831972062587738 and parameters: {'learning_rate': 0.02645882273666691, 'hidden_dim': 8, 'dropout_rate': 0.5, 'num_layers': 3}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,655] Trial 7 finished with value: 0.6945515871047974 and parameters: {'learning_rate': 0.0001872855457130569, 'hidden_dim': 16, 'dropout_rate': 0.0, 'num_layers': 2}. Best is trial 0 with value: 3.814689989667386e-06.\n",
            "[I 2024-11-06 01:04:57,689] Trial 8 finished with value: 0.0 and parameters: {'learning_rate': 0.09648664145833907, 'hidden_dim': 16, 'dropout_rate': 0.0, 'num_layers': 2}. Best is trial 8 with value: 0.0.\n",
            "[I 2024-11-06 01:04:57,730] Trial 9 finished with value: 0.0 and parameters: {'learning_rate': 0.09446887683864426, 'hidden_dim': 16, 'dropout_rate': 0.3, 'num_layers': 3}. Best is trial 8 with value: 0.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.09648664145833907, 'hidden_dim': 16, 'dropout_rate': 0.0, 'num_layers': 2}\n",
            "Best loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Analyzing Results and Selecting the Best Model\n",
        "\n",
        "After completing hyperparameter tuning, analyze the results to select the best-performing model configuration.\n"
      ],
      "metadata": {
        "id": "1tk69LXg2ega"
      },
      "id": "1tk69LXg2ega"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Steps:\n",
        "1. **Compare Loss and Accuracy**: Identify configurations with the lowest loss and highest accuracy.\n",
        "2. **Consider Training Stability**: Ensure that the selected configuration yields stable and consistent performance across epochs.\n",
        "3. **Evaluate Overfitting**: Check if models with high complexity (e.g., high hidden dimensions, many layers) show signs of overfitting, especially if loss is much lower on training than validation.\n"
      ],
      "metadata": {
        "id": "bN8j38Wx2eji"
      },
      "id": "bN8j38Wx2eji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Code Example: Summarizing Results\n"
      ],
      "metadata": {
        "id": "gFwhLHQP2enP"
      },
      "id": "gFwhLHQP2enP"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert the grid search results into a pandas DataFrame\n",
        "# This will allow us to compare configurations in a tabular format easily\n",
        "grid_results_df = pd.DataFrame([\n",
        "    {\n",
        "        \"Configuration\": config,       # Stores the configuration as a dictionary\n",
        "        \"Loss\": result[\"Loss\"],        # Stores the corresponding loss value\n",
        "        \"Prediction\": result[\"Prediction\"]  # Stores the predicted label for each configuration\n",
        "    }\n",
        "    for config, result in grid_results.items()  # Loop through each configuration-result pair\n",
        "])\n",
        "\n",
        "# Step 2: Display the best configuration based on minimum loss\n",
        "# Use DataFrame's idxmin() to find the row index with the lowest loss value\n",
        "best_config = grid_results_df.loc[grid_results_df[\"Loss\"].idxmin()]\n",
        "\n",
        "# Print the best configuration\n",
        "print(\"Best Configuration:\\n\", best_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyJx2AVZ9pXj",
        "outputId": "ceadf281-b76f-4344-ff95-5421fb3740e8"
      },
      "id": "cyJx2AVZ9pXj",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Configuration:\n",
            " Configuration    ((dropout_rate, 0.0), (hidden_dim, 8), (learni...\n",
            "Loss                                                           0.0\n",
            "Prediction                                                       1\n",
            "Name: 4, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6. Code Walkthrough: Complete Hyperparameter Tuning Pipeline\n",
        "\n",
        "Here’s the complete code for setting up a hyperparameter tuning pipeline using grid search, random search, or Optuna (as preferred).\n",
        "\n"
      ],
      "metadata": {
        "id": "7JY2sY1i2erm"
      },
      "id": "7JY2sY1i2erm"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define function to run training and evaluation on the best configuration\n",
        "def run_best_configuration(best_params):\n",
        "    \"\"\"\n",
        "    Trains and evaluates the model using the best hyperparameters found.\n",
        "\n",
        "    Parameters:\n",
        "    - best_params (dict): Dictionary of best hyperparameters for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nTraining with Best Configuration:\")\n",
        "    print(best_params)  # Display the best configuration for clarity\n",
        "\n",
        "    # Step 1: Initialize the model with the best parameters\n",
        "    model = DeepGCNModel(\n",
        "        input_dim,                             # Input dimension of the node features\n",
        "        best_params['hidden_dim'],             # Hidden layer dimension from best configuration\n",
        "        output_dim,                            # Output dimension for classification\n",
        "        num_layers=best_params['num_layers']   # Number of layers as per the best configuration\n",
        "    )\n",
        "\n",
        "    # Step 2: Define the optimizer with the best learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "    # Step 3: Set the dropout rate for the model if applicable\n",
        "    model.dropout_rate = best_params['dropout_rate']\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Step 4: Forward pass - get node outputs from the model\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "\n",
        "        # Step 5: Aggregate node outputs to get a sentence-level embedding\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Step 6: Compute loss\n",
        "        loss = criterion(sentence_output, label)\n",
        "\n",
        "        # Step 7: Backward pass and optimization\n",
        "        optimizer.zero_grad()     # Clear previous gradients\n",
        "        loss.backward()           # Backpropagate the loss\n",
        "        optimizer.step()          # Update model parameters\n",
        "\n",
        "    # Final evaluation on the best configuration\n",
        "    with torch.no_grad():\n",
        "        # Perform the forward pass again to evaluate model performance\n",
        "        node_outputs = model(node_features, adj_matrix)\n",
        "        sentence_output = aggregate_nodes(node_outputs, method=\"mean\")\n",
        "\n",
        "        # Get the predicted label based on the highest output score\n",
        "        _, predicted = torch.max(sentence_output, dim=1)\n",
        "\n",
        "        # Print final loss and prediction for the best configuration\n",
        "        print(f\"Final Loss: {loss.item():.4f}, Predicted Label: {predicted.item()}\")\n",
        "\n",
        "# Example usage after tuning:\n",
        "# - For Optuna, use `study.best_params`.\n",
        "# - For grid or random search, use `best_config`.\n",
        "run_best_configuration(study.best_params if 'study' in globals() else best_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu1piPyu9q1e",
        "outputId": "bd3df03c-30e8-4385-dffb-f586141564f7"
      },
      "id": "xu1piPyu9q1e",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Best Configuration:\n",
            "{'learning_rate': 0.09648664145833907, 'hidden_dim': 16, 'dropout_rate': 0.0, 'num_layers': 2}\n",
            "Final Loss: 0.6931, Predicted Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary and Key Takeaways\n",
        "\n",
        "- **Hyperparameter Tuning**: Essential for optimizing GCNs, as it significantly impacts performance, stability, and convergence speed.\n",
        "- **Grid vs. Random vs. Bayesian Optimization**: Different strategies offer trade-offs in terms of efficiency, coverage, and complexity.\n",
        "- **Automated Tools**: Libraries like Optuna streamline the tuning process, allowing for intelligent exploration of large parameter spaces.\n",
        "- **Evaluating and Analyzing Results**: Summarizing results in a tabular format makes it easier to select the optimal configuration and identify trends among hyperparameters.\n",
        "\n",
        "Hyperparameter tuning provides a structured approach to optimizing GCN models, enhancing their ability to capture relationships in text and improving performance across NLP tasks. With an optimized model, you can apply it confidently to various applications, knowing it’s tuned to achieve the best possible results."
      ],
      "metadata": {
        "id": "PPmNOFtC2evz"
      },
      "id": "PPmNOFtC2evz"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hsnesx-Z2eyN"
      },
      "id": "Hsnesx-Z2eyN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}