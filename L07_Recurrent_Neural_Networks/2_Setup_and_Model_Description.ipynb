{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHL8Wn9qQ+LF1bcYn1gbk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L07_Recurrent_Neural_Networks/2_Setup_and_Model_Description.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hubejFyqx3TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1. Preparing the Data**\n",
        "\n",
        "In most machine learning tasks, data preparation is a critical first step. For Recurrent Neural Networks (RNNs), this is especially important when working with sequential data such as text, speech, or time series. In this case, we are dealing with text data, where the goal is to classify names based on their origin.\n",
        "\n",
        "**Steps to Prepare the Data:**\n",
        "1. **Dataset Selection**: (Example)\n",
        "   - The dataset consists of names from 18 different languages, stored in separate text files (`[Language].txt`). Each file contains a list of names, one per line.\n",
        "\n",
        "2. **Download and Organize the Data**:\n",
        "   - Download the dataset files.\n",
        "   - Place them in a directory structure that is easy to access during processing.\n",
        "\n",
        "3. **Preprocessing the Data**:\n",
        "   - Convert the names from Unicode to ASCII to make them compatible with the model.\n",
        "   - Organize the data by creating a dictionary, where each key represents a language and each value is a list of names corresponding to that language.\n",
        "\n",
        "**Code to Prepare the Data:**\n"
      ],
      "metadata": {
        "id": "WzsuSIX4y_Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob  # Module to retrieve file paths using patterns\n",
        "import os    # Module to interact with the operating system (e.g., file paths)\n",
        "import unicodedata  # Provides access to Unicode character properties\n",
        "import string  # Contains common string constants (like ASCII letters)\n",
        "\n",
        "# Define all valid characters that we will allow in the processed names.\n",
        "# This includes all ASCII letters (both uppercase and lowercase), and a few punctuation characters: \" .,'\"\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)  # Total number of valid characters\n",
        "\n",
        "# Function to convert a Unicode string into plain ASCII.\n",
        "# It removes any accents or diacritics by normalizing the string to 'NFD' form.\n",
        "# Then, it filters out characters that are not part of the 'Mn' category (which represents combining marks, like accents),\n",
        "# and finally, only retains characters that are in the list of allowed ASCII characters (all_letters).\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'  # Exclude diacritic marks\n",
        "        and c in all_letters  # Only keep characters that are in the predefined list (ASCII letters + punctuation)\n",
        "    )\n",
        "\n",
        "# Example usage of the unicodeToAscii function to convert a name with accented characters\n",
        "# This will remove the accents and convert the name to a plain ASCII representation.\n",
        "print(unicodeToAscii(\"Ślusàrski\"))  # Output: Slusarski\n",
        "\n",
        "# Dictionary to store names, categorized by language.\n",
        "# Keys are language names (like 'Arabic', 'English', etc.), and values are lists of names from that language.\n",
        "category_lines = {}\n",
        "\n",
        "# List to store all the categories (languages) found in the dataset.\n",
        "all_categories = []\n",
        "\n",
        "# Function to read lines from a file, process each line by converting it to ASCII, and return the processed lines.\n",
        "# The file is opened with UTF-8 encoding, and each line is stripped of leading/trailing whitespace.\n",
        "# It returns a list of names, where each name has been converted to ASCII using unicodeToAscii.\n",
        "def readLines(filename):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        return [unicodeToAscii(line.strip()) for line in f]\n",
        "\n",
        "# Use glob to find all text files in the 'data/names/' directory that match the pattern '*.txt'.\n",
        "# Each file contains names for a particular language, and the language is inferred from the filename.\n",
        "# For each file:\n",
        "# - Extract the category (language) from the filename.\n",
        "# - Append the category to all_categories.\n",
        "# - Read the lines (names) from the file, process them with readLines, and store them in the category_lines dictionary.\n",
        "for filename in glob.glob('data/names/*.txt'):\n",
        "    # Extract the category name by stripping the file extension from the filename\n",
        "    # Example: 'data/names/Arabic.txt' -> 'Arabic'\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "\n",
        "    # Add the category to the list of all categories\n",
        "    all_categories.append(category)\n",
        "\n",
        "    # Read the lines from the file, convert them to ASCII, and store them in the dictionary\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "# Get the number of categories (languages) we have loaded\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "# Output the list of all categories (languages)\n",
        "print(all_categories)  # Output: ['Arabic', 'Chinese', 'Czech', ...]\n",
        "print(n_categories)  # Output: 18\n",
        "print(category_lines['Arabic'][:5])  # Output: ['Arabic', 'Arabicam', 'Arabicer', 'Arabices', 'Arabicino']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umjg--AScgYR",
        "outputId": "144a0ce6-84c1-4d0c-edc9-1b86fe9487ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slusarski\n",
            "['Polish', 'English', 'Italian', 'German', 'Russian', 'Czech', 'Portuguese', 'Chinese', 'Arabic', 'Spanish', 'Scottish', 'Korean', 'Greek', 'Japanese', 'French', 'Dutch', 'Irish', 'Vietnamese']\n",
            "18\n",
            "['Khoury', 'Nahas', 'Daher', 'Gerges', 'Nazari']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The `unicodeToAscii` function converts any Unicode characters to ASCII, which is important because many names in different languages might have special characters.\n",
        "- The `category_lines` dictionary stores the names categorized by language, which will be used later to create tensors for training the model.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HHo20Y9zy_Xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.2. Turning Names into Tensors**\n",
        "\n",
        "Before feeding the data into the RNN, we need to convert the names (which are sequences of characters) into tensors that the RNN can process. This is done using **one-hot encoding**.\n",
        "\n",
        "**One-hot Encoding Explanation:**\n",
        "- A one-hot vector is a representation where all elements are zero, except for one element that is set to 1.\n",
        "- Each letter in a name is represented by a one-hot vector of size `<1 x n_letters>`, where `n_letters` is the total number of possible characters (e.g., all letters in the alphabet, plus some punctuation).\n",
        "- A name is represented as a sequence of one-hot vectors.\n",
        "\n",
        "**Code for One-hot Encoding and Tensor Conversion:**\n"
      ],
      "metadata": {
        "id": "E5F9-hGBy_aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # PyTorch library for creating and manipulating tensors\n",
        "\n",
        "# Function to convert a single letter into a one-hot encoded tensor\n",
        "# Each letter is represented by a tensor of size (1, n_letters), where n_letters is the total number of possible letters.\n",
        "# A one-hot tensor means all elements are zero except for a single element that is 1, which corresponds to the letter's position.\n",
        "def letterToTensor(letter):\n",
        "    # Initialize a tensor of zeros with shape (1, n_letters).\n",
        "    # This will represent the one-hot encoding of the letter.\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "\n",
        "    # Find the index of the letter in the 'all_letters' string.\n",
        "    # Then, set the corresponding position in the tensor to 1.\n",
        "    # This converts the letter into a one-hot encoded vector.\n",
        "    tensor[0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Function to convert a name (a sequence of letters) into a 3D tensor.\n",
        "# The resulting tensor will have shape (len(name), 1, n_letters), where:\n",
        "# - len(name) is the number of characters in the name (time steps),\n",
        "# - 1 is the batch size (each name is treated as a separate sequence),\n",
        "# - n_letters is the size of the one-hot encoded vector for each character.\n",
        "def nameToTensor(name):\n",
        "    # Initialize a tensor of zeros with shape (len(name), 1, n_letters).\n",
        "    # This tensor will store the one-hot encoded vectors for each letter in the name.\n",
        "    tensor = torch.zeros(len(name), 1, n_letters)\n",
        "\n",
        "    # Loop through each letter in the name and one-hot encode it.\n",
        "    # 'enumerate(name)' gives us both the index (i) and the letter itself.\n",
        "    for i, letter in enumerate(name):\n",
        "        # Find the index of the letter in 'all_letters' and set the corresponding position in the tensor to 1.\n",
        "        tensor[i][0][all_letters.find(letter)] = 1\n",
        "\n",
        "    # Return the 3D tensor representing the name\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# Example usage of the letterToTensor function\n",
        "# This prints the one-hot encoded tensor for the letter 'J'.\n",
        "# The output will be a tensor with a single row (1x57), where the 1 corresponds to the index of 'J' in all_letters.\n",
        "print(letterToTensor('J'))  # Output: tensor([[0., 0., 0., ..., 0., 1., 0.]])\n",
        "\n",
        "# Example usage of the nameToTensor function\n",
        "# This converts the name 'Jones' into a 3D tensor where each letter is one-hot encoded.\n",
        "# The resulting tensor will have shape (5, 1, 57) because 'Jones' has 5 letters, each represented by a one-hot encoded vector of size 57.\n",
        "print(nameToTensor('Jones').size())  # Output: torch.Size([5, 1, 57])\n",
        "print(nameToTensor('Jones'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feUVALGDcj-d",
        "outputId": "f8f2f7cb-d782-4200-95a4-2c8dbdc1c928"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "torch.Size([5, 1, 57])\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The `letterToTensor` function converts a single letter into a one-hot encoded tensor.\n",
        "- The `nameToTensor` function converts a sequence of letters (i.e., a name) into a tensor, where each row is the one-hot encoded vector of a letter.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TvWhsCFNy_dR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.3. Creating the RNN Model**\n",
        "\n",
        "Now that the data is prepared, we can focus on building the RNN model. In PyTorch, we define an RNN by creating a class that inherits from `torch.nn.Module`.\n",
        "\n",
        "**Model Architecture:**\n",
        "- The RNN takes an input (a one-hot vector representing a character) and a hidden state from the previous time step.\n",
        "- The hidden state gets updated with each time step based on the input and the previous hidden state.\n",
        "- After processing the input, the RNN outputs a prediction (the language category) and updates the hidden state for the next time step.\n",
        "\n",
        "**Code for Defining the RNN Model:**\n"
      ],
      "metadata": {
        "id": "nXdKIsivy_f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "# Define a simple RNN class that extends nn.Module\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Store the hidden layer size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define the linear layer that computes the next hidden state\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # Define the linear layer that computes the output\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "\n",
        "        # Define the LogSoftmax layer to convert outputs to log-probabilities\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Define the forward pass through the RNN\n",
        "    def forward(self, input, hidden):\n",
        "        print(f\"Input size: {input.size()}\")  # Display size of input\n",
        "        print(f\"Hidden size before update: {hidden.size()}\")  # Display size of hidden state\n",
        "\n",
        "        # Concatenate the input and hidden state\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        print(f\"Combined input-hidden size: {combined.size()}\")  # Display size after concatenation\n",
        "\n",
        "        # Compute the next hidden state\n",
        "        hidden = self.i2h(combined)\n",
        "        print(f\"Hidden state after update: {hidden.size()}\")  # Display size of updated hidden state\n",
        "\n",
        "        # Compute the output\n",
        "        output = self.i2o(combined)\n",
        "        print(f\"Output before softmax: {output}\")  # Display raw output (logits)\n",
        "\n",
        "        # Apply the softmax to the output\n",
        "        output = self.softmax(output)\n",
        "        print(f\"Output after softmax: {output}\")  # Display log-probabilities\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    # Initialize the hidden state with zeros\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n"
      ],
      "metadata": {
        "id": "veHwjRWDcoLA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming n_letters is the number of unique characters in the dataset and n_categories is the number of output classes\n",
        "n_letters = 57  # Example: number of possible input letters (e.g., 26 letters, uppercase, lowercase, punctuation)\n",
        "n_categories = 18  # Example: number of possible output categories (e.g., different languages or classes)\n",
        "\n",
        "# Example usage of the RNN class\n",
        "n_hidden = 128  # Hidden state size\n",
        "\n",
        "# Create an instance of the RNN\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\n",
        "\n",
        "# Create a dummy input tensor for the letter 'A'\n",
        "input_tensor = letterToTensor('A')\n",
        "print(f\"Input tensor for 'A': {input_tensor}\")\n",
        "\n",
        "# Initialize the hidden state to zeros\n",
        "hidden_tensor = rnn.init_hidden()\n",
        "print(f\"Initial hidden state: {hidden_tensor}\")\n",
        "\n",
        "# Pass the input tensor and the initial hidden state through the RNN\n",
        "print(\"\\n--- Forward pass ---\")\n",
        "output, next_hidden = rnn(input_tensor, hidden_tensor)\n",
        "\n",
        "# Final output\n",
        "print(f\"\\nFinal Output: {output}\")\n",
        "print(f\"Final Output Shape: {output.shape}\")\n",
        "\n",
        "# Final hidden state\n",
        "\n",
        "print(f\"Next Hidden State: {next_hidden}\")\n",
        "print(f\"Next Hidden State Shape: {next_hidden.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QcrWJ-okstJ",
        "outputId": "c7d95012-d189-47a3-c2d9-654ad56ad5e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor for 'A': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "Initial hidden state: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "--- Forward pass ---\n",
            "Input size: torch.Size([1, 57])\n",
            "Hidden size before update: torch.Size([1, 128])\n",
            "Combined input-hidden size: torch.Size([1, 185])\n",
            "Hidden state after update: torch.Size([1, 128])\n",
            "Output before softmax: tensor([[-0.0179,  0.0183, -0.0867, -0.0134,  0.0660, -0.0162,  0.0261, -0.0714,\n",
            "          0.0114,  0.1021, -0.0643,  0.0878,  0.0382,  0.0184,  0.1087,  0.0046,\n",
            "         -0.0956, -0.0315]], grad_fn=<AddmmBackward0>)\n",
            "Output after softmax: tensor([[-2.9148, -2.8786, -2.9836, -2.9103, -2.8309, -2.9131, -2.8707, -2.9682,\n",
            "         -2.8855, -2.7948, -2.9611, -2.8091, -2.8586, -2.8785, -2.7882, -2.8922,\n",
            "         -2.9925, -2.9284]], grad_fn=<LogSoftmaxBackward0>)\n",
            "\n",
            "Final Output: tensor([[-2.9148, -2.8786, -2.9836, -2.9103, -2.8309, -2.9131, -2.8707, -2.9682,\n",
            "         -2.8855, -2.7948, -2.9611, -2.8091, -2.8586, -2.8785, -2.7882, -2.8922,\n",
            "         -2.9925, -2.9284]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Final Output Shape: torch.Size([1, 18])\n",
            "Next Hidden State: tensor([[ 0.0096, -0.0892, -0.1310, -0.0223, -0.0941, -0.0381, -0.0394, -0.1106,\n",
            "         -0.0909,  0.0387, -0.0546, -0.0081, -0.0275,  0.0263, -0.0958, -0.0484,\n",
            "          0.0150,  0.0279,  0.0410,  0.0102, -0.0970,  0.0503,  0.0119,  0.0761,\n",
            "          0.1344,  0.0671, -0.0679, -0.0415, -0.0703,  0.0217, -0.0120,  0.0517,\n",
            "         -0.0152, -0.0883,  0.0510,  0.0335, -0.0758,  0.0364,  0.0269, -0.0520,\n",
            "          0.0547,  0.0491,  0.0203, -0.0059, -0.0734, -0.0639, -0.0319, -0.0110,\n",
            "         -0.0194,  0.0494,  0.0981,  0.1386, -0.0084, -0.0047,  0.0201, -0.0015,\n",
            "          0.0817, -0.0654,  0.0583,  0.0577, -0.0184, -0.1369, -0.0435,  0.0695,\n",
            "          0.0385,  0.0136, -0.0053,  0.0533,  0.0021, -0.0443, -0.0045,  0.1126,\n",
            "         -0.0759, -0.0086, -0.0151,  0.0758, -0.0582,  0.0262,  0.0401,  0.0733,\n",
            "         -0.0606, -0.1170, -0.0364, -0.0014, -0.0487,  0.0993,  0.0902, -0.0160,\n",
            "          0.0195, -0.0928,  0.0722, -0.0969, -0.0152,  0.0449,  0.0277,  0.0159,\n",
            "          0.0634, -0.0687,  0.0945, -0.0292,  0.1004, -0.0183, -0.0468,  0.0650,\n",
            "          0.0858,  0.0079,  0.0227,  0.0046, -0.0618,  0.1150,  0.0297, -0.1030,\n",
            "         -0.0358,  0.1153,  0.0822,  0.0726,  0.0262, -0.0700, -0.0563, -0.0521,\n",
            "         -0.0546, -0.0013,  0.0685,  0.0093, -0.0374, -0.0021, -0.0198,  0.0092]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Next Hidden State Shape: torch.Size([1, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The `RNN` class defines an RNN model with two fully connected layers: one for updating the hidden state (`i2h`) and one for generating the output (`i2o`).\n",
        "- The `forward` method takes an input tensor and a hidden state, concatenates them, and passes them through the network to produce an output and an updated hidden state.\n",
        "- The `init_hidden` method initializes the hidden state to zeros before the first time step.\n",
        "\n",
        "**Components of the Model**:\n",
        "- **Input Layer (`i2h`)**: Combines the input and the hidden state from the previous time step to produce a new hidden state.\n",
        "- **Output Layer (`i2o`)**: Uses the hidden state to predict the class (language) of the input name.\n",
        "- **Softmax Layer**: Converts the raw outputs into probabilities, indicating how likely the input belongs to each category.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1eqyy-Eby_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.4. Understanding the Flow of Data**\n",
        "\n",
        "**Data Flow in the RNN**:\n",
        "1. **Input Processing**:\n",
        "   - A name is broken down into its constituent letters.\n",
        "   - Each letter is converted into a one-hot vector.\n",
        "2. **Sequential Processing**:\n",
        "   - The RNN processes each letter in the name one by one.\n",
        "   - At each time step, the RNN takes the current letter and the hidden state from the previous step to compute a new hidden state and an output.\n",
        "3. **Final Prediction**:\n",
        "   - After processing the last letter in the sequence, the final output is taken as the prediction of the language category.\n",
        "\n",
        "**Explanation of the Flow**:\n",
        "- Each letter of the name is processed sequentially.\n",
        "- The hidden state is updated at each time step, allowing the RNN to \"remember\" information from earlier in the sequence.\n",
        "- The final hidden state and output are used to classify the name based on the patterns the model has learned during training.\n"
      ],
      "metadata": {
        "id": "b4BrYaEey_l9"
      }
    }
  ]
}