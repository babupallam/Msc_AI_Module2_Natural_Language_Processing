{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdRLKypiu6ucWiTsTilyo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L03-Learning%20to%20Classify%20Text/Note_05_Practical_Text_Classification_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This section delves into various practical examples of text classification tasks, each highlighting different approaches and challenges associated with text classification in natural language processing (NLP).\n",
        "- We will cover different tasks such as gender identification, document classification, part-of-speech tagging, sentence segmentation, spam detection, dialogue act classification, named entity recognition (NER), and language identification.\n",
        "- Each example demonstrates how text classification techniques can be applied to solve real-world problems effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "vQ9hf6P8Q6br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1 **Gender Identification**\n",
        "\n",
        "- **Task Description**:\n",
        "  - The goal of gender identification is to classify a person's gender based on their name. This can be useful in various applications such as demographic analysis, user profiling, and personalization.\n",
        "  \n",
        "- **Feature Engineering**:\n",
        "  - Simple features, such as the last letter of the name, can be highly predictive. For example, names ending in \"a\" or \"e\" are more likely to be associated with females, whereas names ending in \"n\" or \"r\" may be more common for males.\n",
        "  - Other features can include the length of the name, vowel/consonant ratios, or n-grams (character sequences within the name).\n",
        "\n",
        "- **Challenges**:\n",
        "  - Some names may be gender-neutral or have different gender associations across cultures. Handling such cases requires incorporating additional cultural or contextual information.\n",
        "  \n",
        "- **Example Approach**:\n",
        "  - Using a Naive Bayes classifier trained on features extracted from a dataset of names labeled with gender. The classifier can predict gender probabilities for new, unseen names based on the features.\n",
        "\n"
      ],
      "metadata": {
        "id": "2ik8dTJcQ6YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "oQ5h9454UAw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to classify a person's gender based on their name using a machine learning model. We will use NLTK for data preprocessing and PyTorch for building and training the model."
      ],
      "metadata": {
        "id": "Maz4Vz15R4n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Download the NLTK names dataset\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# NLTK provides a list of male and female names labeled accordingly\n",
        "def load_data():\n",
        "    male_names = [(name.lower(), 0) for name in names.words('male.txt')]  # Label '0' for male\n",
        "    female_names = [(name.lower(), 1) for name in names.words('female.txt')]  # Label '1' for female\n",
        "\n",
        "    # Combine the male and female names\n",
        "    all_names = male_names + female_names\n",
        "    random.shuffle(all_names)  # Shuffle the dataset\n",
        "\n",
        "    print(f\"Total number of names: {len(all_names)}\")\n",
        "    print(f\"Number of male names: {len(male_names)}\")\n",
        "    print(f\"Number of female names: {len(female_names)}\")\n",
        "    return all_names\n",
        "\n",
        "# Load the dataset\n",
        "all_names = load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-cL7k75R5EL",
        "outputId": "31cf6c48-9843-440e-efd4-3b27ef708b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of names: 7944\n",
            "Number of male names: 2943\n",
            "Number of female names: 5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WhBpNHO_R59E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Feature extraction\n",
        "# We will use the last letter of each name as a feature\n",
        "# Other potential features could include the length of the name, vowel/consonant ratios, etc.\n",
        "\n",
        "def extract_features(name):\n",
        "    \"\"\"Extract features from a given name.\"\"\"\n",
        "    name = name.lower()\n",
        "    last_letter = name[-1]  # Use the last letter as a feature\n",
        "    length = len(name)  # Use the length of the name as a feature\n",
        "    first_letter = name[0]  # Use the first letter as a feature\n",
        "    features = {\n",
        "        'last_letter': ord(last_letter) - ord('a'),  # Convert to numerical value\n",
        "        'length': length,\n",
        "        'first_letter': ord(first_letter) - ord('a')\n",
        "    }\n",
        "    return [features['last_letter'], features['length'], features['first_letter']]\n",
        "\n",
        "# Step 4: Prepare the dataset for training\n",
        "# Split data into features (X) and labels (y)\n",
        "X = [extract_features(name) for name, gender in all_names]\n",
        "y = [gender for name, gender in all_names]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3K_OMXYR59F",
        "outputId": "016852d1-baa1-4ee7-cc04-b3c4b5134a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6355\n",
            "Test set size: 1589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KRR5376IR6Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for gender identification\n",
        "class GenderClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GenderClassifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = 3  # Number of features (last letter, length, first letter)\n",
        "hidden_dim = 8  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = 2  # Number of output classes (male and female)\n",
        "\n",
        "# Initialize the model\n",
        "model = GenderClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbtCQlOyR6Fp",
        "outputId": "09f4d0e4-60a0-4326-eba4-55ce6f347470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenderClassifier(\n",
            "  (fc1): Linear(in_features=3, out_features=8, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gTu4wOdGR6Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 100  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "train_losses = []\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pyPhtJCR6Ox",
        "outputId": "0624164e-43e1-413b-88c1-21e810668597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.5277\n",
            "Epoch [20/100], Loss: 0.5226\n",
            "Epoch [30/100], Loss: 0.5220\n",
            "Epoch [40/100], Loss: 0.5184\n",
            "Epoch [50/100], Loss: 0.5180\n",
            "Epoch [60/100], Loss: 0.5175\n",
            "Epoch [70/100], Loss: 0.5149\n",
            "Epoch [80/100], Loss: 0.5138\n",
            "Epoch [90/100], Loss: 0.5136\n",
            "Epoch [100/100], Loss: 0.5188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TM0b6bboUKZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision = precision_score(y_test, predicted)\n",
        "recall = recall_score(y_test, predicted)\n",
        "f1 = f1_score(y_test, predicted)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UntMWl9UKyf",
        "outputId": "c6d47e30-46ea-4645-f5e8-ddab1b3c634a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.7256\n",
            "Precision: 0.7761\n",
            "Recall: 0.7976\n",
            "F1-Score: 0.7867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_xEXu-jEULdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [4, 8, 16]:\n",
        "        for num_epochs in [50, 100, 200]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = GenderClassifier(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "JphPlzyYUL1E",
        "outputId": "05cb16f8-d873-48e4-de4d-49f5d6aea1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7ae97f5b8c3e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YBgzXzgmY1vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Re-train the model using the best hyperparameters and evaluate it\n",
        "\n",
        "# Unpack the best hyperparameters\n",
        "best_lr, best_hidden_dim, best_num_epochs = best_params\n",
        "\n",
        "# Re-initialize the model with the best hyperparameters\n",
        "model = LanguageIdentifier(input_dim, best_hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "# Training the model with the best hyperparameters\n",
        "print(f\"\\nRe-training the model with the best hyperparameters: Learning Rate={best_lr}, Hidden Dimension={best_hidden_dim}, Epochs={best_num_epochs}\")\n",
        "\n",
        "for epoch in range(best_num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{best_num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Step 13: Evaluate the re-trained model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 14: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
        "\n",
        "# Print the final evaluation results\n",
        "print(f\"\\nFinal Model Evaluation with Best Hyperparameters:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "x-LxJCHUY1vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2 **Document Classification**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Document classification involves assigning a predefined category to a document, such as categorizing a movie review as \"positive\" or \"negative\" (sentiment analysis).\n",
        "  - It can also involve topic categorization, where documents are classified into subjects such as \"sports,\" \"politics,\" or \"technology.\"\n",
        "\n",
        "- **Feature Extraction**:\n",
        "  - Textual features may include word presence or absence (bag-of-words), term frequency-inverse document frequency (TF-IDF), n-grams, and word embeddings.\n",
        "  - Additional features can be derived from document structure (e.g., paragraph lengths, headings) or metadata (e.g., author information).\n",
        "\n",
        "- **Example Approach**:\n",
        "  - Using a Support Vector Machine (SVM) or logistic regression model with TF-IDF features extracted from the text. For instance, the Movie Reviews Corpus can be used to train a model that classifies reviews as positive or negative based on word frequencies.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Handling sarcasm, irony, or ambiguous language can be difficult in sentiment analysis. Additionally, topic categorization may require domain-specific knowledge or large labeled datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "UBJac2XNQ6Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "UuRdfBcUUOwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we will classify text documents into predefined categories. Specifically, we will use the NLTK movie reviews dataset to classify movie reviews as either \"positive\" or \"negative.\""
      ],
      "metadata": {
        "id": "Z5-CuJTtUOwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Download the NLTK movie reviews dataset\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# The NLTK movie reviews dataset contains positive and negative movie reviews\n",
        "def load_data():\n",
        "    documents = [(list(movie_reviews.words(fileid)), category)\n",
        "                 for category in movie_reviews.categories()\n",
        "                 for fileid in movie_reviews.fileids(category)]\n",
        "    print(f\"Total number of documents: {len(documents)}\")\n",
        "    return documents\n",
        "\n",
        "# Load the dataset\n",
        "all_documents = load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOlT2bdiUOwk",
        "outputId": "ea60828f-65f5-4897-be17-d5712e7067d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BWVpnzfqUOwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing and feature extraction\n",
        "# Convert the documents to lowercase, tokenize, and join words back into a single string for vectorization\n",
        "\n",
        "def preprocess_documents(documents):\n",
        "    preprocessed_docs = [\" \".join([word.lower() for word in doc]) for doc, _ in documents]\n",
        "    labels = [1 if label == 'pos' else 0 for _, label in documents]  # 1 for positive, 0 for negative\n",
        "    return preprocessed_docs, labels\n",
        "\n",
        "# Preprocess the documents\n",
        "X, y = preprocess_documents(all_documents)\n",
        "\n",
        "# Step 4: Convert text data to numerical data using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgKRmir1UOwl",
        "outputId": "754558f5-46b9-490d-f0b4-6e27130be1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1600\n",
            "Test set size: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4fSInM1xUOwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for document classification\n",
        "class DocumentClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DocumentClassifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = X_train.size(1)  # Number of features (TF-IDF features)\n",
        "hidden_dim = 128  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = 2  # Number of output classes (positive and negative)\n",
        "\n",
        "# Initialize the model\n",
        "model = DocumentClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_77eJmzZUOwl",
        "outputId": "16a0c75b-11f7-488d-f7be-557ae6042f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DocumentClassifier(\n",
            "  (fc1): Linear(in_features=2000, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZxErzPEtUOwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 20  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "train_losses = []\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlsZNDq0UOwm",
        "outputId": "348bb4a4-8bae-4595-996e-0ae99cf18d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.0043\n",
            "Epoch [10/20], Loss: 0.0004\n",
            "Epoch [15/20], Loss: 0.0001\n",
            "Epoch [20/20], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y0ANu3bvUOwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision = precision_score(y_test, predicted)\n",
        "recall = recall_score(y_test, predicted)\n",
        "f1 = f1_score(y_test, predicted)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpTNPlvDUOwm",
        "outputId": "4621a5a2-263b-4946-d1b7-d518f108ac70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.8350\n",
            "Precision: 0.8230\n",
            "Recall: 0.8557\n",
            "F1-Score: 0.8390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Eq9mAcXzUOwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [64, 128, 256]:\n",
        "        for num_epochs in [10, 20, 30]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = DocumentClassifier(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "id": "WsMTLUIiUOwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3 **Part-of-Speech Tagging**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Part-of-speech (POS) tagging assigns grammatical categories, such as noun, verb, or adjective, to each word in a sentence.\n",
        "  - It is a fundamental task in NLP that serves as a building block for more complex tasks like parsing, named entity recognition, and machine translation.\n",
        "\n",
        "- **Techniques**:\n",
        "  - Features for POS tagging include word suffixes, previous and next word tags, capitalization, and contextual word embeddings.\n",
        "  - Algorithms such as Hidden Markov Models (HMM), Conditional Random Fields (CRF), and Recurrent Neural Networks (RNN) are commonly used for sequence labeling tasks like POS tagging.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Words can have multiple POS tags depending on their context (e.g., \"run\" can be a noun or a verb). Handling such ambiguities requires incorporating surrounding context effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "oMirzax6Q6TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "jn6cIeFWUQaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to classify each word in a sentence into its corresponding part of speech (e.g., noun, verb, adjective)."
      ],
      "metadata": {
        "id": "93NMLJ1tUQaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import Counter\n",
        "\n",
        "# Download the NLTK treebank corpus\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# The NLTK treebank corpus provides sentences tagged with POS tags\n",
        "def load_data():\n",
        "    # Use the universal tagset to simplify the number of tags\n",
        "    tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
        "    print(f\"Total number of sentences: {len(tagged_sentences)}\")\n",
        "    return tagged_sentences\n",
        "\n",
        "# Load the dataset\n",
        "all_sentences = load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTAO7PzyUQaQ",
        "outputId": "e0fa647d-ed96-427c-d72e-41195e7e5d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences: 3914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p2W4uSnmUQaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing and feature extraction\n",
        "# Convert the sentences to lower case and split the data into training and testing sets\n",
        "def preprocess_data(sentences):\n",
        "    # Extract words and tags from the sentences\n",
        "    words = [[word.lower() for word, _ in sentence] for sentence in sentences]\n",
        "    tags = [[tag for _, tag in sentence] for sentence in sentences]\n",
        "\n",
        "    return words, tags\n",
        "\n",
        "# Preprocess the data\n",
        "all_words, all_tags = preprocess_data(all_sentences)\n",
        "\n",
        "# Step 4: Build a vocabulary of words and a tagset\n",
        "# Create a word-to-index mapping and a tag-to-index mapping\n",
        "word_counts = Counter(word for sentence in all_words for word in sentence)\n",
        "vocab = [word for word, freq in word_counts.items() if freq > 1]  # Filter out rare words\n",
        "word_to_idx = {word: idx + 2 for idx, word in enumerate(vocab)}  # Start indexing from 2\n",
        "word_to_idx['<PAD>'] = 0  # Padding index\n",
        "word_to_idx['<UNK>'] = 1  # Unknown word index\n",
        "\n",
        "# Create a tag-to-index mapping\n",
        "tag_to_idx = {tag: idx for idx, tag in enumerate(set(tag for tags in all_tags for tag in tags))}\n",
        "\n",
        "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
        "print(f\"Number of unique tags: {len(tag_to_idx)}\")\n",
        "\n",
        "# Step 5: Convert sentences and tags to sequences of indices\n",
        "def encode_sequences(words, tags, word_to_idx, tag_to_idx):\n",
        "    encoded_words = [[word_to_idx.get(word, word_to_idx['<UNK>']) for word in sentence] for sentence in words]\n",
        "    encoded_tags = [[tag_to_idx[tag] for tag in sentence] for sentence in tags]\n",
        "    return encoded_words, encoded_tags\n",
        "\n",
        "encoded_words, encoded_tags = encode_sequences(all_words, all_tags, word_to_idx, tag_to_idx)\n",
        "\n",
        "# Step 6: Pad the sequences to ensure uniform length\n",
        "def pad_sequences(sequences, max_len, padding_value=0):\n",
        "    return [seq + [padding_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
        "\n",
        "max_len = max(len(sentence) for sentence in encoded_words)  # Maximum sequence length\n",
        "padded_words = pad_sequences(encoded_words, max_len)\n",
        "padded_tags = pad_sequences(encoded_tags, max_len)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(padded_words, dtype=torch.long)\n",
        "y = torch.tensor(padded_tags, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB2kHNqbUQaT",
        "outputId": "ab716832-39f9-43fe-f9fb-96072a147740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5602\n",
            "Number of unique tags: 12\n",
            "Training set size: 3131\n",
            "Test set size: 783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EdTlfVIfUQaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the neural network model for POS tagging\n",
        "class POSTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(POSTagger, self).__init__()\n",
        "        # Define the layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # Embedding layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# Step 8: Set up the model parameters\n",
        "vocab_size = len(word_to_idx)\n",
        "embedding_dim = 100  # Size of the embedding vectors (can be tuned)\n",
        "hidden_dim = 128  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = len(tag_to_idx)  # Number of POS tags\n",
        "\n",
        "# Initialize the model\n",
        "model = POSTagger(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5k8MaAQUQaU",
        "outputId": "92333120-af40-46bd-b2dc-d3caea599c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSTagger(\n",
            "  (embedding): Embedding(5602, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FdiJTeryUQaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index during loss calculation\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate can be tuned\n",
        "\n",
        "# Step 10: Training loop\n",
        "num_epochs = 10  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        outputs = outputs.view(-1, outputs.shape[2])  # Reshape for loss calculation\n",
        "        batch_y = batch_y.view(-1)  # Flatten labels\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKcdInAkUQaV",
        "outputId": "3c48ea08-b96f-4432-93ae-21dd4c2d36fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.4794\n",
            "Epoch [4/10], Loss: 0.2238\n",
            "Epoch [6/10], Loss: 0.1334\n",
            "Epoch [8/10], Loss: 0.0848\n",
            "Epoch [10/10], Loss: 0.0542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0o_qSbUqUQaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    y_pred = torch.argmax(y_pred, dim=2)\n",
        "\n",
        "# Step 12: Calculate evaluation metrics\n",
        "# Convert predictions and labels back to their original shape\n",
        "y_pred = y_pred.view(-1).cpu().numpy()\n",
        "y_true = y_test.view(-1).cpu().numpy()\n",
        "\n",
        "# Filter out padding indices for evaluation\n",
        "valid_indices = y_true != 0\n",
        "y_true = y_true[valid_indices]\n",
        "y_pred = y_pred[valid_indices]\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "u9RvIAl1UQaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vZqxMQlTUQaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, embedding dimensions, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01]:\n",
        "    for embedding_dim in [50, 100, 200]:\n",
        "        for hidden_dim in [64, 128, 256]:\n",
        "            for num_epochs in [5, 10, 20]:\n",
        "                # Initialize the model with current parameters\n",
        "                model = POSTagger(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                # Train the model with the current settings\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    permutation = torch.randperm(X_train.size(0))\n",
        "                    for i in range(0, X_train.size(0), batch_size):\n",
        "                        indices = permutation[i:i + batch_size]\n",
        "                        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                        outputs = model(batch_X)\n",
        "                        outputs = outputs.view(-1, outputs.shape[2])\n",
        "                        batch_y = batch_y.view(-1)\n",
        "\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Evaluate the model\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(X_test)\n",
        "                    y_pred = torch.argmax(y_pred, dim=2)\n",
        "\n",
        "                y_pred = y_pred.view(-1).cpu().numpy()\n",
        "                y_true = y_test.view(-1).cpu().numpy()\n",
        "\n",
        "                valid_indices = y_true != 0\n",
        "                y_true = y_true[valid_indices]\n",
        "                y_pred = y_pred[valid_indices]\n",
        "\n",
        "                accuracy = accuracy_score(y_true, y_pred)\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = (lr, embedding_dim, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Embedding Dimension: {best_params[1]}, Hidden Dimension: {best_params[2]}, Epochs: {best_params[3]}\")\n"
      ],
      "metadata": {
        "id": "LwE2h1W9UQaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.4 **Sentence Segmentation**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Sentence segmentation, also known as sentence boundary detection, involves splitting a block of text into individual sentences.\n",
        "  - It is often used as a preprocessing step for tasks like text summarization, machine translation, and named entity recognition.\n",
        "\n",
        "- **Approaches**:\n",
        "  - Rule-based methods can identify sentence boundaries based on punctuation marks (e.g., periods, question marks). However, this may not work well for abbreviations (e.g., \"Dr.\") or other special cases.\n",
        "  - Machine learning-based methods can use features such as word capitalization, surrounding words, and punctuation to detect boundaries more accurately.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Text may contain complex structures, such as quoted speech or lists, where traditional rules for sentence segmentation may fail.\n",
        "\n"
      ],
      "metadata": {
        "id": "bo9h1rDWQ6QW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "qHGLkMl_UR77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The goal is to split a block of text into individual sentences. Sentence segmentation, or sentence boundary detection, is an important step in preprocessing text data for various NLP tasks such as machine translation, summarization, and named entity recognition."
      ],
      "metadata": {
        "id": "mK79LlY8UR78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Download the NLTK datasets required for sentence tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# We will use the Gutenberg corpus from NLTK, which contains a collection of books\n",
        "def load_data():\n",
        "    # Select a few books for the dataset\n",
        "    text = gutenberg.raw('austen-emma.txt') + gutenberg.raw('austen-persuasion.txt') + gutenberg.raw('austen-sense.txt')\n",
        "    print(f\"Total number of characters in the dataset: {len(text)}\")\n",
        "    return text\n",
        "\n",
        "# Load the dataset\n",
        "raw_text = load_data()\n"
      ],
      "metadata": {
        "id": "b1jeQsd7UR78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z_DgvDRxUR79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "# We will split the text into sentences using the NLTK sent_tokenize function and then create training data\n",
        "\n",
        "def create_dataset(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    dataset = []\n",
        "\n",
        "    # Generate labeled data for each character in the text\n",
        "    # Label '1' if the character is the end of a sentence, otherwise '0'\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        for i in range(len(sentence) - 1):\n",
        "            if i == len(sentence) - 2:\n",
        "                dataset.append((sentence[:i + 1], 1))  # Label '1' at the sentence boundary\n",
        "            else:\n",
        "                dataset.append((sentence[:i + 1], 0))  # Label '0' for non-boundaries\n",
        "\n",
        "    print(f\"Total number of samples: {len(dataset)}\")\n",
        "    return dataset\n",
        "\n",
        "# Create the dataset\n",
        "all_data = create_dataset(raw_text)\n"
      ],
      "metadata": {
        "id": "D89T8ZDGUR79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U4xFgpfEUR7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature extraction\n",
        "# Convert text sequences into numerical features\n",
        "\n",
        "def extract_features(sequence):\n",
        "    # Features will include the ASCII value of the last character and the length of the sequence\n",
        "    last_char = ord(sequence[-1]) if sequence[-1].isalpha() else 0  # ASCII value of the last character\n",
        "    length = len(sequence)\n",
        "    features = [last_char, length]\n",
        "    return features\n",
        "\n",
        "# Prepare the dataset for training\n",
        "X = [extract_features(seq) for seq, label in all_data]\n",
        "y = [label for _, label in all_data]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "id": "oe-MqKWQUR7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YjYkbt6gUR7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for sentence segmentation\n",
        "class SentenceSegmenter(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SentenceSegmenter, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = 2  # Number of features (ASCII value of the last character, length)\n",
        "hidden_dim = 16  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = 2  # Number of output classes (boundary or not)\n",
        "\n",
        "# Initialize the model\n",
        "model = SentenceSegmenter(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "bF7yei2aUR7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t9mwJIDIUR8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 20  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "m6ncohVWUR8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_d-cEulXUR8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='binary')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "UBHOdP0WUR8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6aQ0qZifWyIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [8, 16, 32]:\n",
        "        for num_epochs in [10, 20, 30]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = SentenceSegmenter(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "id": "k2IOvMd1WyiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.5 **Spam Detection**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Spam detection aims to classify email messages or other text content as \"spam\" or \"not spam.\" It is widely used in email filtering, content moderation, and social media monitoring.\n",
        "\n",
        "- **Features**:\n",
        "  - Common features include word frequencies (e.g., presence of specific words like \"free\" or \"win\"), email metadata (e.g., sender's email address), and language characteristics (e.g., excessive use of capital letters).\n",
        "  - Advanced techniques may involve using embeddings from language models such as BERT to capture deeper semantic features.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Spammers frequently change their strategies to bypass detection, requiring models to be updated and retrained regularly. Handling multilingual or obfuscated spam messages can also pose difficulties.\n",
        "\n",
        "- **Example Approach**:\n",
        "  - A Naive Bayes or logistic regression classifier trained on email datasets with labeled spam and non-spam examples. Features like the presence of specific keywords and email sender information are used to classify the messages.\n",
        "\n"
      ],
      "metadata": {
        "id": "DuynONw_Q6Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "c94wah9HUSli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this task is to classify text messages as \"spam\" or \"not spam.\" We will use a dataset of SMS messages for training and testing the spam classifier."
      ],
      "metadata": {
        "id": "JOZBqlrXUSlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# We will use a publicly available dataset containing labeled SMS messages\n",
        "def load_data():\n",
        "    # Dataset URL: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "    # For this demonstration, assume the dataset is already downloaded and extracted\n",
        "    data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "    print(f\"Total number of messages: {len(data)}\")\n",
        "    return data\n",
        "\n",
        "# Load the dataset\n",
        "data = load_data()\n"
      ],
      "metadata": {
        "id": "K8ENB9JoUSlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "esPD4uhAUSlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "# Convert labels to binary values: 'spam' -> 1, 'ham' (not spam) -> 0\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Remove stopwords and perform basic text cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and split into words\n",
        "    words = text.lower().split()\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to each message\n",
        "data['message'] = data['message'].apply(preprocess_text)\n",
        "\n",
        "# Step 4: Convert text data to numerical data using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['message']).toarray()\n",
        "\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "X = X_tfidf\n",
        "y = data['label'].values\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "id": "LDUDzUAQUSll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tSVROv3TUSll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for spam detection\n",
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = X_train.size(1)  # Number of features (TF-IDF features)\n",
        "hidden_dim = 64  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = 2  # Number of output classes (spam and not spam)\n",
        "\n",
        "# Initialize the model\n",
        "model = SpamClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "ulUYVKqzUSlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bCjNrhV8USln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 30  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "pWESpqxqUSln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kdnXMe1hUSlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision = precision_score(y_test, predicted)\n",
        "recall = recall_score(y_test, predicted)\n",
        "f1 = f1_score(y_test, predicted)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "7WIccWu0USlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pTwqyJe2USlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [32, 64, 128]:\n",
        "        for num_epochs in [20, 30, 40]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = SpamClassifier(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "id": "OHCi9lAEUSlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.6 **Dialogue Act Classification**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Dialogue act classification involves categorizing each line in a conversation into predefined categories, such as \"question,\" \"statement,\" or \"command.\"\n",
        "  - It is useful in conversational agents, customer support systems, and dialogue systems for understanding user intentions.\n",
        "\n",
        "- **Feature Engineering**:\n",
        "  - Features can include the presence of question words (e.g., \"what,\" \"how\"), sentence structure, and context from previous dialogue turns.\n",
        "  - Sequential models like RNNs or Transformers can capture dependencies across multiple dialogue turns.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Dialogue contexts can vary significantly between different conversations, making it challenging to generalize. Additionally, the same sentence structure may convey different meanings based on context.\n",
        "\n"
      ],
      "metadata": {
        "id": "8ga1cfAGQ6K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "W1VPreVjUTEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to classify each line in a conversation into predefined categories, such as \"question,\" \"statement,\" or \"command.\" This task helps understand the intention behind a spoken or written message and is useful in conversational agents, customer support systems, and dialogue systems."
      ],
      "metadata": {
        "id": "9id3xEegUTEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Download NLTK's dialogue act data\n",
        "nltk.download('nps_chat')\n",
        "from nltk.corpus import nps_chat\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# The NPS Chat Corpus is a collection of chatroom dialogues labeled with dialogue acts\n",
        "def load_data():\n",
        "    posts = nps_chat.xml_posts()\n",
        "    # Extract text and their corresponding dialogue act labels\n",
        "    data = [(post.text, post.get('class')) for post in posts]\n",
        "    print(f\"Total number of dialogue lines: {len(data)}\")\n",
        "    return data\n",
        "\n",
        "# Load the dataset\n",
        "all_data = load_data()\n"
      ],
      "metadata": {
        "id": "SuLDqFWoUTEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dIl65XW2UTEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "# Extract the text (X) and labels (y) from the dataset\n",
        "X_raw = [text for text, label in all_data]\n",
        "y_raw = [label for _, label in all_data]\n",
        "\n",
        "# Encode the labels as integers\n",
        "label_to_idx = {label: idx for idx, label in enumerate(set(y_raw))}\n",
        "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "y_encoded = [label_to_idx[label] for label in y_raw]\n",
        "\n",
        "print(f\"Number of unique dialogue acts: {len(label_to_idx)}\")\n",
        "\n",
        "# Step 4: Convert text data to numerical data using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X_raw).toarray()\n",
        "\n",
        "# Convert the labels to a PyTorch tensor\n",
        "y = torch.tensor(y_encoded, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the features to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "id": "RHT8tjrUUTEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kV_1OhvRUTEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for dialogue act classification\n",
        "class DialogueActClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DialogueActClassifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = X_train.size(1)  # Number of features (TF-IDF features)\n",
        "hidden_dim = 64  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = len(label_to_idx)  # Number of dialogue act classes\n",
        "\n",
        "# Initialize the model\n",
        "model = DialogueActClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "492TTer8UTEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FNJnzFoyUTEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 20  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "xWxwFoYEUTEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "98LBRnoNUTEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "hD4LSFYZUTEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PK4GQcryUTEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [32, 64, 128]:\n",
        "        for num_epochs in [10, 20, 30]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = DialogueActClassifier(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "id": "ELJQUOvyUTEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.7 **Named Entity Recognition (NER)**\n",
        "\n",
        "- **Task Description**:\n",
        "  - NER is the process of identifying and classifying named entities in text, such as people, organizations, locations, dates, and product names.\n",
        "  - It is widely used in information extraction, knowledge graph construction, and question answering systems.\n",
        "\n",
        "- **Approaches**:\n",
        "  - Features for NER include word shape (e.g., capitalization), POS tags, context words, and embeddings from pre-trained language models.\n",
        "  - Algorithms such as CRF, Bi-LSTM with CRF, and Transformer-based models like BERT are commonly used.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Handling entity boundary detection and recognizing entities in non-standard formats (e.g., informal text, social media) can be difficult. Additionally, entities with multiple words (e.g., \"New York Times\") require special handling.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSsPSdcSQ6IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "X92CgvPyUTkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this task is to identify and classify named entities in text into predefined categories such as person names, locations, organizations, dates, etc. NER is a crucial step in information extraction and various NLP applications."
      ],
      "metadata": {
        "id": "Va46Im2OUTkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Download the NLTK datasets required for NER\n",
        "nltk.download('conll2002')\n",
        "\n",
        "from nltk.corpus import conll2002\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# The CoNLL 2002 corpus contains labeled named entities in Spanish and Dutch\n",
        "def load_data():\n",
        "    # We will use the Spanish dataset for this example\n",
        "    sentences = conll2002.iob_sents('esp.train')\n",
        "    print(f\"Total number of sentences: {len(sentences)}\")\n",
        "    return sentences\n",
        "\n",
        "# Load the dataset\n",
        "all_sentences = load_data()\n"
      ],
      "metadata": {
        "id": "9v5PpJc-UTkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "niVbBcKwUTkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "# Extract words and their corresponding named entity tags from the sentences\n",
        "def preprocess_data(sentences):\n",
        "    words = [[word.lower() for word, _, _ in sentence] for sentence in sentences]\n",
        "    tags = [[tag for _, _, tag in sentence] for sentence in sentences]\n",
        "    return words, tags\n",
        "\n",
        "# Preprocess the data\n",
        "all_words, all_tags = preprocess_data(all_sentences)\n",
        "\n",
        "# Step 4: Build a vocabulary of words and a tagset\n",
        "# Create a word-to-index mapping and a tag-to-index mapping\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(word for sentence in all_words for word in sentence)\n",
        "vocab = [word for word, freq in word_counts.items() if freq > 1]  # Filter out rare words\n",
        "word_to_idx = {word: idx + 2 for idx, word in enumerate(vocab)}  # Start indexing from 2\n",
        "word_to_idx['<PAD>'] = 0  # Padding index\n",
        "word_to_idx['<UNK>'] = 1  # Unknown word index\n",
        "\n",
        "# Create a tag-to-index mapping\n",
        "tag_to_idx = {tag: idx for idx, tag in enumerate(set(tag for tags in all_tags for tag in tags))}\n",
        "\n",
        "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
        "print(f\"Number of unique tags: {len(tag_to_idx)}\")\n",
        "\n",
        "# Step 5: Convert sentences and tags to sequences of indices\n",
        "def encode_sequences(words, tags, word_to_idx, tag_to_idx):\n",
        "    encoded_words = [[word_to_idx.get(word, word_to_idx['<UNK>']) for word in sentence] for sentence in words]\n",
        "    encoded_tags = [[tag_to_idx[tag] for tag in sentence] for sentence in tags]\n",
        "    return encoded_words, encoded_tags\n",
        "\n",
        "encoded_words, encoded_tags = encode_sequences(all_words, all_tags, word_to_idx, tag_to_idx)\n",
        "\n",
        "# Step 6: Pad the sequences to ensure uniform length\n",
        "def pad_sequences(sequences, max_len, padding_value=0):\n",
        "    return [seq + [padding_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
        "\n",
        "max_len = max(len(sentence) for sentence in encoded_words)  # Maximum sequence length\n",
        "padded_words = pad_sequences(encoded_words, max_len)\n",
        "padded_tags = pad_sequences(encoded_tags, max_len)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(padded_words, dtype=torch.long)\n",
        "y = torch.tensor(padded_tags, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "id": "o9OFnh9cUTkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0yMUtey_UTkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the neural network model for NER\n",
        "class NERTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(NERTagger, self).__init__()\n",
        "        # Define the layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # Embedding layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# Step 8: Set up the model parameters\n",
        "vocab_size = len(word_to_idx)\n",
        "embedding_dim = 100  # Size of the embedding vectors (can be tuned)\n",
        "hidden_dim = 128  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = len(tag_to_idx)  # Number of named entity tags\n",
        "\n",
        "# Initialize the model\n",
        "model = NERTagger(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "Ijp9VK-zUTkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8eU2oAmBUTkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index during loss calculation\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate can be tuned\n",
        "\n",
        "# Step 10: Training loop\n",
        "num_epochs = 10  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        outputs = outputs.view(-1, outputs.shape[2])  # Reshape for loss calculation\n",
        "        batch_y = batch_y.view(-1)  # Flatten labels\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "rMWsxbpkUTkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWma7FkpUTkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    y_pred = torch.argmax(y_pred, dim=2)\n",
        "\n",
        "# Step 12: Calculate evaluation metrics\n",
        "# Convert predictions and labels back to their original shape\n",
        "y_pred = y_pred.view(-1).cpu().numpy()\n",
        "y_true = y_test.view(-1).cpu().numpy()\n",
        "\n",
        "# Filter out padding indices for evaluation\n",
        "valid_indices = y_true != 0\n",
        "y_true = y_true[valid_indices]\n",
        "y_pred = y_pred[valid_indices]\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "mS_zNjnRUTkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZHb9QoXGUTkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, embedding dimensions, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01]:\n",
        "    for embedding_dim in [50, 100, 200]:\n",
        "        for hidden_dim in [64, 128, 256]:\n",
        "            for num_epochs in [5, 10, 20]:\n",
        "                # Initialize the model with current parameters\n",
        "                model = NERTagger(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                # Train the model with the current settings\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    permutation = torch.randperm(X_train.size(0))\n",
        "                    for i in range(0, X_train.size(0), batch_size):\n",
        "                        indices = permutation[i:i + batch_size]\n",
        "                        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                        outputs = model(batch_X)\n",
        "                        outputs = outputs.view(-1, outputs.shape[2])\n",
        "                        batch_y = batch_y.view(-1)\n",
        "\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Evaluate the model\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(X_test)\n",
        "                    y_pred = torch.argmax(y_pred, dim=2)\n",
        "\n",
        "                y_pred = y_pred.view(-1).cpu().numpy()\n",
        "                y_true = y_test.view(-1).cpu().numpy()\n",
        "\n",
        "                valid_indices = y_true != 0\n",
        "                y_true = y_true[valid_indices]\n",
        "                y_pred = y_pred[valid_indices]\n",
        "\n",
        "                accuracy = accuracy_score(y_true, y_pred)\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = (lr, embedding_dim, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Embedding Dimension: {best_params[1]}, Hidden Dimension: {best_params[2]}, Epochs: {best_params[3]}\")\n"
      ],
      "metadata": {
        "id": "Toox-gzJUTkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mkz49jTdYwur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Re-train the model using the best hyperparameters and evaluate it\n",
        "\n",
        "# Unpack the best hyperparameters\n",
        "best_lr, best_hidden_dim, best_num_epochs = best_params\n",
        "\n",
        "# Re-initialize the model with the best hyperparameters\n",
        "model = LanguageIdentifier(input_dim, best_hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "# Training the model with the best hyperparameters\n",
        "print(f\"\\nRe-training the model with the best hyperparameters: Learning Rate={best_lr}, Hidden Dimension={best_hidden_dim}, Epochs={best_num_epochs}\")\n",
        "\n",
        "for epoch in range(best_num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{best_num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Step 13: Evaluate the re-trained model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 14: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
        "\n",
        "# Print the final evaluation results\n",
        "print(f\"\\nFinal Model Evaluation with Best Hyperparameters:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "t6bF2bMDYwus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.8 **Language Identification**\n",
        "\n",
        "- **Task Description**:\n",
        "  - Language identification involves determining the language of a given text snippet. It is a preliminary step for multilingual text processing.\n",
        "\n",
        "- **Feature Extraction**:\n",
        "  - Features for language identification can include character-level n-grams, word-level n-grams, and common stopwords associated with specific languages.\n",
        "  - Deep learning approaches can leverage embeddings and language model features to enhance accuracy.\n",
        "\n",
        "- **Challenges**:\n",
        "  - Short text snippets (e.g., a single word or phrase) may not provide enough information for accurate classification. Handling code-mixing, where multiple languages appear in the same text, adds complexity.\n"
      ],
      "metadata": {
        "id": "MvFl6P6jRlG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Demonstration"
      ],
      "metadata": {
        "id": "GttS5mHaUUC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The goal of this task is to identify the language of a given text snippet. Language identification is often used as a preliminary step in multilingual text processing, machine translation, and language-specific text analysis."
      ],
      "metadata": {
        "id": "-_HJVP2KUUC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import random\n",
        "\n",
        "# Download the NLTK datasets required for language identification\n",
        "nltk.download('udhr')\n",
        "\n",
        "from nltk.corpus import udhr\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "# The UDHR (Universal Declaration of Human Rights) corpus contains translations in many languages\n",
        "def load_data():\n",
        "    languages = ['English-Latin1', 'French_Francais-Latin1', 'Spanish_Espanol-Latin1', 'German_Deutsch-Latin1']\n",
        "    sentences = []\n",
        "\n",
        "    # Load text samples for each language\n",
        "    for language in languages:\n",
        "        text = udhr.raw(language)\n",
        "        # Split the text into sentences and take a subset for training\n",
        "        sentences.extend([(sent, language.split('-')[0]) for sent in text.split('\\n') if sent])\n",
        "\n",
        "    print(f\"Total number of sentences: {len(sentences)}\")\n",
        "    return sentences\n",
        "\n",
        "# Load the dataset\n",
        "all_data = load_data()\n"
      ],
      "metadata": {
        "id": "3LmngjFaUUC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PefoleF7UUC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "# Extract the text (X) and labels (y) from the dataset\n",
        "X_raw = [text for text, label in all_data]\n",
        "y_raw = [label for _, label in all_data]\n",
        "\n",
        "# Encode the labels as integers\n",
        "label_to_idx = {label: idx for idx, label in enumerate(set(y_raw))}\n",
        "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "y_encoded = [label_to_idx[label] for label in y_raw]\n",
        "\n",
        "print(f\"Number of unique languages: {len(label_to_idx)}\")\n",
        "\n",
        "# Step 4: Convert text data to numerical data using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X_raw).toarray()\n",
        "\n",
        "# Convert the labels to a PyTorch tensor\n",
        "y = torch.tensor(y_encoded, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the features to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "print(f\"Training set size: {X_train.size(0)}\")\n",
        "print(f\"Test set size: {X_test.size(0)}\")\n"
      ],
      "metadata": {
        "id": "yg1zQlGGUUC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yg11HWxIUUC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the neural network model for language identification\n",
        "class LanguageIdentifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LanguageIdentifier, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First hidden layer\n",
        "        self.relu = nn.ReLU()  # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Step 6: Set up the model parameters\n",
        "input_dim = X_train.size(1)  # Number of features (TF-IDF features)\n",
        "hidden_dim = 64  # Number of neurons in the hidden layer (can be tuned)\n",
        "output_dim = len(label_to_idx)  # Number of languages\n",
        "\n",
        "# Initialize the model\n",
        "model = LanguageIdentifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "8Nl-0Y0bUUC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tm84oMm3UUC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Learning rate can be tuned\n",
        "\n",
        "# Step 8: Training loop\n",
        "num_epochs = 20  # Number of training epochs (can be tuned)\n",
        "batch_size = 32  # Size of each batch (can be tuned)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "9J4ZGsiVUUC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IwUMo23fUUC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 10: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "AHE_FylKUUC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "64gZGx4_UUC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter tuning\n",
        "# Try different combinations of learning rates, hidden dimensions, and number of epochs\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in [0.001, 0.01, 0.1]:\n",
        "    for hidden_dim in [32, 64, 128]:\n",
        "        for num_epochs in [10, 20, 30]:\n",
        "            # Initialize the model with current parameters\n",
        "            model = LanguageIdentifier(input_dim, hidden_dim, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model with the current settings\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                permutation = torch.randperm(X_train.size(0))\n",
        "                for i in range(0, X_train.size(0), batch_size):\n",
        "                    indices = permutation[i:i + batch_size]\n",
        "                    batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(X_test)\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            accuracy = accuracy_score(y_test, predicted)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, hidden_dim, num_epochs)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best Hyperparameters - Learning Rate: {best_params[0]}, Hidden Dimension: {best_params[1]}, Epochs: {best_params[2]}\")\n"
      ],
      "metadata": {
        "id": "m8sV3J7NUUC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tVh6V6JcYr3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Re-train the model using the best hyperparameters and evaluate it\n",
        "\n",
        "# Unpack the best hyperparameters\n",
        "best_lr, best_hidden_dim, best_num_epochs = best_params\n",
        "\n",
        "# Re-initialize the model with the best hyperparameters\n",
        "model = LanguageIdentifier(input_dim, best_hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "# Training the model with the best hyperparameters\n",
        "print(f\"\\nRe-training the model with the best hyperparameters: Learning Rate={best_lr}, Hidden Dimension={best_hidden_dim}, Epochs={best_num_epochs}\")\n",
        "\n",
        "for epoch in range(best_num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the batch loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss /= (X_train.size(0) / batch_size)\n",
        "\n",
        "    # Print training progress\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{best_num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Step 13: Evaluate the re-trained model on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "# Step 14: Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
        "\n",
        "# Print the final evaluation results\n",
        "print(f\"\\nFinal Model Evaluation with Best Hyperparameters:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "6KicCXJYYsWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 5.9 **Transition to the Next Section**\n",
        "\n",
        "This section has covered practical examples of text classification, highlighting different tasks, feature extraction techniques, challenges, and example approaches. These examples demonstrate the versatility of text classification in solving diverse NLP problems.\n",
        "\n",
        "The next section, **\"Sequence Classification Techniques,\"** will delve deeper into methods for classifying sequences of text, such as using RNNs, Transformers, and other sequence models to handle tasks where context and word order are critical. These techniques are particularly useful for tasks like POS tagging, NER, and dialogue act classification, which have been introduced here."
      ],
      "metadata": {
        "id": "OSOc_gPzRlDa"
      }
    }
  ]
}