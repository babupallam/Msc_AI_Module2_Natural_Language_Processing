{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "based on the official documentation from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
      ],
      "metadata": {
        "id": "puLwyMk6n5o_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Introduction to Sequence-to-Sequence (Seq2Seq) Translation\n"
      ],
      "metadata": {
        "id": "1XkKNYTTsL75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.1 Objective\n",
        "   - Understand Seq2Seq's purpose: primarily used for transforming input sequences to output sequences, commonly in translation tasks.\n",
        "   - Context of use: often in language translation, speech recognition, and text generation.\n"
      ],
      "metadata": {
        "id": "5fof_qbMsL4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.2 Background\n",
        "   - **Recurrent Neural Networks (RNNs)**:\n",
        "      - RNNs are specialized for sequence data.\n",
        "      - Their architecture allows retaining information from previous time steps, making them useful for sequence-to-sequence tasks.\n",
        "\n",
        "   - **Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)**:\n",
        "      - Popular RNN variants for Seq2Seq as they address the vanishing gradient problem.\n",
        "      - LSTM and GRU can retain information over longer sequences, making them more effective for complex translations.\n"
      ],
      "metadata": {
        "id": "RBJvPM6_sL1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "  - **Long Short-Term Memory (LSTM)** and **Gated Recurrent Units (GRU)** are popular variants of Recurrent Neural Networks (RNNs) widely used in **sequence-to-sequence (Seq2Seq)** models, particularly for tasks like **machine translation, text summarization,** and **speech recognition**. Both LSTM and GRU architectures were developed to address the **vanishing gradient problem** commonly encountered in traditional RNNs, which limits their ability to capture long-term dependencies in sequences.\n",
        "\n",
        " 1. Long Short-Term Memory (LSTM)\n",
        "\n",
        "        LSTMs are designed with a complex internal structure to better capture long-term dependencies in sequences by selectively remembering and forgetting information. Each LSTM cell includes three main gates:\n",
        "\n",
        "        - **Forget Gate**: Decides what information to discard from the cell state.\n",
        "        - **Input Gate**: Determines what new information to add to the cell state.\n",
        "        - **Output Gate**: Controls what information to pass on to the next time step.\n",
        "\n",
        "        **Advantages**:\n",
        "          - **Effective for Long Sequences**: LSTMs are well-suited to handle long sequences due to their ability to manage information through gates. This makes them a popular choice for Seq2Seq tasks with lengthy inputs and outputs.\n",
        "          - **Mitigates Vanishing Gradient**: By maintaining cell states over time and carefully controlling gradient flow through gating mechanisms, LSTMs avoid the vanishing gradient issue seen in vanilla RNNs.\n",
        "\n",
        "        **Use Cases**:\n",
        "          - LSTMs are often used in translation models, especially in contexts where there are lengthy dependencies across sequences, such as translating long sentences or paragraphs where context from earlier in the sentence is relevant at later steps.\n",
        "\n",
        " 2. Gated Recurrent Units (GRU)\n",
        "\n",
        "        GRUs are a simplified version of LSTMs that also use gating mechanisms but have a reduced structure, making them computationally faster. GRUs have two main gates:\n",
        "\n",
        "        - **Update Gate**: Decides how much of the previous memory to retain.\n",
        "        - **Reset Gate**: Controls the influence of the previous state on the current input, which helps in resetting memory for shorter dependencies.\n",
        "\n",
        "        **Advantages**:\n",
        "          - **Computationally Efficient**: GRUs are typically faster to train than LSTMs because they have fewer parameters. This makes GRUs a good choice when computational resources are limited or when faster training is required.\n",
        "          - **Simplicity and Performance**: Despite being simpler than LSTMs, GRUs perform comparably in many tasks and sometimes even outperform LSTMs on shorter sequences or when the dataset is smaller.\n",
        "\n",
        "        **Use Cases**:\n",
        "          - GRUs are used in Seq2Seq models where speed and efficiency are prioritized, such as real-time applications where lower latency is essential.\n",
        "\n",
        "\n",
        "\n",
        " - Why They’re Effective for Seq2Seq Tasks\n",
        "\n",
        "  - **Ability to Retain Context**: Both LSTMs and GRUs can retain context over long sequences, which is crucial for Seq2Seq tasks that involve dependencies across time steps, such as translating a sentence where the meaning of later words depends on the beginning.\n",
        "  - **Better Gradient Flow**: The gating mechanisms allow gradients to propagate more effectively across many time steps, solving the vanishing gradient problem and enabling the models to learn longer dependencies.\n",
        "  - **Flexibility in Sequence Lengths**: LSTMs and GRUs can handle variable-length input and output sequences, making them versatile for tasks like text translation, where sentences vary in length.\n"
      ],
      "metadata": {
        "id": "yvXF_5MiMyyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.3 Model Overview: Seq2Seq Architecture\n",
        "   - **Encoder-Decoder Structure**:\n",
        "      - **Encoder**: Processes input sequence and outputs a \"context vector\" containing encoded information.\n",
        "      - **Decoder**: Takes the context vector and generates the target sequence.\n",
        "      "
      ],
      "metadata": {
        "id": "5hEtI4YIsLyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  - How the Encoder-Decoder Structure Works (In Simple Terms)\n",
        "\n",
        "    -  **What the Encoder Does:**\n",
        "      - Think of the encoder as a person reading a sentence in one language (like English) and trying to understand its meaning.\n",
        "      - The encoder reads each word in the input sentence, one by one, and remembers important details as it goes along.\n",
        "      - When it’s done reading, the encoder “summarizes” the whole sentence’s meaning into a single collection of numbers called the **context vector**.\n",
        "\n",
        "      **Example**:\n",
        "      - Input sentence: \"I love learning languages.\"\n",
        "      - The encoder reads each word, processes it, and creates a context vector representing the sentence’s meaning (e.g., `[0.8, -0.5, 1.2, ...]`).\n",
        "\n",
        "    - **What the Decoder Does:**\n",
        "      - The decoder now takes the context vector and “translates” or generates a new sentence in the target language.\n",
        "      - It generates the output word-by-word, using the information in the context vector to create a meaningful translation.\n",
        "\n",
        "      **Example**:\n",
        "      - With the context vector created from \"I love learning languages,\" the decoder might translate it to French by generating words one by one to form \"J'aime apprendre les langues.\"\n",
        "\n",
        "    - **How They Work Together (Full Process):**\n",
        "      - The encoder first reads and processes the entire input sentence to create the context vector.\n",
        "      - The decoder takes this context vector and starts generating the translated sentence.\n",
        "      - The model is trained to make this process smooth, so it produces the most accurate sentence possible.\n"
      ],
      "metadata": {
        "id": "4qCAYn1ftsW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Demonstration:"
      ],
      "metadata": {
        "id": "oHidzcyFtrvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample encoder and decoder to understand context vector passing\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        # Initialize the encoder RNN module with input and hidden size parameters.\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Define hidden_size as an instance variable to be used within the encoder.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer maps the input tokens to dense vectors of dimension hidden_size.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # GRU (Gated Recurrent Unit) layer to process the embedded input sequence.\n",
        "        # Takes inputs of size hidden_size and outputs hidden_size as well.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Embed the input token and reshape for GRU compatibility: [1, 1, hidden_size]\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        # Pass the embedded input and initial hidden state through the GRU.\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # Returns the output (the encoder hidden state) and the hidden state\n",
        "        # to pass as the initial hidden state to the next time step.\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Initialize hidden state to zero for the first step of the GRU.\n",
        "        # Shape: [num_layers * num_directions, batch_size, hidden_size]\n",
        "        # Here, num_layers * num_directions = 1 (single layer, unidirectional).\n",
        "        return torch.zeros(1, 1, self.hidden_size)\n",
        "\n",
        "# Decoder RNN with a similar GRU-based architecture to handle the encoded context.\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        # Initialize the decoder RNN with hidden size and output size parameters.\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        # Define hidden_size as an instance variable to be used within the decoder.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer to map the target sequence tokens to dense vectors.\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # GRU layer takes in the embedded input and processes it similarly to the encoder.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Linear layer to map GRU output to the vocabulary space (output_size).\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # LogSoftmax activation for normalized log-probabilities over vocabulary.\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Embed the input token for the current time step and reshape: [1, 1, hidden_size]\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        # Apply ReLU activation to add non-linearity to the embedded input.\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        # Pass the processed input and previous hidden state to the GRU.\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        # Map GRU output to vocabulary space and apply LogSoftmax to get log-probabilities.\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "\n",
        "        # Return the output (log-probabilities of vocabulary) and the updated hidden state.\n",
        "        return output, hidden\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "# Encoder input size = vocabulary size, hidden size = dimensionality of embeddings and GRU output.\n",
        "encoder = EncoderRNN(input_size=10, hidden_size=20)\n",
        "\n",
        "# Initialize hidden state for the encoder.\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "# Decoder input size = vocabulary size, hidden size = encoder's output size.\n",
        "decoder = DecoderRNN(hidden_size=20, output_size=10)\n",
        "\n",
        "# Initialize hidden state for the decoder.\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "print(\"Encoder Architecture:\")\n",
        "print(encoder)\n",
        "print(\"-\" * 40)\n",
        "print(\"Decoder Architecture:\")\n",
        "print(decoder)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(\"Encoder Hidden State:\")\n",
        "print(encoder_hidden)\n",
        "shape = encoder_hidden.shape\n",
        "print(\"size\", shape)\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "print(\"Decoder Hidden State:\")\n",
        "print(decoder_hidden)\n",
        "\n",
        "print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBjdWA18tSQx",
        "outputId": "da9c47aa-f8ef-4f18-ce89-aa82985a3094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Architecture:\n",
            "EncoderRNN(\n",
            "  (embedding): Embedding(10, 20)\n",
            "  (gru): GRU(20, 20)\n",
            ")\n",
            "----------------------------------------\n",
            "Decoder Architecture:\n",
            "DecoderRNN(\n",
            "  (embedding): Embedding(10, 20)\n",
            "  (gru): GRU(20, 20)\n",
            "  (out): Linear(in_features=20, out_features=10, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "----------------------------------------\n",
            "Encoder Hidden State:\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "size torch.Size([1, 1, 20])\n",
            "----------------------------------------\n",
            "Decoder Hidden State:\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Encoder Architecture\n",
        "\n",
        "```\n",
        "EncoderRNN(\n",
        "  (embedding): Embedding(10, 20)\n",
        "  (gru): GRU(20, 20)\n",
        ")\n",
        "```\n",
        "\n",
        "- **Embedding Layer**: `Embedding(10, 20)`\n",
        "  - This layer converts input token IDs into dense vectors of size 20. The input vocabulary size is 10, so there are 10 possible token IDs that can be mapped to these embeddings.\n",
        "  - Each input token ID is transformed into a 20-dimensional vector before being passed to the GRU.\n",
        "\n",
        "- **GRU Layer**: `GRU(20, 20)`\n",
        "  - The GRU layer has an input size and hidden state size of 20. This means that each 20-dimensional embedding from the input passes through the GRU, and the GRU also has a 20-dimensional hidden state.\n",
        "  - This GRU layer updates its hidden state based on the current input token’s embedding and the previous hidden state.\n",
        "\n",
        "---\n",
        "\n",
        "##### Decoder Architecture\n",
        "\n",
        "```\n",
        "DecoderRNN(\n",
        "  (embedding): Embedding(10, 20)\n",
        "  (gru): GRU(20, 20)\n",
        "  (out): Linear(in_features=20, out_features=10, bias=True)\n",
        "  (softmax): LogSoftmax(dim=1)\n",
        ")\n",
        "```\n",
        "\n",
        "- **Embedding Layer**: `Embedding(10, 20)`\n",
        "  - Similar to the encoder, the decoder also has an embedding layer that maps input token IDs to 20-dimensional embeddings, with a vocabulary size of 10.\n",
        "\n",
        "- **GRU Layer**: `GRU(20, 20)`\n",
        "  - This GRU layer also has input and hidden sizes of 20. It takes the 20-dimensional embedding of the previous token (or the start token) as input, along with the hidden state (which initially comes from the encoder).\n",
        "  \n",
        "- **Output Layer**: `Linear(in_features=20, out_features=10, bias=True)`\n",
        "  - A fully connected (linear) layer that maps the 20-dimensional GRU output to a 10-dimensional vector. Each dimension in this output vector represents the logit for a token in the output vocabulary.\n",
        "  \n",
        "- **Softmax Layer**: `LogSoftmax(dim=1)`\n",
        "  - The LogSoftmax function is applied along dimension 1 to convert the output logits into log-probabilities. This layer is commonly used in classification tasks with `CrossEntropyLoss`, as it stabilizes the calculations by working in log space.\n",
        "\n",
        "---\n",
        "\n",
        "##### Initial Hidden State\n",
        "\n",
        "The initial hidden state for both the encoder and decoder is a tensor of zeros, which serves as a starting point for the GRU layers.\n",
        "\n",
        "###### Encoder Hidden State\n",
        "\n",
        "```\n",
        "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
        "size torch.Size([1, 1, 20])\n",
        "```\n",
        "\n",
        "- **Shape Explanation**: `torch.Size([1, 1, 20])`\n",
        "  - The hidden state has a shape of `[1, 1, 20]`:\n",
        "    - `1` (first dimension) represents the number of layers in the GRU (in this case, it’s a single-layer GRU).\n",
        "    - `1` (second dimension) represents the batch size. In this case, each example is processed one at a time.\n",
        "    - `20` (third dimension) represents the size of the hidden state, matching the dimensionality specified in the GRU.\n",
        "\n",
        "- **Contents**: The tensor is initialized to zeros, which is typical for the initial hidden state in RNN-based models.\n",
        "\n",
        "###### Decoder Hidden State\n",
        "\n",
        "```\n",
        "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
        "```\n",
        "\n",
        "- The initial hidden state of the decoder is also set to zeros here. However, in a typical Seq2Seq setup, the decoder’s initial hidden state would be initialized with the final hidden state from the encoder, allowing it to \"carry over\" the context from the input sequence.\n"
      ],
      "metadata": {
        "id": "bVmbWLzZOYjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Encoder Output (Context Vector)**:\n",
        "      - Captures the entire source sentence's meaning and passes it to the decoder.\n",
        "      - Decoder then uses this context to generate the target sentence.\n"
      ],
      "metadata": {
        "id": "66f2j_CmsLvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.4 Training Focus\n",
        "   - During training, both encoder and decoder are optimized to minimize the difference between predicted and actual target sequences.\n",
        "   - **Teacher Forcing**: Method where the model is fed the actual output sequence for better learning.\n"
      ],
      "metadata": {
        "id": "MkEQ7gW0sLtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.5 Practical Use Cases of Seq2Seq Models\n",
        "   - **Language Translation**: Converts sentences from one language to another.\n",
        "   - **Chatbots**: Generates appropriate responses based on input queries.\n",
        "   - **Text Summarization**: Condenses long pieces of text into brief summaries.\n"
      ],
      "metadata": {
        "id": "LLcfEp8jsLp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Demonstration Checkpoint\n",
        "   - Code to initialize both encoder and decoder, with an example of passing data through the encoder to produce a hidden state:"
      ],
      "metadata": {
        "id": "5mZ8AcYisLl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Encoder reads the sentence \"I love learning languages.\"\n",
        "input_sentence = torch.tensor([1, 2, 3, 4])  # Assume these integers represent the tokens of the sentence\n",
        "print(\"Input Sentence Tokens:\", input_sentence.tolist())\n",
        "\n",
        "# Initialize the encoder's hidden state (all zeros initially)\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "print(\"Initial Encoder Hidden State:\", encoder_hidden)\n",
        "print(\"size\", encoder_hidden.size())\n",
        "\"\"\"\n",
        "encoder_hidden.size():\n",
        "  - return torch.zeros(1, 1, self.hidden_size) ([num_layers * num_directions, batch_size, hidden_size])\n",
        "  - [1, 1, 20]\n",
        "  - [num_layers * num_directions, batch_size, hidden_size]\n",
        "  - In this:\n",
        "    - num_layers: This is the number of RNN layers. Since we didn't specify multiple layers in the EncoderRNN, it defaults to 1.\n",
        "    - num_directions: This indicates whether the RNN is bidirectional. Here, it's a simple (unidirectional) GRU, so num_directions is 1.\n",
        "    - batch_size: The size of each batch. Here, it's set to 1 because we are processing one sequence (sentence) at a time.\n",
        "    - hidden_size: The dimensionality of the hidden layer, which we specified as 20 in EncoderRNN.\n",
        "\n",
        "    - num_layers * num_directions = 1 * 1 = 1\n",
        "    - batch_size = 1\n",
        "    - hidden_size = 20\n",
        "    - [1, 1, 20]\n",
        "\"\"\"\n",
        "print(\"\\nEncoder Architecture:\")\n",
        "print(encoder)\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyPJbUitfIz",
        "outputId": "c4b1e5c6-fb05-481e-b2ec-6b689435b805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence Tokens: [1, 2, 3, 4]\n",
            "Initial Encoder Hidden State: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "size torch.Size([1, 1, 20])\n",
            "\n",
            "Encoder Architecture:\n",
            "EncoderRNN(\n",
            "  (embedding): Embedding(10, 20)\n",
            "  (gru): GRU(20, 20)\n",
            ")\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encoder processes each word, updating its hidden state at each step\n",
        "for i, word in enumerate(input_sentence):\n",
        "    print(f\"\\nEncoding word {i+1} with token {word.item()}\")\n",
        "    encoder_output, encoder_hidden = encoder(word, encoder_hidden)\n",
        "    print(\"Encoder Output at this step:\", encoder_output)\n",
        "    print(\"Updated Encoder Hidden State:\", encoder_hidden)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# The final hidden state of the encoder is the \"context vector\"\n",
        "context_vector = encoder_hidden\n",
        "print(\"\\nFinal Context Vector from Encoder:\", context_vector)\n",
        "print(shape := context_vector.shape)\n",
        "\"\"\"\n",
        "shape: torch.Size([1, 1, 20])\n",
        "  - [num_layers * num_directions, batch_size, hidden_size]\n",
        "  - [1, 1, 20]\n",
        "  - [num_layers * num_directions, batch_size, hidden_size]\n",
        "  - in this,\n",
        "    - num_layers: This is the number of RNN layers. Since we didn't specify multiple layers in the EncoderRNN, it defaults to 1.\n",
        "    - num_directions: This indicates whether the RNN is bidirectional. Here, it's a simple (unidirectional) GRU, so num_directions is 1.\n",
        "    - batch_size: The size of each batch. Here, it's set to 1 because we are processing one sequence (sentence) at a time.\n",
        "    - hidden_size: The dimensionality of the hidden layer, which we specified as 20 in EncoderRNN.\n",
        "    - num_layers * num_directions = 1 * 1 = 1\n",
        "    - batch_size = 1\n",
        "    - hidden_size = 20\n",
        "    - [1, 1, 20]\n",
        "\"\"\"\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2Eop5Gxt5w",
        "outputId": "fd1eec71-f058-4eb8-965f-c50ee9479370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoding word 1 with token 1\n",
            "Encoder Output at this step: tensor([[[-0.0648, -0.0025, -0.0313,  0.2967, -0.2079,  0.0492, -0.0844,\n",
            "           0.0437,  0.1022, -0.1832,  0.1682,  0.1017, -0.0553,  0.1413,\n",
            "           0.2185,  0.2790, -0.1875,  0.3304, -0.0245, -0.2174]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Updated Encoder Hidden State: tensor([[[-0.0648, -0.0025, -0.0313,  0.2967, -0.2079,  0.0492, -0.0844,\n",
            "           0.0437,  0.1022, -0.1832,  0.1682,  0.1017, -0.0553,  0.1413,\n",
            "           0.2185,  0.2790, -0.1875,  0.3304, -0.0245, -0.2174]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "----------------------------------------\n",
            "\n",
            "Encoding word 2 with token 2\n",
            "Encoder Output at this step: tensor([[[-0.0248, -0.1319, -0.2588, -0.1063, -0.3861, -0.1231,  0.2801,\n",
            "          -0.0507,  0.3570, -0.0026,  0.6982, -0.0689, -0.3127,  0.0483,\n",
            "          -0.4812, -0.3486,  0.1057, -0.2017, -0.4577, -0.4893]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Updated Encoder Hidden State: tensor([[[-0.0248, -0.1319, -0.2588, -0.1063, -0.3861, -0.1231,  0.2801,\n",
            "          -0.0507,  0.3570, -0.0026,  0.6982, -0.0689, -0.3127,  0.0483,\n",
            "          -0.4812, -0.3486,  0.1057, -0.2017, -0.4577, -0.4893]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "----------------------------------------\n",
            "\n",
            "Encoding word 3 with token 3\n",
            "Encoder Output at this step: tensor([[[ 0.0445,  0.0952,  0.0232,  0.0076, -0.5316, -0.0896,  0.2765,\n",
            "           0.1050,  0.2733, -0.0257,  0.4356,  0.3127, -0.3198,  0.2148,\n",
            "           0.1419,  0.1997, -0.2908, -0.0462, -0.1734, -0.5076]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Updated Encoder Hidden State: tensor([[[ 0.0445,  0.0952,  0.0232,  0.0076, -0.5316, -0.0896,  0.2765,\n",
            "           0.1050,  0.2733, -0.0257,  0.4356,  0.3127, -0.3198,  0.2148,\n",
            "           0.1419,  0.1997, -0.2908, -0.0462, -0.1734, -0.5076]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "----------------------------------------\n",
            "\n",
            "Encoding word 4 with token 4\n",
            "Encoder Output at this step: tensor([[[ 0.3055,  0.1887, -0.3194,  0.1775, -0.5275, -0.3057,  0.3027,\n",
            "           0.3125, -0.0330, -0.1627,  0.4594, -0.4864,  0.2740,  0.1488,\n",
            "           0.0589,  0.4023, -0.1008,  0.2835, -0.3795, -0.0149]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Updated Encoder Hidden State: tensor([[[ 0.3055,  0.1887, -0.3194,  0.1775, -0.5275, -0.3057,  0.3027,\n",
            "           0.3125, -0.0330, -0.1627,  0.4594, -0.4864,  0.2740,  0.1488,\n",
            "           0.0589,  0.4023, -0.1008,  0.2835, -0.3795, -0.0149]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "----------------------------------------\n",
            "\n",
            "Final Context Vector from Encoder: tensor([[[ 0.3055,  0.1887, -0.3194,  0.1775, -0.5275, -0.3057,  0.3027,\n",
            "           0.3125, -0.0330, -0.1627,  0.4594, -0.4864,  0.2740,  0.1488,\n",
            "           0.0589,  0.4023, -0.1008,  0.2835, -0.3795, -0.0149]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 20])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Decoder starts with this context vector and begins generating the translation\n",
        "output_sentence = []  # Initialize an empty list to store the generated tokens of the output sentence\n",
        "decoder_input = torch.tensor([0])  # Initial decoder input (often a 'start' token)\n",
        "print(\"\\nInitial Decoder Input (start token):\", decoder_input.item())\n",
        "\n",
        "# Begin the decoding process to generate the output sentence\n",
        "for i in range(4):  # Generate a 4-word output sentence as an example\n",
        "    print(f\"\\nDecoding step {i+1}\")\n",
        "\n",
        "    # Pass the current input and hidden state (context vector) to the decoder\n",
        "    decoder_output, context_vector = decoder(decoder_input, context_vector)\n",
        "\n",
        "    # decoder_output contains log-probabilities for each token in the vocabulary\n",
        "    print(\"Decoder Output (log-probabilities over vocabulary):\", decoder_output)\n",
        "\n",
        "    # Choose the word with the highest probability\n",
        "    next_word = decoder_output.argmax(dim=1).item()\n",
        "    output_sentence.append(next_word)\n",
        "    print(\"Generated Word (token):\", next_word)\n",
        "\n",
        "    # Set the decoder's next input to the word just generated\n",
        "    decoder_input = torch.tensor([next_word])\n",
        "    print(\"Next Decoder Input:\", decoder_input.item())\n",
        "\n",
        "# Output the final generated sentence\n",
        "print(\"\\nGenerated Output Sentence Tokens:\", output_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpg6WDHNyDEW",
        "outputId": "2074e574-87f7-42bd-db0f-d9909b783b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial Decoder Input (start token): 0\n",
            "\n",
            "Decoding step 1\n",
            "Decoder Output (log-probabilities over vocabulary): tensor([[-2.6561, -2.0425, -2.5580, -2.0883, -2.0079, -2.3423, -2.1447, -2.6558,\n",
            "         -2.3995, -2.4055]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Generated Word (token): 4\n",
            "Next Decoder Input: 4\n",
            "\n",
            "Decoding step 2\n",
            "Decoder Output (log-probabilities over vocabulary): tensor([[-2.8051, -2.1524, -2.4980, -1.9632, -2.0954, -2.2619, -2.1049, -2.6580,\n",
            "         -2.4242, -2.3753]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Generated Word (token): 3\n",
            "Next Decoder Input: 3\n",
            "\n",
            "Decoding step 3\n",
            "Decoder Output (log-probabilities over vocabulary): tensor([[-2.7122, -2.2422, -2.5320, -1.9993, -2.0923, -2.2665, -2.0618, -2.5692,\n",
            "         -2.4998, -2.3080]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Generated Word (token): 3\n",
            "Next Decoder Input: 3\n",
            "\n",
            "Decoding step 4\n",
            "Decoder Output (log-probabilities over vocabulary): tensor([[-2.6756, -2.2843, -2.5348, -2.0194, -2.0989, -2.2689, -2.0460, -2.5284,\n",
            "         -2.5392, -2.2702]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Generated Word (token): 3\n",
            "Next Decoder Input: 3\n",
            "\n",
            "Generated Output Sentence Tokens: [4, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2: Data Preparation and Processing\n"
      ],
      "metadata": {
        "id": "ifByi37FsLdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.1 Objective\n",
        "   - Learn how to format language data for input into Seq2Seq models.\n",
        "   - Standardize data formats to ensure smooth processing by the model.\n"
      ],
      "metadata": {
        "id": "7cEzWZ6jsLZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.2 Steps in Data Preparation\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - **Purpose**: Convert each sentence into tokens (words or subwords) that the model can process.\n",
        "   - Each token represents a word or part of a word, transformed into a unique number (integer).\n",
        "   - **Example**:\n",
        "      - Sentence: \"I love learning languages.\"\n",
        "      - Tokens: `[I, love, learning, languages, .]`\n",
        "      - Token IDs: `[1, 2, 3, 4, 5]`\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "PVIN-L3usLU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Importing the word_tokenize function from the Natural Language Toolkit (NLTK) library.\n",
        "# This function splits a sentence into individual words or tokens.\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Defining a sentence to tokenize.\n",
        "sentence = \"I love learning languages.\"\n",
        "\n",
        "# Converting the sentence to lowercase with .lower() to ensure uniformity.\n",
        "# This step is useful when we want case-insensitive processing.\n",
        "# For example, \"Learning\" and \"learning\" would both be treated as the same word.\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "# Printing the resulting tokens to the console.\n",
        "# Expected output: a list of lowercase words, e.g., ['i', 'love', 'learning', 'languages', '.']\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MUSjJ66H-TE",
        "outputId": "c501966a-1c34-4b94-fb14-67892a1e4a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['i', 'love', 'learning', 'languages', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Vocabulary Creation**:\n",
        "   - **Purpose**: Build a vocabulary, mapping each unique token to a unique index, to handle known words.\n",
        "   - This vocabulary is used for converting words to their respective token IDs during model training.\n",
        "   - **Example**:\n",
        "      - Vocabulary: `{\"I\": 1, \"love\": 2, \"learning\": 3, \"languages\": 4, \".\": 5}`\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "Up6DclIDsLQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a vocabulary dictionary from the list of tokens.\n",
        "# The set(tokens) function removes duplicate words, as a vocabulary only needs unique terms.\n",
        "# Using a dictionary comprehension, each word in the vocabulary is mapped to a unique index.\n",
        "# The enumerate function assigns each word an index, starting from 1.\n",
        "# Starting from 1 (instead of 0) is common in NLP when 0 is reserved for padding tokens or special purposes.\n",
        "\n",
        "vocabulary = {word: idx for idx, word in enumerate(set(tokens), start=1)}\n",
        "\n",
        "# Printing the created vocabulary dictionary.\n",
        "# Expected output: a dictionary where each unique word in `tokens` has a unique index,\n",
        "# e.g., {'i': 1, 'love': 2, 'learning': 3, 'languages': 4, '.': 5}\n",
        "print(\"Vocabulary:\", vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk1UuAYlIGu-",
        "outputId": "934b20ff-58be-4647-f474-129edf517bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'.': 1, 'languages': 2, 'i': 3, 'love': 4, 'learning': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Padding Sequences**:\n",
        "   - **Purpose**: Pad each sentence to a fixed length so they all match the input shape required by the model.\n",
        "   - **Padding Process**: Add special tokens (e.g., `<PAD>`) to shorter sentences so they match the longest sequence in the batch.\n",
        "   - **Example**:\n",
        "      - Sentences before padding: `[I love], [I love learning languages]`\n",
        "      - After padding to a length of 5: `[I love <PAD> <PAD>], [I love learning languages]`\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "LcP3zT3IsLNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the pad_sequences function from TensorFlow's Keras module.\n",
        "# This function is used to ensure that all sequences (lists of token IDs) have the same length,\n",
        "# which is necessary for batch processing in machine learning models.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Defining a list of tokenized sentences where each sentence is represented as a list of token IDs.\n",
        "# For example, `tokenized_sentences` might represent two sentences that have been converted to sequences of word indices:\n",
        "# - The first sentence has the tokens [1, 2].\n",
        "# - The second sentence has the tokens [1, 2, 3, 4].\n",
        "tokenized_sentences = [[1, 2], [1, 2, 3, 4]]\n",
        "\n",
        "# Applying padding to make each sequence the same length.\n",
        "# - `maxlen=5` specifies that all sequences should have a length of 5.\n",
        "# - `padding='post'` indicates that padding should be added at the end (after the tokens).\n",
        "# If a sequence is shorter than `maxlen`, zeros are added to the end until it reaches the desired length.\n",
        "# If a sequence is longer than `maxlen`, it will be truncated to fit.\n",
        "padded_sentences = pad_sequences(tokenized_sentences, maxlen=5, padding='post')\n",
        "\n",
        "print(\"Padded Sentences:\", padded_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2MC0d-GIRQh",
        "outputId": "5c77400b-8789-4c99-da0a-52caba1f410d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Sentences: [[1 2 0 0 0]\n",
            " [1 2 3 4 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Handling Out-of-Vocabulary (OOV) Words**:\n",
        "   - **Purpose**: Handle words that are not in the vocabulary with a special token like `<UNK>`.\n",
        "   - This ensures the model can process unknown words without errors.\n",
        "   - **Example**:\n",
        "      - Original sentence: \"I enjoy learning new languages.\"\n",
        "      - With `<UNK>` token for “enjoy” and “new” if not in vocabulary: `[I <UNK> learning <UNK> languages .]`\n"
      ],
      "metadata": {
        "id": "FxC5yaCNsLIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. **Batching and Shuffling**:\n",
        "   - **Purpose**: Organize the data into batches and shuffle them to improve model training.\n",
        "   - Batching helps manage memory better, and shuffling prevents the model from learning any particular order.\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "vRrG4_VtsLEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules from PyTorch.\n",
        "# - torch: the core library for tensor operations.\n",
        "# - DataLoader: a utility that loads data from a dataset in batches, allowing efficient data handling.\n",
        "# - TensorDataset: a dataset wrapper that combines input and target tensors, useful for supervised learning tasks.\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Defining example tensors for input and target data.\n",
        "# Here, `inputs` is a 3x3 tensor where each row represents a sample.\n",
        "# `targets` is also a 3x3 tensor where each row represents the target output for a corresponding input sample.\n",
        "inputs = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "targets = torch.tensor([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n",
        "\n",
        "# Creating a TensorDataset to hold the input and target tensors together.\n",
        "# The dataset allows the DataLoader to treat each pair of input and target as a single sample.\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "\n",
        "# Setting up a DataLoader to load data from the dataset in batches.\n",
        "# - `batch_size=2` specifies that each batch will contain 2 samples.\n",
        "# - `shuffle=True` means the data will be shuffled each time a new epoch starts, which is useful for training.\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Iterating through the DataLoader to access batches of data.\n",
        "# Each iteration returns a batch, which is a tuple containing a batch of inputs and a batch of targets.\n",
        "for batch in dataloader:\n",
        "    # Printing the input part of the batch.\n",
        "    print(\"Batch of Inputs:\", batch[0])\n",
        "\n",
        "    # Printing the target part of the batch.\n",
        "    print(\"Batch of Targets:\", batch[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QAM1BHcIa33",
        "outputId": "f12536e2-ba09-43c7-f6dc-5b813239981b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of Inputs: tensor([[1, 2, 3],\n",
            "        [7, 8, 9]])\n",
            "Batch of Targets: tensor([[9, 8, 7],\n",
            "        [3, 2, 1]])\n",
            "Batch of Inputs: tensor([[4, 5, 6]])\n",
            "Batch of Targets: tensor([[6, 5, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3: Building the Seq2Seq Model\n"
      ],
      "metadata": {
        "id": "H25xIzr-sLA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3.1 Objective\n",
        "   - Understand how to set up and structure a Seq2Seq model, specifically by building the **Encoder** and **Decoder** components.\n",
        "   - Each part has a specialized role in transforming the input sequence to the output sequence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oIwjS6OysK9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3.2 Components of the Seq2Seq Model\n"
      ],
      "metadata": {
        "id": "S8qkh57ssK5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Encoder**:\n",
        "   - **Purpose**: Read and \"understand\" the input sentence by encoding it into a **context vector**.\n",
        "   - The encoder uses a Recurrent Neural Network (RNN), such as GRU or LSTM, to process the input sequence token by token and update its hidden state with each token.\n",
        "   - **Key Parameters**:\n",
        "      - `input_size`: Size of the vocabulary (number of unique tokens).\n",
        "      - `hidden_size`: Dimension of the hidden state.\n",
        "   - **Code Demonstration**:"
      ],
      "metadata": {
        "id": "eEehe0HZsK12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the neural network module from PyTorch, which contains various neural network layers and functions.\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Defining an encoder RNN class, which is a subclass of nn.Module, PyTorch's base class for neural networks.\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        # Initializing the parent class with super to inherit nn.Module's properties and methods.\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Defining the hidden state size, which determines the number of features in the hidden layer.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Creating an embedding layer, which converts input token IDs into dense vectors.\n",
        "        # `input_size` is the vocabulary size, and `hidden_size` is the dimensionality of the embeddings.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # Defining a GRU (Gated Recurrent Unit) layer, which is a type of RNN layer.\n",
        "        # The GRU takes embeddings of `hidden_size` as input and outputs a hidden state of the same size.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Passing the input token ID through the embedding layer, converting it to a dense vector.\n",
        "        # .view(1, 1, -1) reshapes the embedding to match GRU's expected input shape:\n",
        "        # (sequence_length, batch_size, embedding_size), where sequence length and batch size are both 1 here.\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        # Passing the embedded input and hidden state to the GRU.\n",
        "        # GRU returns:\n",
        "        # - `output`: the output of the current time step.\n",
        "        # - `hidden`: the updated hidden state, which is carried over to the next time step.\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # Returning the output and hidden state.\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Initializing the hidden state with zeros.\n",
        "        # The shape (1, 1, hidden_size) corresponds to (num_layers, batch_size, hidden_size),\n",
        "        # where num_layers is 1 and batch size is 1 in this case.\n",
        "        return torch.zeros(1, 1, self.hidden_size)\n",
        "\n",
        "# Example usage of the EncoderRNN\n",
        "encoder = EncoderRNN(input_size=10, hidden_size=20)  # Creating an Encoder with vocabulary size 10 and hidden state size 20.\n",
        "\n",
        "# Defining an example input tensor with a single token ID (e.g., ID `1`).\n",
        "input_tensor = torch.tensor([1])\n",
        "\n",
        "# Initializing the encoder's hidden state.\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "# Passing the input tensor and hidden state through the encoder.\n",
        "encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "# Printing the output of the encoder (from the GRU) and the final hidden state.\n",
        "print(\"Encoder Output:\", encoder_output)\n",
        "print(\"Encoder Hidden State:\", encoder_hidden)\n",
        "print(shape := encoder_hidden.shape)\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McVZSMmcIh9Z",
        "outputId": "dbfe5a2d-4fd4-489f-92af-b48df1109acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output: tensor([[[ 0.1837,  0.3054,  0.4854,  0.4326,  0.4158,  0.2499,  0.0676,\n",
            "           0.1700, -0.0586,  0.1174,  0.1410,  0.0982,  0.3166, -0.4173,\n",
            "          -0.1035, -0.5546,  0.4376, -0.1398,  0.1916,  0.3888]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Encoder Hidden State: tensor([[[ 0.1837,  0.3054,  0.4854,  0.4326,  0.4158,  0.2499,  0.0676,\n",
            "           0.1700, -0.0586,  0.1174,  0.1410,  0.0982,  0.3166, -0.4173,\n",
            "          -0.1035, -0.5546,  0.4376, -0.1398,  0.1916,  0.3888]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 20])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Decoder**:\n",
        "   - **Purpose**: Generate the translated sentence in the target language, word by word, based on the **context vector** from the encoder.\n",
        "   - Each token generated by the decoder is fed back into it to produce the next token, which continues until an end-of-sequence token is generated.\n",
        "   - **Key Parameters**:\n",
        "      - `hidden_size`: Matches the encoder’s hidden size.\n",
        "      - `output_size`: Vocabulary size of the target language.\n",
        "   - **Teacher Forcing**: During training, we can provide the actual target word as input to the decoder to improve learning efficiency.\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "9-rGEG_3Ij03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a DecoderRNN class, inheriting from nn.Module for PyTorch compatibility.\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        # Initializing the parent class with super to inherit nn.Module properties.\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        # Setting up the hidden state size.\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Defining an embedding layer to convert token IDs to dense vectors.\n",
        "        # `output_size` represents the vocabulary size (total possible output tokens),\n",
        "        # and `hidden_size` is the embedding dimensionality.\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # Initializing a GRU layer that processes the embedded input.\n",
        "        # The GRU takes the `hidden_size` as both input and output size for simplicity.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Defining a linear layer that maps the GRU output to the vocabulary size.\n",
        "        # This layer produces logits for each possible output token.\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Defining a softmax layer to convert logits into log-probabilities for the output tokens.\n",
        "        # LogSoftmax is used here to stabilize computations and is compatible with loss functions like NLLLoss.\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Passing the input token ID through the embedding layer, converting it to a dense vector.\n",
        "        # .view(1, 1, -1) reshapes it for compatibility with the GRU layer input format\n",
        "        # (sequence_length=1, batch_size=1, embedding_size=hidden_size).\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        # Applying a ReLU activation function to add non-linearity to the embedding.\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        # Passing the embedded input and hidden state to the GRU layer.\n",
        "        # The GRU layer outputs:\n",
        "        # - `output`: the output vector at the current time step.\n",
        "        # - `hidden`: the updated hidden state to pass to the next time step.\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        # Mapping the GRU output to the output vocabulary size using the linear layer.\n",
        "        # output[0] takes the sequence dimension (1) out, as we're processing only one step.\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "\n",
        "        # Returning the output (log-probabilities over the vocabulary) and the updated hidden state.\n",
        "        return output, hidden\n",
        "\n",
        "# Example usage of the DecoderRNN\n",
        "decoder = DecoderRNN(hidden_size=20, output_size=10)  # Creating a decoder with hidden size 20 and output vocabulary size 10.\n",
        "\n",
        "# Defining an initial input token ID (e.g., ID `1`, representing a 'start' token).\n",
        "decoder_input = torch.tensor([1])\n",
        "\n",
        "# Using the encoder's last hidden state as the initial hidden state for the decoder.\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "# Passing the initial decoder input and hidden state through the decoder.\n",
        "decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "# Printing the output of the decoder (log-probabilities of the next token) and the final hidden state.\n",
        "print(\"Decoder Output:\", decoder_output)\n",
        "print(\"Decoder Hidden State:\", decoder_hidden)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XLWKqw6IkRW",
        "outputId": "96acace6-f525-4545-ba06-b61bc71cc03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Output: tensor([[-2.3317, -2.3014, -2.2132, -2.1056, -2.3385, -2.2453, -2.2450, -2.6170,\n",
            "         -2.5809, -2.1663]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Decoder Hidden State: tensor([[[-0.0403,  0.3763, -0.3927,  0.3389, -0.3842, -0.0567, -0.2804,\n",
            "           0.0584,  0.1843,  0.3884,  0.2937,  0.0597, -0.0106, -0.2252,\n",
            "          -0.2182,  0.2580, -0.0408,  0.2065,  0.3312,  0.0425]]],\n",
            "       grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3.3 Encoder-Decoder Combined Workflow\n",
        "   - During translation, we:\n",
        "      1. Pass the input sentence through the encoder to get the context vector.\n",
        "      2. Initialize the decoder with this context vector and generate each word in the output sequence.\n",
        "\n",
        "   - **Code Demonstration** (Putting it all together):\n"
      ],
      "metadata": {
        "id": "NYAC7HUFInl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Encoding the input sentence\n",
        "\n",
        "# Initialize the encoder's hidden state.\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "# Define an example tokenized input sentence. Each integer represents a word token ID in the sentence.\n",
        "input_sentence = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "# Pass each token in the input sentence sequentially through the encoder.\n",
        "# This loop simulates feeding tokens one-by-one into the encoder, updating its hidden state each time.\n",
        "for token in input_sentence:\n",
        "    # Process the token through the encoder and update the hidden state.\n",
        "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
        "\n",
        "\n",
        "print(\"Encoder Output:\", encoder_output)\n",
        "print(\"Encoder Hidden State:\", encoder_hidden)\n",
        "print(shape := encoder_hidden.shape)\n",
        "print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpZnG21VIpBS",
        "outputId": "508ec717-9554-4a94-851a-0017f6c6fdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output: tensor([[[-0.0503,  0.2322, -0.4237,  0.1497,  0.0010, -0.3578,  0.2943,\n",
            "          -0.0563,  0.1356, -0.1272, -0.0036,  0.1375,  0.3599, -0.2305,\n",
            "          -0.4118,  0.4302,  0.1737,  0.4199,  0.2408,  0.1447]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "Encoder Hidden State: tensor([[[-0.0503,  0.2322, -0.4237,  0.1497,  0.0010, -0.3578,  0.2943,\n",
            "          -0.0563,  0.1356, -0.1272, -0.0036,  0.1375,  0.3599, -0.2305,\n",
            "          -0.4118,  0.4302,  0.1737,  0.4199,  0.2408,  0.1447]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 20])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Decoding to generate an output sentence\n",
        "\n",
        "# Set the initial input for the decoder to a \"start\" token (commonly used to signal the start of decoding).\n",
        "decoder_input = torch.tensor([1])\n",
        "\n",
        "# Initialize the decoder's hidden state with the final hidden state from the encoder.\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "# Define an empty list to store the predicted tokens in the generated sentence.\n",
        "output_sentence = []\n",
        "\n",
        "# Generate a sentence by decoding one word at a time.\n",
        "# Here, we limit the loop to 4 iterations to generate a sentence of 4 tokens.\n",
        "for _ in range(4):\n",
        "    # Pass the decoder input and current hidden state through the decoder.\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "    # Get the predicted token by finding the index of the highest value in the output log-probabilities.\n",
        "    # `argmax(dim=1)` gives the index of the most likely token in the vocabulary.\n",
        "    predicted_token = decoder_output.argmax(dim=1).item()\n",
        "\n",
        "    # Append the predicted token to the output sentence.\n",
        "    output_sentence.append(predicted_token)\n",
        "\n",
        "    # Set the decoder input for the next step to the predicted token,\n",
        "    # which enables the decoder to generate the next token based on previous output.\n",
        "    decoder_input = torch.tensor([predicted_token])\n",
        "\n",
        "# Print the generated sequence of token IDs.\n",
        "print(\"Generated Output Sentence:\", output_sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk6ttTdQTezo",
        "outputId": "52a4e983-7c22-44e8-ea3a-d7bb51957885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output Sentence: [0, 3, 0, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Section 4: Training the Model\n"
      ],
      "metadata": {
        "id": "W8DaS_M0zZxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.1 Objective\n",
        "   - Master the process of training Seq2Seq models, focusing on loss calculation, backpropagation, and optimization.\n",
        "   - Use techniques like **teacher forcing** to improve model learning.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7sZmK9SozZ2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.2 Key Steps in Training\n"
      ],
      "metadata": {
        "id": "d3KlXvnhzZ6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Loss Calculation**:\n",
        "   - **Purpose**: Measure the difference between the model's predicted sequence and the actual target sequence.\n",
        "   - **Cross-Entropy Loss** is commonly used for Seq2Seq models, as it calculates the error across all predicted tokens.\n",
        "   - **Example**:\n",
        "      - Predicted sentence: \"I like learning.\"\n",
        "      - Target sentence: \"I enjoy learning.\"\n",
        "      - Cross-Entropy Loss penalizes each word in the predicted sentence based on how close it is to the corresponding target word.\n",
        "\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "u3IRzGw9zZ_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Define the loss function using CrossEntropyLoss, which is suitable for multi-class classification tasks.\n",
        "# CrossEntropyLoss combines `LogSoftmax` and `NLLLoss` in one step, so the `predicted` tensor should contain raw logits.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an example tensor of predicted logits for each class in each sample.\n",
        "# Each row represents a sample, and each column represents the logit for a class.\n",
        "# Here, the shape (3, 3) indicates 3 samples and 3 possible classes for each sample.\n",
        "# `requires_grad=True` allows the loss to backpropagate through these predictions during training.\n",
        "predicted = torch.tensor([[0.1, 0.9, 0.8],\n",
        "                          [0.2, 0.3, 0.5],\n",
        "                          [0.6, 0.3, 0.1]], requires_grad=True)\n",
        "\n",
        "# Define the target tensor, which contains the correct class indices for each sample.\n",
        "# Each integer in `target` represents the correct class for the corresponding row in `predicted`.\n",
        "# For example, `target[0] = 1` means the correct class for the first sample is class 1.\n",
        "target = torch.tensor([1, 0, 2])  # Indexes of correct tokens\n",
        "\n",
        "# Calculate the loss between the predicted logits and target labels.\n",
        "# CrossEntropyLoss expects `predicted` to contain raw logits, and `target` to contain class indices.\n",
        "loss = criterion(predicted, target)\n",
        "\n",
        "# Output the loss value. `loss.item()` extracts the scalar value of the loss.\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMk5VGWLJN6x",
        "outputId": "a1aeb449-d53a-4d0a-99e5-7389d501fc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.1497679948806763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Optimization**:\n",
        "   - **Purpose**: Update model parameters to minimize the loss.\n",
        "   - Popular optimizers like **Adam** or **SGD** are typically used for training Seq2Seq models.\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "oKGgwjLszaCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the optimizer using Adam, a popular optimization algorithm that adjusts learning rates adaptively.\n",
        "# - `list(encoder.parameters()) + list(decoder.parameters())` combines the parameters of the encoder and decoder.\n",
        "#   This allows the optimizer to update weights in both models simultaneously during training.\n",
        "# - `lr=0.01` sets the learning rate for the optimizer. The learning rate determines the step size at each iteration.\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.01)\n"
      ],
      "metadata": {
        "id": "SPkkq-qpJTAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Training Loop**:\n",
        "   - **Purpose**: Perform multiple passes over the data, updating model weights each time.\n",
        "   - **Steps**:\n",
        "      1. **Forward Pass**: Pass data through the encoder and decoder to get predictions.\n",
        "      2. **Compute Loss**: Calculate the loss using the predicted output and actual target.\n",
        "      3. **Backpropagation**: Compute gradients for each model parameter.\n",
        "      4. **Optimization Step**: Adjust model parameters based on gradients to reduce loss.\n",
        "\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "EjXiKdONzaFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input and target tensor for one training iteration (representing a pair of sentences).\n",
        "input_tensor = torch.tensor([1, 2, 3, 4])  # Example input sequence (token IDs)\n",
        "target_tensor = torch.tensor([5, 6, 7, 8])  # Example target sequence (token IDs)\n",
        "\n",
        "# Initialize the encoder's hidden state to start the encoding process.\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "# Step 1: Encoder Forward Pass\n",
        "# Clear previous gradients by zeroing them, a necessary step to prevent gradient accumulation.\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Pass each token in the input sequence through the encoder sequentially.\n",
        "for token in input_tensor:\n",
        "    # Process each token, updating the hidden state at each step.\n",
        "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
        "\n",
        "# Step 2: Decoder Forward Pass and Loss Calculation\n",
        "# Initialize the first input to the decoder with a \"start\" token.\n",
        "decoder_input = torch.tensor([1])\n",
        "\n",
        "# Set the decoder's initial hidden state to the encoder's final hidden state.\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "# Initialize loss to 0. This variable accumulates the loss across all tokens in the target sequence.\n",
        "loss = 0\n",
        "\n",
        "# Teacher Forcing: using the actual target token as input to the decoder during training\n",
        "for di in range(len(target_tensor)):\n",
        "    # Pass the current decoder input and hidden state through the decoder.\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "    # Calculate the loss between the decoder's output and the actual target token.\n",
        "    # `unsqueeze(0)` adds a dimension to match expected shape for CrossEntropyLoss.\n",
        "    loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "\n",
        "    # Set the next decoder input to the actual target token (teacher forcing),\n",
        "    # which helps the model learn by providing correct context during training.\n",
        "    decoder_input = target_tensor[di]\n",
        "\n",
        "# Step 3: Backpropagation and Optimization\n",
        "# Perform backpropagation to compute gradients of the loss with respect to model parameters.\n",
        "loss.backward()\n",
        "\n",
        "# Update model parameters using the gradients and the optimizer.\n",
        "optimizer.step()\n",
        "\n",
        "# Print the average loss for this epoch, calculated by dividing total loss by the sequence length.\n",
        "print(\"Training Loss for this epoch:\", loss.item() / len(target_tensor))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EW9K79ZJY_J",
        "outputId": "d99d04a6-a9a2-4d40-c558-4af298022238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss for this epoch: 2.2404701709747314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Teacher Forcing**:\n",
        "   - **Purpose**: Improve learning by providing the actual target word as input to the decoder during training instead of its own predicted word.\n",
        "   - Teacher forcing is applied randomly (usually a specified percentage of the time) to allow the model to learn from both the correct word and its own predictions.\n",
        "   - **Code Adjustment**:\n"
      ],
      "metadata": {
        "id": "AKhc7JUrzaH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set the teacher forcing ratio, which controls the probability of using the actual target\n",
        "# as the next input during training. Here, 0.5 means there's a 50% chance to apply teacher forcing.\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# Randomly decide whether to use teacher forcing based on the teacher_forcing_ratio.\n",
        "# random.random() generates a float between 0 and 1.\n",
        "if random.random() < teacher_forcing_ratio:\n",
        "    # If the condition is true (50% chance), use the actual target token as the next input.\n",
        "    # `target_tensor[di]` is the correct token at the current step.\n",
        "    decoder_input = target_tensor[di]\n",
        "else:\n",
        "    # Otherwise, use the model's predicted token from the previous step as the next input.\n",
        "    # `decoder_output.argmax(dim=1)` gets the predicted token with the highest score.\n",
        "    decoder_input = decoder_output.argmax(dim=1)\n"
      ],
      "metadata": {
        "id": "C4IxW1gsJfGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.3 Iterating and Monitoring Performance\n",
        "   - **Epochs**: Repeat the training loop for multiple epochs (full data passes) to improve the model.\n",
        "   - **Metrics**: Track loss over time to monitor model performance.\n",
        "\n",
        "   - **Example Loop**:\n"
      ],
      "metadata": {
        "id": "PEzuOFzIzz1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the number of epochs\n",
        "n_epochs = 10\n",
        "\n",
        "# Loop over the specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    # Loop through each input-target pair (assuming `dataloader` provides batches of input and target sequences)\n",
        "    for input_tensor, target_tensor in dataloader:\n",
        "        batch_size = input_tensor.size(1)  # Get the batch size from the input tensor\n",
        "\n",
        "        # Initialize the encoder's hidden state to start the encoding process, matching the batch size.\n",
        "        # Instead of using encoder.init_hidden(), directly create a zero tensor with the required shape.\n",
        "        encoder_hidden = torch.zeros(1, batch_size, encoder.hidden_size)\n",
        "\n",
        "        # Step 1: Encoder Forward Pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass each token in the input sequence through the encoder sequentially.\n",
        "        for i in range(input_tensor.size(0)):\n",
        "            # Extract the token (or batch of tokens) and pass it through the embedding layer\n",
        "            embedded = encoder.embedding(input_tensor[i])  # Shape: (batch_size, embedding_dim)\n",
        "            # Pass the embedded token through the GRU layer\n",
        "            encoder_output, encoder_hidden = encoder.gru(embedded.unsqueeze(0), encoder_hidden)\n",
        "\n",
        "        # Step 2: Decoder Forward Pass and Loss Calculation\n",
        "\n",
        "        # Decoder Forward Pass and Loss Calculation\n",
        "        # Initialize the first input to the decoder with a \"start\" token for each element in the batch.\n",
        "        decoder_input = torch.tensor([1] * batch_size).view(1, batch_size)  # Start token for each batch element\n",
        "        decoder_hidden = encoder_hidden  # Set the decoder's initial hidden state\n",
        "\n",
        "        # Initialize loss to 0 for this batch\n",
        "        loss = 0\n",
        "\n",
        "        # Teacher Forcing: using the actual target token as input to the decoder during training\n",
        "        for di in range(target_tensor.size(0)):\n",
        "            # Embed the decoder input\n",
        "            embedded = decoder.embedding(decoder_input)  # Shape: (1, batch_size, embedding_dim)\n",
        "\n",
        "            # Pass the embedded input through the GRU\n",
        "            decoder_output, decoder_hidden = decoder.gru(embedded, decoder_hidden)\n",
        "\n",
        "            # Calculate the loss between the decoder's output and the actual target token\n",
        "            loss += criterion(decoder_output.squeeze(0), target_tensor[di])\n",
        "\n",
        "            # Set the next decoder input to the actual target token (teacher forcing)\n",
        "            decoder_input = target_tensor[di].unsqueeze(0)\n",
        "\n",
        "        # Step 3: Backpropagation and Optimization\n",
        "        loss.backward()  # Perform backpropagation to compute gradients\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        # Accumulate the loss for this batch to get the total loss for the epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate and print the average loss for this epoch\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {average_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLIGaIQ1Jj-V",
        "outputId": "55f6a132-5a09-43f6-f817-981815212113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.919054925441742\n",
            "Epoch 2/10, Loss: 3.4380860924720764\n",
            "Epoch 3/10, Loss: 2.956254780292511\n",
            "Epoch 4/10, Loss: 3.1284552216529846\n",
            "Epoch 5/10, Loss: 2.7401310205459595\n",
            "Epoch 6/10, Loss: 2.711756110191345\n",
            "Epoch 7/10, Loss: 2.8363680243492126\n",
            "Epoch 8/10, Loss: 2.781024932861328\n",
            "Epoch 9/10, Loss: 2.623547911643982\n",
            "Epoch 10/10, Loss: 2.6619701981544495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "\n",
        "\n",
        "| **Model**      | **Layer**                  | **Input Shape**            | **Output Shape**           | **Description**                                                                                   |\n",
        "|----------------|----------------------------|----------------------------|----------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Encoder**    | **Input Token ID**         | `(1,)`                     | `(1, 1, 20)`               | Single token ID is embedded to a dense vector of size 20                                          |\n",
        "|                | Embedding                  | `(1,)`                     | `(1, 1, 20)`               | Converts token ID to a dense vector of size `hidden_size`                                         |\n",
        "|                | GRU                        | `(1, 1, 20)`, `(1, 1, 20)` | `(1, 1, 20)`, `(1, 1, 20)` | Processes embedding with the hidden state; outputs updated hidden state                          |\n",
        "|                | Hidden State Initialization | `(1, 1, hidden_size)`      | `(1, 1, 20)`               | Initializes a zeroed hidden state with shape `(num_layers, batch_size, hidden_size)`              |\n",
        "| **Encoder Output** |                         |                            |                            |                                                                                                   |\n",
        "|                | Output (`encoder_output`)   |                            | `(1, 1, 20)`               | Final output of the GRU for the input sequence                                                    |\n",
        "|                | Hidden State (`encoder_hidden`) |                       | `(1, 1, 20)`               | Hidden state at the end of the input sequence                                                     |\n",
        "| **Decoder**    | **Input Token ID**         | `(1,)`                     | `(1, 1, 20)`               | Single token ID (start token) is embedded to a dense vector of size 20                            |\n",
        "|                | Embedding                  | `(1,)`                     | `(1, 1, 20)`               | Converts token ID to a dense vector of size `hidden_size`                                         |\n",
        "|                | ReLU                       | `(1, 1, 20)`               | `(1, 1, 20)`               | Applies ReLU activation, adding non-linearity; shape remains unchanged                           |\n",
        "|                | GRU                        | `(1, 1, 20)`, `(1, 1, 20)` | `(1, 1, 20)`, `(1, 1, 20)` | Processes the embedding and hidden state; updates hidden state for the next time step            |\n",
        "|                | Linear (`out`)             | `(1, 1, 20)`               | `(1, 10)`                 | Maps the GRU output to the vocabulary size (10), producing logits for each possible output token |\n",
        "|                | LogSoftmax                 | `(1, 10)`                  | `(1, 10)`                  | Converts logits to log-probabilities over the vocabulary                                          |\n",
        "| **Decoder Output** |                        |                            |                            |                                                                                                   |\n",
        "|                | Output (`decoder_output`)   |                            | `(1, 10)`                  | Final output of the decoder: log-probabilities for each token in the output vocabulary            |\n",
        "|                | Hidden State (`decoder_hidden`) |                        | `(1, 1, 20)`               | Hidden state at the end of the output sequence                                                    |\n",
        "\n",
        "##### Summary of the Flow\n",
        "1. **Encoder**:\n",
        "   - Input token ID → Embedding (→ ReLU activation in decoder) → GRU (processes embedding and hidden state) → Final hidden state.\n",
        "\n",
        "2. **Decoder**:\n",
        "   - Input token ID (start token) → Embedding → ReLU activation → GRU (processes embedding and hidden state) → Linear → LogSoftmax"
      ],
      "metadata": {
        "id": "ouhh04lTZWku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5: Evaluating and Testing the Model\n"
      ],
      "metadata": {
        "id": "kAJG7Xbez0AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 5.1 Objective\n",
        "   - Assess the performance of the trained Seq2Seq model by evaluating its output against unseen test data.\n",
        "   - Calculate metrics to gauge translation quality and identify areas for improvement.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Kn7iPve8z0Fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 5.2 Steps in Evaluation and Testing\n"
      ],
      "metadata": {
        "id": "QIp2auLcz0JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Testing with Sample Data**:\n",
        "   - **Purpose**: Evaluate model translation on sample sentences that the model has not seen during training.\n",
        "   - We provide an input sentence to the model’s encoder and generate the translated sentence through the decoder.\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "rtKD8I9Qz0Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to translate an input sentence using the encoder and decoder models.\n",
        "def translate_sentence(input_sentence, encoder, decoder, max_length=10):\n",
        "    # Initialize the encoder's hidden state.\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    # Step 1: Encode the input sentence\n",
        "    # Pass each token in the input sentence through the encoder, updating the hidden state.\n",
        "    for token in input_sentence:\n",
        "        encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
        "\n",
        "    # Step 2: Initialize the decoder with the final encoder hidden state\n",
        "    # Set the initial decoder input to a \"start\" token to begin decoding.\n",
        "    decoder_input = torch.tensor([1])  # Assuming 1 is the 'start' token\n",
        "    decoder_hidden = encoder_hidden  # Initialize the decoder's hidden state with the encoder's final hidden state.\n",
        "\n",
        "    # Initialize an empty list to store the tokens of the translated sentence.\n",
        "    translated_sentence = []\n",
        "\n",
        "    # Step 3: Decode to generate the translated sentence\n",
        "    # Loop until `max_length` is reached or an \"end\" token is produced.\n",
        "    for _ in range(max_length):\n",
        "        # Pass the decoder input and hidden state through the decoder.\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "        # Get the predicted token by finding the index of the maximum log-probability in `decoder_output`.\n",
        "        predicted_token = decoder_output.argmax(dim=1).item()\n",
        "\n",
        "        # Append the predicted token to the translated sentence list.\n",
        "        translated_sentence.append(predicted_token)\n",
        "\n",
        "        # Stop decoding if the \"end\" token is generated (assuming `2` is the end token).\n",
        "        if predicted_token == 2:\n",
        "            break\n",
        "\n",
        "        # Set the decoder input for the next iteration to the predicted token.\n",
        "        decoder_input = torch.tensor([predicted_token])\n",
        "\n",
        "    # Return the full translated sentence, a list of token IDs.\n",
        "    return translated_sentence\n",
        "\n",
        "# Example usage of the translate_sentence function\n",
        "input_sentence = torch.tensor([3, 4, 5, 6])  # Example tokenized input sentence\n",
        "translated = translate_sentence(input_sentence, encoder, decoder)\n",
        "print(\"Translated Sentence:\", translated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9Yr5QhaJqIO",
        "outputId": "0025f1d4-8266-4b69-d058-7646a586e911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Sentence: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Metric Calculation**:\n",
        "   - **Purpose**: Quantitatively measure translation quality.\n",
        "   - Common metrics include:\n",
        "      - **BLEU Score**: Compares predicted translation against the reference translation to calculate similarity.\n",
        "      - **Accuracy**: Measures the percentage of correctly translated words (mainly for simpler models and datasets).\n",
        "   - **BLEU Score Calculation**:\n"
      ],
      "metadata": {
        "id": "p4zZ6javz0Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Define the reference sentence (target sentence that the model should ideally generate).\n",
        "# This reference is wrapped in an additional list because the `sentence_bleu` function expects\n",
        "# multiple reference sentences (even if there is only one).\n",
        "reference = [[5, 6, 7, 8]]  # Example reference sentence, where each number is a token ID.\n",
        "\n",
        "# Define the candidate sentence (output from the model).\n",
        "# This is the sentence generated by the model to be evaluated against the reference.\n",
        "candidate = [5, 6, 7, 2]  # Example model output, ending with token ID `2`, assumed as an end token.\n",
        "\n",
        "# Calculate the BLEU score between the reference and candidate sentences.\n",
        "# BLEU score is calculated based on n-gram overlap between the reference and candidate.\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "\n",
        "# Print the resulting BLEU score.\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQUeyVIvJw7D",
        "outputId": "34238c97-00a7-4062-8ce1-5fadb1d52531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 8.636168555094496e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Error Analysis**:\n",
        "   - **Purpose**: Identify and analyze translation errors to refine the model.\n",
        "   - Examine samples where the model’s output differs significantly from the target, focusing on:\n",
        "      - **Common Mistranslations**: Repeated patterns of incorrect translations.\n",
        "      - **Long Sentence Handling**: Seq2Seq models can struggle with long sequences, leading to degraded accuracy.\n"
      ],
      "metadata": {
        "id": "EIyi74rVz0QO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **Adjusting Model Based on Evaluation**:\n",
        "   - After reviewing errors and metric results, consider adjustments:\n",
        "      - **Hyperparameter Tuning**: Adjust learning rate, hidden layer size, or number of epochs.\n",
        "      - **Teacher Forcing Ratio**: Modify teacher forcing ratio during training to encourage better independent predictions.\n",
        "      - **Model Architecture**: Introduce attention layers or upgrade to a Transformer model if necessary.\n"
      ],
      "metadata": {
        "id": "BRgfR0Oiz0T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 6: Deploying and Using the Translation Model\n"
      ],
      "metadata": {
        "id": "C0PpgAAlz0Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 6.1 Objective\n",
        "   - Implement the trained Seq2Seq model for real-world translation tasks.\n",
        "   - Set up interactive functions for translating user-provided sentences and explore basic deployment options.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KYd-4AJmz0Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 6.2 Steps for Deployment and Use\n"
      ],
      "metadata": {
        "id": "Q1CU6VKEz0bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Interactive Translation Function**:\n",
        "   - **Purpose**: Create a function that accepts input text from a user, processes it, and returns the translated output.\n",
        "   - This function can serve as the primary interface for the model, allowing users to input text and receive translations directly.\n",
        "   - **Code Demonstration**:\n"
      ],
      "metadata": {
        "id": "LV5dOo9xz0eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_translate(input_sentence, encoder, decoder, input_lang_vocab, output_lang_vocab, max_length=10):\n",
        "    # Define a placeholder ID for unknown words (UNK). This handles words in the input sentence that are not found in the input vocabulary.\n",
        "    UNK_TOKEN_ID = input_lang_vocab.get(\"<UNK>\", 0)\n",
        "\n",
        "    # Convert the input sentence into a tensor of token IDs by looking up each word in the input vocabulary.\n",
        "    # If a word is not found, it defaults to UNK_TOKEN_ID.\n",
        "    input_tensor = torch.tensor([\n",
        "        input_lang_vocab.get(word.lower(), UNK_TOKEN_ID)  # Convert words to lowercase to handle case insensitivity.\n",
        "        for word in input_sentence.split(' ')            # Split sentence into individual words.\n",
        "    ])\n",
        "\n",
        "    # Initialize a list to store the translated token IDs that will be predicted by the decoder.\n",
        "    translated_tokens = []\n",
        "\n",
        "    # Initialize the encoder's hidden state.\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    # Loop through each token in the input tensor and pass it through the encoder.\n",
        "    for token in input_tensor:\n",
        "        encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
        "        # `encoder_output` is usually a representation of the current token,\n",
        "        # and `encoder_hidden` is the updated hidden state passed to the next iteration.\n",
        "\n",
        "    # Set up the decoder with an initial input (start token) and the hidden state from the encoder.\n",
        "    decoder_input = torch.tensor([1])  # Assuming `1` is the ID for the 'start' token in the output vocabulary.\n",
        "    decoder_hidden = encoder_hidden     # Transfer the encoder's final hidden state to the decoder.\n",
        "\n",
        "    # Decode the output step-by-step up to `max_length`, printing intermediate outputs.\n",
        "    for _ in range(max_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        # Obtain the token ID with the highest probability (argmax) from the decoder output.\n",
        "        predicted_token = decoder_output.argmax(dim=1).item()\n",
        "\n",
        "        # Print intermediate translation steps for debugging purposes.\n",
        "        print(f\"Predicted token ID: {predicted_token}, word: {output_lang_vocab.get(predicted_token, '<UNK>')}\")\n",
        "\n",
        "        # Append the predicted token to the list of translated tokens.\n",
        "        translated_tokens.append(predicted_token)\n",
        "\n",
        "        # Check if the predicted token is the end token; if so, break out of the loop.\n",
        "        if predicted_token == 2:  # Assuming `2` is the ID for the 'end' token in the output vocabulary.\n",
        "            break\n",
        "\n",
        "        # Update the decoder input to be the predicted token for the next iteration.\n",
        "        decoder_input = torch.tensor([predicted_token])\n",
        "\n",
        "    # Convert the list of token IDs in `translated_tokens` back to words using the output vocabulary.\n",
        "    # Only include tokens that have valid word mappings in `output_lang_vocab`.\n",
        "    translated_sentence = ' '.join([\n",
        "        output_lang_vocab[token] for token in translated_tokens if token in output_lang_vocab\n",
        "    ])\n",
        "\n",
        "    # Return the final translated sentence.\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "Bfbq4zTCJ5g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang_vocab = {\n",
        "    \"I\": 1, \"love\": 2, \"learning\": 3, \"languages\": 4, \"<UNK>\": 0, \"<START>\": 1, \"<END>\": 2\n",
        "}\n",
        "\n",
        "output_lang_vocab = {\n",
        "    1: \"yo\", 2: \"<END>\", 3: \"amo\", 4: \"aprender\", 5: \"idiomas\", 0: \"<UNK>\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "bfglWZ6DKnDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"I Love Learning\"\n",
        "print(\"Translation:\", interactive_translate(input_sentence, encoder, decoder, input_lang_vocab, output_lang_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EkteRBeKpeZ",
        "outputId": "4518c471-200d-44ae-e8ef-e9dd12b24771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Translation: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"I love coding\"\n",
        "print(\"Translation:\", interactive_translate(input_sentence, encoder, decoder, input_lang_vocab, output_lang_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpXikQ2GKz8f",
        "outputId": "11b9c1fa-4c4b-4a2e-ed53-f746d4e8b726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Translation: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"learning\"\n",
        "print(\"Translation:\", interactive_translate(input_sentence, encoder, decoder, input_lang_vocab, output_lang_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1__HVplK27Z",
        "outputId": "44d994a2-70e1-41ea-d498-d83f6035c105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Predicted token ID: 6, word: <UNK>\n",
            "Translation: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Preparing for Web or Mobile Deployment**:\n",
        "   - **Frameworks for Deployment**:\n",
        "      - **Flask/Django (Python)**: Suitable for creating web APIs that host the model.\n",
        "      - **TensorFlow Lite / PyTorch Mobile**: For mobile deployment.\n",
        "   - **Deployment Steps**:\n",
        "      - Save the trained model weights to load them when needed for translation.\n",
        "      - Set up a simple web API to handle incoming translation requests and return outputs.\n",
        "   - **Code Example (Saving Model)**:\n",
        "      ```python\n",
        "      # Save encoder and decoder model weights\n",
        "      torch.save(encoder.state_dict(), 'encoder_weights.pth')\n",
        "      torch.save(decoder.state_dict(), 'decoder_weights.pth')\n",
        "\n",
        "      # Load model weights when deploying\n",
        "      encoder.load_state_dict(torch.load('encoder_weights.pth'))\n",
        "      decoder.load_state_dict(torch.load('decoder_weights.pth'))\n",
        "      ```\n"
      ],
      "metadata": {
        "id": "5Vzu9KUsz0go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Setting up a Basic Web API (Using Flask)**:\n",
        "   - **Purpose**: Allow users to input sentences via a web interface and receive translations from the model.\n",
        "   - **Code Demonstration**:\n",
        "\n",
        "```\n",
        "from flask import Flask, request, jsonify  # Import Flask and necessary functions for handling requests and responses.\n",
        "\n",
        "# Initialize the Flask application\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load pre-trained encoder and decoder model weights.\n",
        "# These weights are loaded from saved `.pth` files into the encoder and decoder models.\n",
        "# This assumes `encoder` and `decoder` are already defined and initialized.\n",
        "encoder.load_state_dict(torch.load('encoder_weights.pth'))  # Load the encoder's weights from file.\n",
        "decoder.load_state_dict(torch.load('decoder_weights.pth'))  # Load the decoder's weights from file.\n",
        "\n",
        "# Define a route for translation. This route listens for POST requests on the `/translate` endpoint.\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate():\n",
        "    # Parse the JSON data from the incoming request.\n",
        "    # `data` is expected to be a JSON object containing a \"sentence\" key with the text to translate.\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Extract the input sentence from the parsed JSON data.\n",
        "    # `input_sentence` is the sentence provided by the user that needs to be translated.\n",
        "    input_sentence = data['sentence']\n",
        "\n",
        "    # Perform the translation using the `interactive_translate` function.\n",
        "    # This function takes the input sentence, encoder, decoder, input and output vocabularies.\n",
        "    # The result, `translation`, is the translated sentence generated by the model.\n",
        "    translation = interactive_translate(input_sentence, encoder, decoder, input_lang_vocab, output_lang_vocab)\n",
        "\n",
        "    # Return the translation as a JSON response.\n",
        "    # `jsonify` converts the Python dictionary to JSON format for the response.\n",
        "    return jsonify({'translation': translation})\n",
        "\n",
        "# Run the Flask app when the script is executed directly (i.e., not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Start the Flask app in debug mode, which provides detailed error logs and auto-reloads the server on code changes.\n",
        "    app.run(debug=True)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7EKpk3Liz0jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. **User Interface for Translation (Optional)**:\n",
        "   - **Purpose**: Create a front-end UI to interact with the translation model.\n",
        "   - Options include:\n",
        "      - **HTML/JavaScript** interface: Simple, text-based input and output fields for translation.\n",
        "      - **Web Frameworks** like React/Vue.js: For more interactive and styled interfaces.\n"
      ],
      "metadata": {
        "id": "XlDPwPgWz0mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wQZTB_FXz07h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Heagp_UwzaKC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}