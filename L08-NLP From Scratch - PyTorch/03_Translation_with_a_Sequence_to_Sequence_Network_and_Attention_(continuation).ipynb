{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UlOJPHUHGdK"
      },
      "source": [
        "based on the official documentation from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urZhHdZY3gWe"
      },
      "source": [
        "Database:\n",
        "\n",
        "Available links:\n",
        "\n",
        "- https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages\n",
        "- https://tatoeba.org/eng/downloads\n",
        "- https://www.manythings.org/anki/\n",
        "\n",
        "This demonstration uses a simple database of The English to French pairs  which is given in `data/eng-fra.txt`.\n",
        "Link to download: https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-GY1V1XsXCx"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrQYz4fisXCy"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "from io import open\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTuc_GxwsXC1"
      },
      "source": [
        "# Step 1: Data Preprocessing and Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVjTr0EgsXC2"
      },
      "outputs": [],
      "source": [
        "# Step 1: Data Preprocessing and Vocabulary\n",
        "\n",
        "# Special tokens to represent the start and end of sentences\n",
        "SOS_token = 0  # Start Of Sentence token\n",
        "EOS_token = 1  # End Of Sentence token\n",
        "\n",
        "# Class for handling language data, vocabulary creation, and word indexing\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        \"\"\"\n",
        "        Initialize the Lang class with attributes to hold word mappings and word counts.\n",
        "\n",
        "        Parameters:\n",
        "        name (str): The name of the language (e.g., 'English', 'French')\n",
        "        \"\"\"\n",
        "        self.name = name  # Language name\n",
        "        self.word2index = {}  # Dictionary mapping words to indices\n",
        "        self.word2count = {}  # Dictionary counting occurrences of each word\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # Mapping indices to words, starting with SOS and EOS\n",
        "        self.n_words = 2  # Vocabulary size, starting with SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        \"\"\"\n",
        "        Add each word in a sentence to the vocabulary.\n",
        "\n",
        "        Parameters:\n",
        "        sentence (str): The sentence to be added to the vocabulary\n",
        "        \"\"\"\n",
        "        for word in sentence.split(' '):  # Split sentence into words\n",
        "            self.add_word(word)  # Add each word to vocabulary\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"\n",
        "        Add a word to the vocabulary if it's not already present.\n",
        "        If it is, increase its count.\n",
        "\n",
        "        Parameters:\n",
        "        word (str): The word to add to the vocabulary\n",
        "        \"\"\"\n",
        "        if word not in self.word2index:\n",
        "            # If the word is new, add it to the dictionaries\n",
        "            self.word2index[word] = self.n_words  # Map word to index\n",
        "            self.word2count[word] = 1  # Initialize word count to 1\n",
        "            self.index2word[self.n_words] = word  # Map index to word\n",
        "            self.n_words += 1  # Increment the vocabulary size\n",
        "        else:\n",
        "            # If the word already exists, increment its count\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Function to convert a Unicode string to plain ASCII for compatibility\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Convert a Unicode string to ASCII by removing accents.\n",
        "\n",
        "    Parameters:\n",
        "    s (str): The Unicode string to convert\n",
        "\n",
        "    Returns:\n",
        "    str: The ASCII version of the string\n",
        "    \"\"\"\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'  # Exclude non-spacing marks (accents)\n",
        "    )\n",
        "\n",
        "# Function to normalize and clean a string\n",
        "def normalize_string(s):\n",
        "    \"\"\"\n",
        "    Convert a string to lowercase, trim whitespace, convert to ASCII, and remove non-letter characters.\n",
        "\n",
        "    Parameters:\n",
        "    s (str): The string to normalize\n",
        "\n",
        "    Returns:\n",
        "    str: The normalized string\n",
        "    \"\"\"\n",
        "    s = unicode_to_ascii(s.lower().strip())  # Convert to lowercase ASCII and strip whitespace\n",
        "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)  # Replace any non-letter characters with a space\n",
        "    return s\n",
        "\n",
        "# Function to read language data, split lines into pairs, and normalize text\n",
        "def read_langs(lang1, lang2, reverse=False):\n",
        "    \"\"\"\n",
        "    Read text data for two languages, normalize it, and optionally reverse the language pair order.\n",
        "\n",
        "    Parameters:\n",
        "    lang1 (str): The first language (source language)\n",
        "    lang2 (str): The second language (target language)\n",
        "    reverse (bool): Whether to reverse the input-output language pair\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing input_lang, output_lang, and pairs (sentence pairs)\n",
        "    \"\"\"\n",
        "    # Load data from a text file and split it into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split each line into language pairs and normalize the sentences\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "\n",
        "    sample_size = int(1 * len(pairs))\n",
        "    pairs = random.sample(pairs, sample_size)\n",
        "\n",
        "    # If reverse is True, swap the language pairs\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]  # Reverse each pair\n",
        "        input_lang = Lang(lang2)  # Set the output language as the input language\n",
        "        output_lang = Lang(lang1)  # Set the input language as the output language\n",
        "    else:\n",
        "        input_lang = Lang(lang1)  # Set input language normally\n",
        "        output_lang = Lang(lang2)  # Set output language normally\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))  # Print the number of sentence pairs\n",
        "\n",
        "    return input_lang, output_lang, pairs  # Return languages and the list of pairs\n",
        "\n",
        "# Function to prepare data by reading and creating vocabulary for each language\n",
        "def prepare_data(lang1, lang2, reverse=False):\n",
        "    \"\"\"\n",
        "    Prepare language data by reading and building vocabulary for each language.\n",
        "\n",
        "    Parameters:\n",
        "    lang1 (str): The first language (source language)\n",
        "    lang2 (str): The second language (target language)\n",
        "    reverse (bool): Whether to reverse the input-output language pair\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing input_lang, output_lang, and pairs (sentence pairs)\n",
        "    \"\"\"\n",
        "    # Read language data and obtain the language pair vocabularies\n",
        "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
        "\n",
        "    # Add each word in each sentence of each pair to the respective vocabulary\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])  # Add words in the input language sentence\n",
        "        output_lang.add_sentence(pair[1])  # Add words in the output language sentence\n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "\n",
        "    return input_lang, output_lang, pairs  # Return languages and the list of pairs\n",
        "\n",
        "# The following utility functions assist in string normalization\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Convert a Unicode string to ASCII by removing accents.\n",
        "\n",
        "    Parameters:\n",
        "    s (str): The Unicode string to convert\n",
        "\n",
        "    Returns:\n",
        "    str: The ASCII version of the string\n",
        "    \"\"\"\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'  # Exclude non-spacing marks (accents)\n",
        "    )\n",
        "\n",
        "def normalize_string(s):\n",
        "    \"\"\"\n",
        "    Convert a string to lowercase, trim whitespace, convert to ASCII, and remove non-letter characters.\n",
        "\n",
        "    Parameters:\n",
        "    s (str): The string to normalize\n",
        "\n",
        "    Returns:\n",
        "    str: The normalized string\n",
        "    \"\"\"\n",
        "    s = unicode_to_ascii(s.lower().strip())  # Convert to lowercase ASCII and strip whitespace\n",
        "\n",
        "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)  # Replace any non-letter characters with a space\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtZCoajsXC5"
      },
      "source": [
        "# Step 2: Encoder, Decoder, and Attention Mechanisms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae4wL7J6sXC6"
      },
      "outputs": [],
      "source": [
        "# EncoderRNN class definition\n",
        "# This class defines the encoder part of a sequence-to-sequence (Seq2Seq) model, which reads input sentences and encodes them into a hidden representation.\n",
        "class EncoderRNN(nn.Module):\n",
        "    # Initialize the encoder with input size and hidden layer size\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Save the hidden size (the number of features in the hidden state)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer to convert input indices into dense vectors of size `hidden_size`\n",
        "        # input_size represents the size of the vocabulary for the input language\n",
        "        # hidden_size represents the dimension of the embedded vectors\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # GRU layer for processing sequences\n",
        "        # It takes in `hidden_size` as both input and output dimensions, making it suitable for encoding the embedded input sequence.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    # Forward function defines the computation for a single time step in the encoder\n",
        "    # `input` is a tensor containing word indices, and `hidden` is the previous hidden state\n",
        "    def forward(self, input, hidden):\n",
        "        # Convert the input word index into a dense vector representation using the embedding layer\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Reshape to [1, 1, hidden_size] for compatibility with GRU\n",
        "\n",
        "        # Pass the embedded input and the previous hidden state into the GRU\n",
        "        # `output` is the output of the GRU at the current time step\n",
        "        # `hidden` is the updated hidden state that will be passed to the next time step\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # Return the output (not used further in the encoder) and the updated hidden state\n",
        "        return output, hidden\n",
        "\n",
        "    # Initializes the hidden state to zero at the beginning of each sentence\n",
        "    def init_hidden(self):\n",
        "        # Create a tensor of zeros with shape [1, 1, hidden_size]\n",
        "        # `device` ensures the tensor is on the correct hardware (e.g., GPU if available)\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2id7sahsXC8"
      },
      "outputs": [],
      "source": [
        "# BahdanauAttention class definition\n",
        "# This class implements the attention mechanism as described by Bahdanau et al., where the model learns to attend to specific parts of the encoder outputs.\n",
        "class BahdanauAttention(nn.Module):\n",
        "    # Initialize the attention mechanism with hidden size\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "\n",
        "        # Linear layer to compute attention scores\n",
        "        # It takes as input the concatenated hidden state and encoder output (each of size `hidden_size`),\n",
        "        # hence `hidden_size * 2` as input dimension, and outputs a vector of size `hidden_size`.\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # Learnable attention vector used to calculate the alignment score\n",
        "        # It is initialized randomly and has the same size as the hidden dimension\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    # Forward function calculates attention weights based on the current hidden state and all encoder outputs\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Initialize a tensor to store the attention energies for each encoder output\n",
        "        # The size is set to the length of encoder outputs\n",
        "        attn_energies = torch.zeros(encoder_outputs.size(0), device=device)\n",
        "\n",
        "        # Calculate energy score for each encoder output\n",
        "        # Loop through each encoder output (time step) and compute its alignment score with the current hidden state\n",
        "        for i in range(encoder_outputs.size(0)):\n",
        "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
        "\n",
        "        # Apply softmax to normalize the energies into probabilities\n",
        "        # Reshape to [1, 1, encoder_outputs.size(0)] to match expected dimensions in subsequent layers\n",
        "        return torch.softmax(attn_energies, dim=0).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Score function calculates an alignment score between the current hidden state and a given encoder output\n",
        "    def score(self, hidden, encoder_output):\n",
        "        # Ensure hidden and encoder_output are properly shaped\n",
        "        # Remove the extra dimension from hidden for concatenation\n",
        "        hidden = hidden.squeeze(0)  # Shape: [1, hidden_size]\n",
        "\n",
        "        # Add an extra dimension to encoder_output for concatenation\n",
        "        encoder_output = encoder_output.unsqueeze(0)  # Shape: [1, hidden_size]\n",
        "\n",
        "        # Concatenate hidden state and encoder output to form a [1, hidden_size * 2] vector\n",
        "        concat = torch.cat((hidden, encoder_output), dim=1)  # Shape: [1, hidden_size * 2]\n",
        "\n",
        "        # Pass the concatenated vector through a linear layer to compute an intermediate energy vector\n",
        "        # This linear transformation is where the model learns the alignment score between hidden and encoder output\n",
        "        energy = self.attn(concat)  # Shape: [1, hidden_size]\n",
        "\n",
        "        # Compute the final energy score by taking the dot product of the attention vector `self.v` with the energy vector\n",
        "        # `self.v` serves as a weight vector to transform the intermediate energy into a scalar score\n",
        "        energy = torch.dot(self.v, energy.squeeze(0))  # Scalar value\n",
        "\n",
        "        return energy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEJPU-IMsXC-"
      },
      "outputs": [],
      "source": [
        "# Attention-based Decoder (AttnDecoderRNN) class\n",
        "# This class represents a decoder with attention mechanism that helps the decoder focus on specific parts of the input sequence.\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    # Initialization function for defining layers and parameters\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Setting the hidden size, output size, dropout probability, and maximum length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Embedding layer: maps each word in the target language to a vector of dimension `hidden_size`\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "\n",
        "        # Attention mechanism: using BahdanauAttention to compute attention weights\n",
        "        self.attn = BahdanauAttention(hidden_size)\n",
        "\n",
        "        # GRU layer: receives both the embedded word vector and the attention-applied context vector as input\n",
        "        # The input dimension for GRU is `hidden_size * 2` because it combines the hidden state and context vector\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # Output layer: maps the GRU output and context vector to a vector of size `output_size`\n",
        "        # This will produce a probability distribution over the target vocabulary\n",
        "        self.out = nn.Linear(hidden_size * 2, self.output_size)\n",
        "\n",
        "    # Forward pass for the decoder\n",
        "    # Takes in the current word, hidden state from the previous step, and the encoder outputs\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # Embed the input word into a vector of `hidden_size` dimensions\n",
        "        # The shape becomes [1, 1, hidden_size] after reshaping\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "\n",
        "        # Compute attention weights using the current hidden state and encoder outputs\n",
        "        attn_weights = self.attn(hidden, encoder_outputs)\n",
        "\n",
        "        # Apply the attention weights to encoder outputs to obtain a weighted context vector\n",
        "        # Shape of `attn_applied` will be [1, 1, hidden_size] after batch matrix multiplication\n",
        "        attn_applied = torch.bmm(attn_weights, encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # Concatenate the embedded input and attention-applied context vector along the last dimension\n",
        "        # The shape becomes [1, hidden_size * 2] after concatenation\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "\n",
        "        # Pass the concatenated vector through the GRU layer\n",
        "        # The GRU takes input of shape [1, 1, hidden_size * 2] and outputs [1, 1, hidden_size]\n",
        "        output, hidden = self.gru(output.unsqueeze(0), hidden)\n",
        "\n",
        "        # Concatenate the GRU output and context vector again for final output layer\n",
        "        # Output shape is [1, output_size], representing the log probabilities of each word in the target vocabulary\n",
        "        output = torch.log_softmax(self.out(torch.cat((output[0], attn_applied[0]), 1)), dim=1)\n",
        "\n",
        "        # Return the output, new hidden state, and attention weights\n",
        "        return output, hidden, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii8roB9rsXC_"
      },
      "source": [
        "# Step 3: Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIAok8S_sXDA"
      },
      "outputs": [],
      "source": [
        "# Function to train the Encoder-Decoder model for a single sequence pair (input and target tensors)\n",
        "# It performs forward propagation through the encoder and decoder, computes the loss, and backpropagates the error.\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
        "\n",
        "    # Initialize the hidden state of the encoder\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    # Zero the gradients for both the encoder and decoder optimizers before the backward pass\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Get the length of the input and target tensors\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # Initialize a tensor to store all encoder outputs, adjusting the size based on input length\n",
        "    # This will store the hidden states from each time step of the encoder to be used for attention\n",
        "    encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    # Variable to accumulate the loss over the target sequence\n",
        "    loss = 0\n",
        "\n",
        "    # Encoder forward pass: process each word in the input sequence\n",
        "    for ei in range(input_length):\n",
        "        # Pass the input token and the previous hidden state to the encoder\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "        # Store the encoder's output (hidden state) for the current time step\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # Initialize the first input for the decoder as the Start of Sentence (SOS) token\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    # Set the initial hidden state of the decoder to the last hidden state of the encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Decoder forward pass: generate each word in the output sequence\n",
        "    for di in range(target_length):\n",
        "        # Pass the current input, hidden state, and encoder outputs to the decoder\n",
        "        # The decoder returns the predicted output, the new hidden state, and attention weights\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "        # Compute the loss between the decoder's output and the target word at this time step\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "        # Set the input for the next time step of the decoder as the current target word\n",
        "        # This is known as \"teacher forcing\" where the target word is fed back to the decoder\n",
        "        decoder_input = target_tensor[di]\n",
        "\n",
        "    # Perform backpropagation to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters of both the encoder and decoder using the gradients\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return the average loss per word in the target sequence\n",
        "    return loss.item() / target_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az8MwrD6sXDB"
      },
      "outputs": [],
      "source": [
        "# Helper function to convert seconds into minutes and seconds format\n",
        "def as_minutes(s):\n",
        "    # Compute the number of minutes by dividing seconds by 60\n",
        "    m = math.floor(s / 60)\n",
        "    # Subtract the computed minutes from the total seconds to get remaining seconds\n",
        "    s -= m * 60\n",
        "    # Return the formatted time as 'Xm Ys'\n",
        "    return f'{m}m {s}s'\n",
        "\n",
        "# Helper function to calculate elapsed time and remaining time for training\n",
        "# 'since' is the start time and 'percent' is the fraction of training completed\n",
        "def time_since(since, percent):\n",
        "    # Get the current time\n",
        "    now = time.time()\n",
        "    # Calculate elapsed time in seconds\n",
        "    s = now - since\n",
        "    # Estimate total time by dividing elapsed time by percent completed\n",
        "    es = s / percent\n",
        "    # Calculate remaining time by subtracting elapsed time from estimated total time\n",
        "    rs = es - s\n",
        "    # Return formatted elapsed and remaining time\n",
        "    return f'{as_minutes(s)} (- {as_minutes(rs)})'\n",
        "\n",
        "# Function to plot the loss over time during training\n",
        "# 'points' is a list of loss values recorded over training iterations\n",
        "def show_plot(points):\n",
        "    # Create a new figure\n",
        "    plt.figure()\n",
        "    # Create a subplot\n",
        "    fig, ax = plt.subplots()\n",
        "    # Set the y-axis to have ticks every 0.2 units\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    # Plot the loss points over training iterations\n",
        "    plt.plot(points)\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63iE-eJusXDC"
      },
      "source": [
        "# Step 4: Evaluate Model with Attention Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4EFMmRvsXDD"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the translation of a given sentence using the trained encoder and decoder\n",
        "# 'encoder' and 'decoder' are the trained models\n",
        "# 'sentence' is the input sentence in the source language\n",
        "# 'input_lang' and 'output_lang' are the vocabulary objects for the input and output languages\n",
        "# 'max_length' is the maximum allowed length for the output sentence\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=10):\n",
        "    \"\"\"\n",
        "    Evaluates the model's ability to translate a sentence from the input language to the output language.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():  # Disable gradient calculations for inference\n",
        "        input_tensor = tensor_from_sentence(input_lang, sentence)  # Convert input to tensor\n",
        "        input_length = input_tensor.size()[0]  # Determine the length of the input sentence\n",
        "        encoder_hidden = encoder.init_hidden()  # Initialize hidden state for the encoder\n",
        "\n",
        "        # Initialize encoder outputs with zeros and dimensions [max_length, encoder.hidden_size]\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Loop through the input tensor and run the encoder for each word\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            # Store each output from the encoder in encoder_outputs, but clip index to prevent out of bounds error\n",
        "            encoder_outputs[min(ei, max_length - 1)] = encoder_output[0, 0]\n",
        "\n",
        "        # Prepare the decoder input, initialized to the Start-Of-Sentence (SOS) token\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        # Set the initial hidden state of the decoder to the final hidden state of the encoder\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # List to store the decoded words (output translation)\n",
        "        decoded_words = []\n",
        "        # Tensor to store attention weights for each decoding step\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        # Run the decoder to generate the output sentence, word by word\n",
        "        for di in range(max_length):\n",
        "            # Pass the current decoder input and hidden state through the decoder\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            # Store the attention weights for the current step\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            # Find the word with the highest probability in the decoder output\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            # If the EOS (End-Of-Sentence) token is generated, stop decoding\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                # Append the decoded word to the output sentence\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            # Set the current output word as the next input to the decoder, detached from computation graph\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        # Return the decoded words (output sentence) and the attention weights up to the last decoding step\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virl6Sf8sXDE"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dGeZW3asXDE"
      },
      "outputs": [],
      "source": [
        "# Converts a sentence to a list of word indices based on the language vocabulary\n",
        "# 'lang' is the language object containing word mappings\n",
        "# 'sentence' is the input sentence to be converted\n",
        "def indexes_from_sentence(lang, sentence):\n",
        "    # Split sentence into words and convert each word to its index in the vocabulary\n",
        "    # Only include words that are in the vocabulary to avoid errors\n",
        "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index]\n",
        "\n",
        "# Converts a sentence into a tensor of word indices with an EOS token appended\n",
        "# 'lang' is the language object containing word mappings\n",
        "# 'sentence' is the input sentence to be converted\n",
        "def tensor_from_sentence(lang, sentence):\n",
        "    # Get the list of word indices for the sentence\n",
        "    indexes = indexes_from_sentence(lang, sentence)\n",
        "    # Append the EOS (End-Of-Sequence) token to signify the end of the sentence\n",
        "    indexes.append(EOS_token)\n",
        "    # Convert the list of indices to a tensor and reshape it to have each word index on a new line\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# Converts a pair of sentences (input sentence and target sentence) into tensors\n",
        "# 'pair' is a tuple or list with the input sentence and target sentence\n",
        "# 'input_lang' and 'output_lang' are the language objects for the input and output languages\n",
        "def tensors_from_pair(pair, input_lang, output_lang):\n",
        "    # Convert the input sentence to a tensor using the input language vocabulary\n",
        "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
        "    # Convert the target sentence to a tensor using the output language vocabulary\n",
        "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
        "    # Return both tensors as a tuple\n",
        "    return (input_tensor, target_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mv64F18sXDF"
      },
      "outputs": [],
      "source": [
        "def train_iters(encoder, decoder, n_iters, pairs, input_lang, output_lang, print_every=1000, learning_rate=0.01):\n",
        "    # Record the start time for tracking the time elapsed\n",
        "    start = time.time()\n",
        "\n",
        "    # List to store loss values for plotting\n",
        "    plot_losses = []\n",
        "\n",
        "    # Cumulative losses for printing and plotting, reset periodically\n",
        "    print_loss_total = 0  # Reset every `print_every`\n",
        "    plot_loss_total = 0  # Reset every `plot_every`\n",
        "\n",
        "    # Optimizers for the encoder and decoder using Stochastic Gradient Descent\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Pre-generate random training pairs from the dataset\n",
        "    # Each pair is converted to tensors (input_tensor, target_tensor)\n",
        "    training_pairs = [tensors_from_pair(random.choice(pairs), input_lang, output_lang) for _ in range(n_iters)]\n",
        "\n",
        "    # Loss function: Negative Log Likelihood Loss for classification\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    print(f\"Starting training for {n_iters} iterations with learning rate {learning_rate}...\")\n",
        "\n",
        "    # Training loop\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        # Select the training pair for this iteration\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        # Print the current training pair's input and target tensors for insight\n",
        "        if iter % (print_every * 10) == 1:\n",
        "            print(f\"\\nIteration {iter}/{n_iters}:\")\n",
        "            print(f\"Input tensor: {input_tensor}\")\n",
        "            print(f\"Target tensor: {target_tensor}\")\n",
        "\n",
        "        # Train on the selected pair and get the loss\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "        # Accumulate loss for averaging\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        # Print the average loss every `print_every` iterations\n",
        "        if iter % print_every == 0:\n",
        "            # Calculate average loss over the interval\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0  # Reset print loss total\n",
        "\n",
        "            # Print training progress with time elapsed and loss\n",
        "            print(f'{time_since(start, iter / n_iters)} | Iteration {iter} ({iter / n_iters * 100:.2f}%) | Average Loss: {print_loss_avg:.4f}')\n",
        "\n",
        "            # Append average loss to plot_losses for later plotting\n",
        "            plot_losses.append(plot_loss_total / print_every)\n",
        "            plot_loss_total = 0  # Reset plot loss total\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    # Plot the accumulated losses over time\n",
        "    show_plot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXh9iLiusXDG"
      },
      "outputs": [],
      "source": [
        "def evaluate_randomly(encoder, decoder, pairs, input_lang, output_lang, n=10):\n",
        "    \"\"\"\n",
        "    Evaluates the model by randomly selecting sentence pairs from the dataset, generating predictions,\n",
        "    and displaying the results.\n",
        "\n",
        "    Parameters:\n",
        "    - encoder: Trained encoder model for encoding input sentences.\n",
        "    - decoder: Trained decoder model for generating output sentences.\n",
        "    - pairs: List of sentence pairs (input sentence, target sentence) for evaluation.\n",
        "    - input_lang: Language object for the input language, used to handle vocabulary and tokenization.\n",
        "    - output_lang: Language object for the output language, used to decode predicted indices to words.\n",
        "    - n: Number of random evaluations to perform.\n",
        "\n",
        "    Output:\n",
        "    - Prints the input sentence, expected output sentence, and the model's predicted sentence for `n` randomly\n",
        "      selected pairs. Also includes the attention weights for each predicted word.\n",
        "    \"\"\"\n",
        "    for i in range(n):\n",
        "        # Select a random sentence pair from the dataset\n",
        "        pair = random.choice(pairs)\n",
        "\n",
        "        # Display the input sentence (source language)\n",
        "        print('>', pair[0])\n",
        "\n",
        "        # Display the target sentence (expected output in target language)\n",
        "        print('=', pair[1])\n",
        "\n",
        "        # Generate the predicted output and attention weights using the evaluate function\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "\n",
        "        # Join the list of output words to form the predicted sentence\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        # Display the model's predicted sentence\n",
        "        print('<', output_sentence)\n",
        "        print('')  # Print an empty line for separation between evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryawKxSXsXDG"
      },
      "outputs": [],
      "source": [
        "def show_attention(input_sentence, output_words, attentions):\n",
        "    \"\"\"\n",
        "    Visualizes the attention weights for each word in the output relative to each word in the input.\n",
        "\n",
        "    Parameters:\n",
        "    - input_sentence: The input sentence (string) from which the model generated the output.\n",
        "    - output_words: A list of words produced by the model as the output sentence.\n",
        "    - attentions: The attention weights (tensor) showing the focus on input words for each output word.\n",
        "    \"\"\"\n",
        "    # Create a new figure for the attention plot\n",
        "    fig = plt.figure()\n",
        "\n",
        "    # Add a subplot to the figure\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    # Display the attention matrix as a color-coded plot\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')  # Using 'bone' colormap for a grayscale effect\n",
        "    fig.colorbar(cax)  # Add a color bar to indicate attention intensity\n",
        "\n",
        "    # Set up axis labels for the input sentence and output words\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)  # Input words on x-axis\n",
        "    ax.set_yticklabels([''] + output_words)  # Output words on y-axis\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_and_show_attention(input_sentence, encoder, decoder, input_lang, output_lang):\n",
        "    \"\"\"\n",
        "    Evaluates a single input sentence, generates the output, and visualizes the attention weights.\n",
        "\n",
        "    Parameters:\n",
        "    - input_sentence: The input sentence to evaluate and visualize.\n",
        "    - encoder: The trained encoder model.\n",
        "    - decoder: The trained decoder model with attention.\n",
        "    - input_lang: The language object for the input language (used for tokenization).\n",
        "    - output_lang: The language object for the output language (used for decoding).\n",
        "    \"\"\"\n",
        "    # Generate the output words and attention weights from the model\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "\n",
        "    # Display the input sentence and the model's output sentence\n",
        "    print('Input:', input_sentence)\n",
        "    print('Output:', ' '.join(output_words))\n",
        "\n",
        "    # Show the attention visualization for this input-output pair\n",
        "    show_attention(input_sentence, output_words, attentions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XxI0lgTsXDH",
        "outputId": "e2919cde-b1e0-434e-e42f-8f24e9be7bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 135842 sentence pairs\n",
            "Counted words:\n",
            "fra 21326\n",
            "eng 13034\n",
            "Number of sentence pairs: 135842\n",
            "Vocabulary size: 21326 13034\n",
            "Starting training for 40000 iterations with learning rate 0.01...\n",
            "\n",
            "Iteration 1/40000:\n",
            "Input tensor: tensor([[   36],\n",
            "        [ 4599],\n",
            "        [   51],\n",
            "        [  203],\n",
            "        [ 3325],\n",
            "        [  303],\n",
            "        [12118],\n",
            "        [   10],\n",
            "        [    1]])\n",
            "Target tensor: tensor([[  51],\n",
            "        [7956],\n",
            "        [2549],\n",
            "        [ 271],\n",
            "        [  51],\n",
            "        [  71],\n",
            "        [ 693],\n",
            "        [  11],\n",
            "        [   1]])\n",
            "29m 43.883622884750366s (- 208m 7.1853601932525635s) | Iteration 5000 (12.50%) | Average Loss: 4.2928\n",
            "58m 49.68821716308594s (- 176m 29.064651489257812s) | Iteration 10000 (25.00%) | Average Loss: 3.3449\n",
            "87m 57.799914836883545s (- 146m 36.33319139480591s) | Iteration 15000 (37.50%) | Average Loss: 3.0034\n",
            "119m 0.40483999252319336s (- 119m 0.40483999252319336s) | Iteration 20000 (50.00%) | Average Loss: 2.7839\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare data\n",
        "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)  # Load English-French sentence pairs, reversing the language order\n",
        "print(f\"Number of sentence pairs: {len(pairs)}\")  # Print the total number of training pairs loaded\n",
        "print(\"Vocabulary size:\", input_lang.n_words, output_lang.n_words)  # Print vocabulary sizes for both input and output languages\n",
        "\n",
        "# Initialize models\n",
        "hidden_size = 256  # Set hidden layer size for the RNNs\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)  # Initialize the encoder model with the input language's vocabulary size\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)  # Initialize the attention-based decoder model\n",
        "\n",
        "# Train models for 1000 iterations\n",
        "train_iters(encoder, attn_decoder, n_iters=40000, pairs=pairs, input_lang=input_lang, output_lang=output_lang, print_every=5000)\n",
        "# Train the encoder and decoder models on the data for 75000 iterations and print the loss every 5000 iterations\n",
        "\n",
        "# Evaluate on random pairs and visualize attention\n",
        "evaluate_randomly(encoder, attn_decoder, pairs, input_lang, output_lang)  # Evaluate and print translations of random pairs from the dataset\n",
        "evaluate_and_show_attention(\"je suis trop fatigué\", encoder, attn_decoder, input_lang, output_lang)  # Evaluate and visualize attention for a specific input sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d20jTBt9vL1O"
      },
      "source": [
        "# Saving and Loading the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkcNC2t7sXDK"
      },
      "outputs": [],
      "source": [
        "# Function to save the model\n",
        "def save_model(encoder, decoder, encoder_path=\"savedModels\\encoder.pth\", decoder_path=\"savedModels\\decoder.pth\"):\n",
        "    torch.save(encoder.state_dict(), encoder_path)\n",
        "    torch.save(decoder.state_dict(), decoder_path)\n",
        "    print(\"Models saved to disk.\")\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(encoder, decoder, encoder_path=\"encoder.pth\", decoder_path=\"decoder.pth\"):\n",
        "    encoder.load_state_dict(torch.load(encoder_path))\n",
        "    decoder.load_state_dict(torch.load(decoder_path))\n",
        "    print(\"Models loaded from disk.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U03NVyrHwU-j"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWHTdsavS7w"
      },
      "outputs": [],
      "source": [
        "save_model(encoder, attn_decoder)  # Save the trained models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DHxpdqTwYse"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl7bGfNivUc0"
      },
      "outputs": [],
      "source": [
        "load_model(encoder, attn_decoder)  # Load the trained models for evaluation or inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRw4Hxowwscd"
      },
      "source": [
        "## Loading the Model for Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O_ws9Icwuax"
      },
      "outputs": [],
      "source": [
        "# Define model architecture\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# Load the saved weights into the model\n",
        "encoder.load_state_dict(torch.load(\"encoder.pth\"))\n",
        "attn_decoder.load_state_dict(torch.load(\"decoder.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYgNZw-4wxuJ"
      },
      "source": [
        "## Using the Loaded Model for New Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhViZuDcwyla"
      },
      "outputs": [],
      "source": [
        "# New input sentence to translate\n",
        "new_sentence = \"je suis très heureux\"\n",
        "\n",
        "# Evaluate and print the output\n",
        "output_words, attentions = evaluate(encoder, attn_decoder, new_sentence, input_lang, output_lang)\n",
        "output_sentence = ' '.join(output_words)\n",
        "print(\"Translated sentence:\", output_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdRgkOmPvPj6"
      },
      "source": [
        "# Evaluating Multiple Sentences At Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiWb3IrMvaqT"
      },
      "outputs": [],
      "source": [
        "def evaluate_batch(encoder, decoder, sentences, input_lang, output_lang, max_length=10):\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        output_words, attentions = evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        results.append((sentence, output_sentence))\n",
        "    return results\n",
        "\n",
        "# Usage\n",
        "sentences_to_translate = [\"je suis trop fatigué\", \"il fait beau aujourd'hui\", \"j'aime programmer\"]\n",
        "translations = evaluate_batch(encoder, attn_decoder, sentences_to_translate, input_lang, output_lang)\n",
        "for original, translated in translations:\n",
        "    print(f\"Input: {original} -> Output: {translated}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40JkEhHivhVq"
      },
      "source": [
        "# Calculate BLEU Score for Translation Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8CLkRjavcwK"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu(encoder, decoder, pairs, input_lang, output_lang):\n",
        "    scores = []\n",
        "    # Smoothing function to handle cases with zero n-gram overlap\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "    for pair in pairs:\n",
        "        # Get the predicted output\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        reference = pair[1].split()  # Target sentence split into words\n",
        "        candidate = output_words[:-1]  # Exclude <EOS> token from prediction\n",
        "\n",
        "        # Calculate BLEU score with smoothing\n",
        "        score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg_bleu_score = sum(scores) / len(scores)\n",
        "    print(f\"Average BLEU score for the dataset: {avg_bleu_score:.4f}\")\n",
        "    return avg_bleu_score\n",
        "\n",
        "# Usage\n",
        "calculate_bleu(encoder, attn_decoder, pairs, input_lang, output_lang)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZY6J8VGvkWi"
      },
      "source": [
        "# Interactive Translation with User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5hL42HFvjvh"
      },
      "outputs": [],
      "source": [
        "def interactive_translation(encoder, decoder, input_lang, output_lang):\n",
        "    while True:\n",
        "        sentence = input(\"Enter a sentence in French (or type 'quit' to exit): \")\n",
        "        if sentence.lower() == 'quit':\n",
        "            break\n",
        "        output_words, attentions = evaluate(encoder, decoder, sentence, input_lang, output_lang)\n",
        "        print(\"Translation:\", ' '.join(output_words))\n",
        "\n",
        "# Usage\n",
        "interactive_translation(encoder, attn_decoder, input_lang, output_lang)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8CCFGL3vpt3"
      },
      "source": [
        "# Detailed Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG_kJV23vqoh"
      },
      "outputs": [],
      "source": [
        "def show_detailed_attention(input_sentence, output_words, attentions):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))  # Adjust figure size\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='viridis')  # Choose a different color map for clarity\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axis labels with input and output words\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90, fontsize=12)\n",
        "    ax.set_yticklabels([''] + output_words, fontsize=12)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "output_words, attentions = evaluate(encoder, attn_decoder, \"je suis trop fatigué\", input_lang, output_lang)\n",
        "show_detailed_attention(\"je suis trop fatigué\", output_words, attentions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRuKiKrvwZI"
      },
      "source": [
        "# Saving Attention Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORW8gkZzvv_9"
      },
      "outputs": [],
      "source": [
        "def save_attention_plot(input_sentence, output_words, attentions, file_name=\"attention_plot.png\"):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='coolwarm')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90, fontsize=12)\n",
        "    ax.set_yticklabels([''] + output_words, fontsize=12)\n",
        "\n",
        "    # Create 'savedPlots' folder if it doesn't exist\n",
        "    os.makedirs(\"savedPlots\", exist_ok=True)\n",
        "\n",
        "    # Corrected and improved path for saving\n",
        "    save_path = os.path.join(\"savedPlots\", file_name)\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"Attention plot saved as {save_path}\")\n",
        "\n",
        "# Usage\n",
        "output_words, attentions = evaluate(encoder, attn_decoder, \"je suis trop fatigué\", input_lang, output_lang)\n",
        "save_attention_plot(\"je suis trop fatigué\", output_words, attentions, file_name=\"attention_fatigue.png\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}