{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNskLOVuLETOX6RNxuRgvT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L06-Feed%20Forward%20Networks%20for%20Natural%20Language%20Processing/06_2D_Convolutions_on_Image_Like_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KUUzDssxAgyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Introduction**\n",
        "\n",
        "- **What are 2D Convolutions?**\n",
        "  - **2D Convolutions** are used to process **image data** where the input is a grid of pixels (e.g., a 2D array representing an image).\n",
        "  - Each convolutional layer applies **filters** (also called kernels) that slide over the image to detect patterns like edges, textures, or shapes.\n",
        "  - The filter moves across the image, performing **element-wise multiplication** with the input, and produces an output called a **feature map**.\n",
        "\n",
        "- **Key Concepts**:\n",
        "  - **Input Channels**: The number of channels in the input image (e.g., 1 for grayscale, 3 for RGB images).\n",
        "  - **Output Channels**: The number of filters applied in the convolution, each producing its own feature map.\n",
        "  - **Kernel Size**: The size of the sliding window used to extract local patterns from the image (e.g., 3x3, 5x5).\n",
        "  \n",
        "- **Applications**:\n",
        "  - Image classification (e.g., identifying objects in images).\n",
        "  - Object detection (e.g., detecting faces, cars, or other objects).\n",
        "  - Feature extraction (e.g., identifying edges, textures).\n",
        "\n",
        "**Observation**:\n",
        "- **2D convolutions** are the backbone of most computer vision models. They allow the network to learn spatial hierarchies by capturing local patterns in images.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Wcq3ml1-J13T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. **Defining 2D Convolutions**\n",
        "\n",
        "- **Creating Input Data**:\n",
        "  - We will create a **4D tensor** to represent a batch of images where:\n",
        "    - **Batch size**: The number of images processed in parallel.\n",
        "    - **Input channels**: Number of channels in the image (e.g., 3 for RGB).\n",
        "    - **Height and width**: The spatial dimensions of the image (e.g., 3x3 or 5x5).\n",
        "\n",
        "  **Example 1: Defining a Simple 2D Convolution**:\n",
        "  - Create a tensor of shape `(1, 2, 3, 3)` representing a batch of 1 image with 2 channels (e.g., a 2-channel grayscale image) and a size of `3x3` pixels.\n",
        "  - Define a **2D convolutional layer** (`conv1`) with:\n",
        "    - 2 input channels (matching the imageâ€™s 2 channels).\n",
        "    - 1 output channel (meaning we will use 1 filter).\n",
        "    - A kernel size of `2x2`.\n",
        "\n",
        "**Code**:\n"
      ],
      "metadata": {
        "id": "fEjZeNfbJ18o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input tensor (1 image, 2 channels, 3x3 pixels)\n",
        "x = torch.randn(1, 2, 3, 3)  # Shape: (batch_size, input_channels, height, width)\n",
        "print(\"Input shape:\", x.shape)\n",
        "\n",
        "# Define 2D convolutional layer (2 input channels, 1 output channel, kernel 2x2)\n",
        "conv1 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2)\n",
        "\n",
        "# Apply convolution\n",
        "output = conv1(x)\n",
        "print(\"Output shape after conv1:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1yclymX_hS1",
        "outputId": "815d2884-476a-4275-f578-5a513d73912f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 2, 3, 3])\n",
            "Output shape after conv1: torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- The convolution layer applies a 2x2 filter over the input image, sliding it across the image and producing a smaller output (feature map).\n",
        "  \n",
        "**Output Shape**:\n",
        "- After the convolution, the spatial dimensions of the image (height and width) shrink because the filter captures local information as it moves across the image.\n",
        "\n",
        "**Observation**:\n",
        "- The **output size** depends on the kernel size, stride, padding, and input size. A smaller kernel reduces the spatial dimensions of the output.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NXZ40Up4J2Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. **Visualizing Convolution Weights**\n",
        "\n",
        "- **Understanding Convolutional Weights**:\n",
        "  - The filters (kernels) in the convolution layer are **learnable parameters**. Each filter extracts specific features from the image (e.g., horizontal edges, vertical lines).\n",
        "  - The weights of the convolutional layer represent the filter values that will be applied to the input image.\n",
        "\n",
        "- **Printing the Convolution Weights**:\n",
        "  - The weights of the convolution layer can be accessed and printed. This helps visualize how filters operate during the forward pass.\n",
        "\n",
        "**Code**:\n"
      ],
      "metadata": {
        "id": "UjGtIoGpJ2Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Convolution weights (filter values):\")\n",
        "print(conv1.weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMgVLTey_oLi",
        "outputId": "d74e03f5-ee8e-44ad-ebf5-214fe8ed0671"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolution weights (filter values):\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0304,  0.1318],\n",
            "          [-0.3129, -0.1099]],\n",
            "\n",
            "         [[-0.0658,  0.3372],\n",
            "          [ 0.1099,  0.0564]]]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Convolution weights** (filter values) are initialized randomly at first but will be adjusted during training to capture relevant features (e.g., edges, textures).\n",
        "\n",
        "**Observation**:\n",
        "- Each **filter** in the convolutional layer learns to detect different patterns in the image, such as edges, corners, or more complex textures.\n",
        "\n",
        "**Demonstration**:\n",
        "- Visualize how the filters operate:\n"
      ],
      "metadata": {
        "id": "W-RPL8e7_p1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Filter shape:\", conv1.weight.shape)  # Shows filter size and channels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muxkMK9s_qMY",
        "outputId": "cae1f452-3306-41fd-8ab9-a6511f36f880"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter shape: torch.Size([1, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 4. **Exercise**\n",
        "\n",
        "- **Modify the Number of Output Channels**:\n",
        "  - The **number of output channels** defines how many different filters will be applied to the input. Each filter produces a feature map that captures different aspects of the input image.\n",
        "  \n",
        "  **Task**:\n",
        "  - Modify the convolutional layer to have **2 output channels** (i.e., 2 filters), each producing a different feature map:\n"
      ],
      "metadata": {
        "id": "-1fgr4DGJ2Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=2)\n",
        "output = conv2(x)\n",
        "print(\"Output shape with 2 output channels:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOn8UELt_xad",
        "outputId": "94d5229f-d544-40b8-ae64-7475dccb56ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with 2 output channels: torch.Size([1, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Observation**:\n",
        "  - Increasing the number of output channels increases the number of feature maps produced by the layer, allowing the network to capture more complex and varied patterns.\n",
        "\n",
        "- **Experiment with Kernel Sizes**:\n",
        "  - The **kernel size** determines the size of the region in the input image that the filter covers at each step. Larger kernels capture broader patterns, while smaller kernels capture finer details.\n",
        "  \n",
        "  **Task**:\n",
        "  - Change the kernel size to `3x3` and observe how the output shape changes:\n"
      ],
      "metadata": {
        "id": "0qACGUIXJ2Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv3 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=3)\n",
        "output = conv3(x)\n",
        "print(\"Output shape with 3x3 kernel:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WygGd8Vf_1E_",
        "outputId": "57062007-0131-4fcd-aab2-6bb9d0920a05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with 3x3 kernel: torch.Size([1, 2, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation**:\n",
        "  - A larger kernel reduces the spatial dimensions more significantly because it captures a larger portion of the image at each step.\n",
        "\n",
        "**Observation**:\n",
        "- **Larger kernels** tend to capture broader, more general patterns in the image, while **smaller kernels** capture finer, more localized details.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oe0qgLVCJ3QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. **Conclusion**\n",
        "\n",
        "- **Recap**:\n",
        "  - **2D Convolutions** are essential for feature extraction in image data, where they learn to detect local patterns (e.g., edges, textures).\n",
        "  - **Filters** (kernels) are the learnable parameters of the convolutional layer, and each filter captures different features in the image.\n",
        "  - The output of a convolutional layer is a **feature map** that represents the detected patterns at various locations in the input image.\n",
        "  \n",
        "- **Relationship Between Kernel Size, Stride, and Output Size**:\n",
        "  - The size of the output feature map depends on several factors:\n",
        "    - **Kernel size**: Larger kernels reduce the spatial dimensions more.\n",
        "    - **Stride**: The step size of the filter as it moves across the image. Larger strides reduce the output size more quickly.\n",
        "    - **Padding**: Adding padding around the input image helps preserve spatial dimensions after convolution.\n",
        "\n",
        "**Quiz**:\n",
        "- What happens to the output size when you increase the kernel size?\n",
        "- How does increasing the number of output channels affect the modelâ€™s ability to detect patterns?\n",
        "\n",
        "**Takeaway**:\n",
        "- 2D convolutions are a powerful tool for image processing because they allow the network to automatically learn relevant features from the data, making them a cornerstone of modern computer vision models.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_W91bPV7J3QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations:\n"
      ],
      "metadata": {
        "id": "rxCxaiK7AcmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. **Observation: 2D Convolutions Learn Local Patterns in Images**\n",
        "   - **2D convolutions** allow CNNs to detect spatial patterns like edges, textures, or shapes in images. Each convolutional layer applies multiple filters (kernels) to the input image, extracting local features.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "k2bYLWAmAci_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create input tensor (1 image, 2 channels, 3x3 pixels)\n",
        "x = torch.randn(1, 2, 3, 3)  # Shape: (batch_size, channels, height, width)\n",
        "print(\"Input shape:\", x.shape)\n",
        "\n",
        "# Define 2D convolutional layer (2 input channels, 1 output channel, kernel size 2x2)\n",
        "conv1 = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2)\n",
        "\n",
        "# Apply convolution\n",
        "output = conv1(x)\n",
        "print(\"Output shape after conv1:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDKh5pPYB3Dk",
        "outputId": "cbfedaf7-fbea-451c-f140-74cc8362d2fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 2, 3, 3])\n",
            "Output shape after conv1: torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. **Observation: Output Size Depends on Kernel Size, Stride, Padding, and Input Size**\n",
        "   - The size of the output feature map is determined by the kernel size, stride, padding, and the input image size.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "oVQ5-Ck7Acga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input size:\", x.shape)\n",
        "print(\"Kernel size:\", conv1.kernel_size)\n",
        "print(\"Output size after convolution:\", output.shape)\n",
        "\n",
        "# Smaller kernel sizes and larger strides reduce the output size. Padding can help preserve the original input size.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8qtEnTBCAk5",
        "outputId": "e0ccffe2-d2e1-449f-8fc9-4e410b99e92b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size: torch.Size([1, 2, 3, 3])\n",
            "Kernel size: (2, 2)\n",
            "Output size after convolution: torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. **Observation: Convolutional Layer Weights are Learnable Parameters**\n",
        "   - The filters (kernels) in a convolutional layer are **learnable parameters**, meaning they are updated during training to capture relevant features.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "Vq6mMpr4Acdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Convolution weights (filter values):\")\n",
        "print(conv1.weight)\n",
        "\n",
        "# The weights of the convolutional layer start with random initialization and are adjusted during training to capture patterns such as edges or textures.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdzoK5BCCD45",
        "outputId": "9a93dd8d-edd7-49c4-bf64-12ccc59f7af7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolution weights (filter values):\n",
            "Parameter containing:\n",
            "tensor([[[[-0.2693,  0.1791],\n",
            "          [-0.1832,  0.3400]],\n",
            "\n",
            "         [[-0.0794,  0.0911],\n",
            "          [-0.0279, -0.1263]]]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. **Observation: Increasing Output Channels Increases Feature Maps**\n",
        "   - Each output channel corresponds to a different filter applied to the input. More output channels mean more feature maps, each capturing different aspects of the input image.\n",
        "   - **Demonstration**:\n",
        ""
      ],
      "metadata": {
        "id": "09xrMEBgAcbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=2)  # Two filters\n",
        "output = conv2(x)\n",
        "print(\"Output shape with 2 output channels:\", output.shape)\n",
        "\n",
        "# By increasing the number of output channels, the model learns a broader set of features. In this example, the output shape changes from `(1, 1, 2, 2)` to `(1, 2, 2, 2)`.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MARNQS8BCEHG",
        "outputId": "89148c07-8793-485b-b263-fd9890dc1f05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with 2 output channels: torch.Size([1, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. **Observation: Changing Kernel Size Affects the Output**\n",
        "   - A larger kernel size reduces the spatial dimensions of the output more significantly as it captures a larger portion of the input image at each step.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "552e5TQLAcYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv3 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=3)  # Kernel size 3x3\n",
        "output = conv3(x)\n",
        "print(\"Output shape with 3x3 kernel:\", output.shape)\n",
        "\n",
        "# The output size is much smaller when a 3x3 kernel is used. This is because a larger kernel covers more of the image, reducing the output dimensions faster.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZmHLIbCESS",
        "outputId": "5046a8d0-a165-4a8c-fd7a-a04c8e60aaee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with 3x3 kernel: torch.Size([1, 2, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6. **Observation: Larger Kernels Capture Broader Patterns, Smaller Kernels Capture Finer Details**\n",
        "   - Larger kernels are better at capturing broad, global patterns (e.g., shapes), while smaller kernels capture finer, more localized details (e.g., edges).\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "9brIkoKGAcWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Small kernel (2x2) captures fine details, larger kernel (3x3) captures broad patterns.\")\n",
        "# Example of smaller and larger kernel application shown previously\n",
        "\n",
        "\n",
        "# For tasks where capturing fine details is important (e.g., object detection), smaller kernels are often preferred. For capturing global context, larger kernels are useful.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFpZJUVCCEct",
        "outputId": "4e744960-be96-4ca4-951d-06d2c03cac0b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small kernel (2x2) captures fine details, larger kernel (3x3) captures broad patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 7. **Observation: Visualizing Filters and Feature Maps**\n",
        "   - Each filter learns to detect a specific feature, such as edges, textures, or more complex shapes. Printing the filter values shows how they are applied to the image.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "iHvER3ULAcTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Visualizing filters:\")\n",
        "print(conv1.weight.shape)  # Shape of the filter (1, 2, 2, 2)\n",
        "print(\"Convolution weights:\", conv1.weight)\n",
        "\n",
        "\n",
        "# This demonstrates the shape of the convolutional filters and how they affect the input. Each filter extracts unique features during the forward pass.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a00o7Ea2CEnS",
        "outputId": "a78866a4-c64b-4781-c25c-088f5491f916"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing filters:\n",
            "torch.Size([1, 2, 2, 2])\n",
            "Convolution weights: Parameter containing:\n",
            "tensor([[[[-0.2693,  0.1791],\n",
            "          [-0.1832,  0.3400]],\n",
            "\n",
            "         [[-0.0794,  0.0911],\n",
            "          [-0.0279, -0.1263]]]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 8. **Observation: Padding Helps Preserve Spatial Dimensions**\n",
        "   - Adding padding around the input can help preserve the input's original dimensions after convolution.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "TaK91Q0PAuEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_with_padding = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2, padding=1)\n",
        "padded_output = conv_with_padding(x)\n",
        "print(\"Output shape with padding:\", padded_output.shape)\n",
        "\n",
        "# The padding ensures that the outputâ€™s spatial dimensions are closer to the original inputâ€™s dimensions, which is useful when you want to maintain a certain size.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puTjNVPpCExV",
        "outputId": "31a0ea70-5c39-46e9-a810-6d3d8f2f63df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with padding: torch.Size([1, 1, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 9. **Observation: Stride Affects How Much the Filter Moves Across the Input**\n",
        "   - Stride determines how far the filter moves in each step. Larger strides reduce the output size by skipping over parts of the input.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "2RrIH5kdAuAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_with_stride = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=2, stride=2)\n",
        "stride_output = conv_with_stride(x)\n",
        "print(\"Output shape with stride 2:\", stride_output.shape)\n",
        "\n",
        "# With a stride of 2, the output size is halved compared to a stride of 1. Stride is useful for reducing the dimensions of the feature map without using pooling.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcMtULGvCE7o",
        "outputId": "e4768309-8168-4236-99ae-fc2228c19c8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape with stride 2: torch.Size([1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 10. **Observation: Multiple Convolutional Layers Capture More Complex Patterns**\n",
        "   - Stacking convolutional layers allows the network to build upon the simple features captured by earlier layers and detect more complex patterns in the data.\n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "GCFU9NrRAt-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiConvCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiConvCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model and pass the input through multiple convolutional layers\n",
        "multi_conv_model = MultiConvCNN()\n",
        "output = multi_conv_model(x)\n",
        "print(\"Output shape after multiple convolutions:\", output.shape)\n",
        "\n",
        "# The first layer detects simple features like edges, while the second layer builds on those to capture more complex features like textures or shapes. Each layer adds more depth and complexity to the feature extraction process.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6eD7lF-CFGs",
        "outputId": "3e45d7b2-b7f5-4161-a284-02038a07d81f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape after multiple convolutions: torch.Size([1, 8, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 11. **Observation: Increasing the Number of Input Channels Increases Complexity**\n",
        "   - The number of input channels corresponds to the depth of the input data. More input channels increase the complexity of the convolutional layer, allowing the model to process multi-channel data (e.g., RGB images with 3 channels or multi-spectral images with more than 3 channels).\n",
        "   \n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "OiJpUkWyAt7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with 3 input channels (e.g., an RGB image with height=5, width=5)\n",
        "x = torch.randn(1, 3, 5, 5)  # Shape: (batch_size, input_channels, height, width)\n",
        "print(\"Input shape with 3 channels:\", x.shape)\n",
        "\n",
        "# Define a 2D convolutional layer with 3 input channels and 2 output channels\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3)\n",
        "\n",
        "# Apply convolution\n",
        "output = conv(x)\n",
        "print(\"Output shape after convolution with 3 input channels:\", output.shape)\n",
        "\n",
        "# This demonstrates how the number of input channels affects the convolutional layer's operation. Each filter processes all the input channels, combining information from them to produce the output feature maps.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9QKcaV5CFS1",
        "outputId": "67c41d74-368f-45ad-b1dd-ee48df004eb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape with 3 channels: torch.Size([1, 3, 5, 5])\n",
            "Output shape after convolution with 3 input channels: torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 12. **Observation: Pooling Layers Reduce Spatial Dimensions and Focus on Important Features**\n",
        "   - Pooling layers are used to downsample feature maps by reducing their spatial dimensions, while retaining the most important information (e.g., maximum values in max pooling).\n",
        "   \n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "mKYKS2o1At4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a 2D max pooling layer with a 2x2 kernel\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# Create an input tensor with a larger spatial dimension\n",
        "x = torch.randn(1, 1, 6, 6)  # Shape: (batch_size, channels, height, width)\n",
        "print(\"Input shape before pooling:\", x.shape)\n",
        "\n",
        "# Apply max pooling\n",
        "pooled_output = pool(x)\n",
        "print(\"Output shape after max pooling:\", pooled_output.shape)\n",
        "\n",
        "# Max pooling reduces the input size by taking the maximum value from each 2x2 region. This operation helps to reduce the computational load and focus on the most significant features of the image.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t75UHVIJCFer",
        "outputId": "67290b3a-afb1-4536-b9c2-8df15d6a8b40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape before pooling: torch.Size([1, 1, 6, 6])\n",
            "Output shape after max pooling: torch.Size([1, 1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 13. **Observation: Batch Size Affects How Many Images Are Processed Simultaneously**\n",
        "   - The batch size determines how many images or samples are processed in parallel. Larger batch sizes allow more data to be processed at once, increasing efficiency during training.\n",
        "   \n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "FWbvjaPVAt18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input tensor with batch size of 4 (4 images, each with 1 channel, 5x5 pixels)\n",
        "x = torch.randn(4, 1, 5, 5)  # Shape: (batch_size, channels, height, width)\n",
        "print(\"Input shape with batch size 4:\", x.shape)\n",
        "\n",
        "# Define a 2D convolutional layer\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
        "\n",
        "# Apply convolution\n",
        "output = conv(x)\n",
        "print(\"Output shape after convolution with batch size 4:\", output.shape)\n",
        "\n",
        "# Batch size affects how many samples are processed simultaneously during training or inference. In this example, four images are processed at once, each resulting in a corresponding output after the convolution operation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iX2Bc33CFp_",
        "outputId": "65a358ec-baeb-4d79-a6e5-0074f067ff4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape with batch size 4: torch.Size([4, 1, 5, 5])\n",
            "Output shape after convolution with batch size 4: torch.Size([4, 3, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 14. **Observation: Multiple Convolutions and Pooling Layers Can Be Stacked to Learn Hierarchical Features**\n",
        "   - By stacking multiple convolutional and pooling layers, CNNs can learn hierarchical representations. Early layers capture low-level features (e.g., edges), while deeper layers capture more abstract features (e.g., shapes, textures).\n",
        "   \n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "rFrQz0sKAtzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # Import the necessary module for activation functions\n",
        "\n",
        "class CNNModel(nn.Module):  # Define a Convolutional Neural Network (CNN) model\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()  # Call the parent class's constructor to initialize the network\n",
        "\n",
        "        # Describe the overall architecture\n",
        "        print(\"Initializing CNN with two convolutional and two pooling layers.\")\n",
        "\n",
        "        # First convolutional layer: 1 input channel, 6 output channels, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
        "        print(\"Layer 1: Convolutional Layer with 1 input channel and 6 output channels, kernel size 3x3.\")\n",
        "\n",
        "        # First pooling layer: 2x2 pooling with stride of 2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        print(\"Layer 2: Max Pooling Layer with 2x2 window and stride 2.\")\n",
        "\n",
        "        # Second convolutional layer: 6 input channels, 12 output channels, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3)\n",
        "        print(\"Layer 3: Convolutional Layer with 6 input channels and 12 output channels, kernel size 3x3.\")\n",
        "\n",
        "        # Second pooling layer: 2x2 pooling with stride of 2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        print(\"Layer 4: Max Pooling Layer with 2x2 window and stride 2.\")\n",
        "\n",
        "        # Describe the architecture summary after initialization\n",
        "        print(\"\\nCNN Model Initialized with two convolutional and two pooling layers.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Describe the input before passing through layers\n",
        "        print(f\"\\nForward Pass - Input Tensor Shape: {x.shape}\")\n",
        "\n",
        "        # Apply first convolution, activation (ReLU), and pooling\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        print(f\"After Layer 1 (Conv1 -> ReLU -> Pool1) - Tensor Shape: {x.shape}\")\n",
        "\n",
        "        # Apply second convolution, activation (ReLU), and pooling\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        print(f\"After Layer 2 (Conv2 -> ReLU -> Pool2) - Tensor Shape: {x.shape}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "    def describe_model_architecture(self):\n",
        "        # Additional method to describe the architecture in more detail\n",
        "        print(\"\\n==== Model Architecture Description ====\")\n",
        "        print(f\"First Convolutional Layer:\\n\\t\"\n",
        "              f\"Input: 1 channel, Output: 6 channels, Kernel: 3x3.\")\n",
        "        print(\"First layer detects basic features such as edges in the image.\")\n",
        "\n",
        "        print(f\"First Pooling Layer:\\n\\t\"\n",
        "              f\"Max Pooling with 2x2 window. Reduces spatial dimensions while preserving important features.\")\n",
        "\n",
        "        print(f\"Second Convolutional Layer:\\n\\t\"\n",
        "              f\"Input: 6 channels, Output: 12 channels, Kernel: 3x3.\")\n",
        "        print(\"Second layer detects more complex patterns formed by combinations of edges, such as shapes.\")\n",
        "\n",
        "        print(f\"Second Pooling Layer:\\n\\t\"\n",
        "              f\"Max Pooling with 2x2 window. Further reduces spatial dimensions and enhances features.\")\n",
        "        print(\"========================================\\n\")\n",
        "\n",
        "# Instantiate the CNN model\n",
        "model = CNNModel()\n",
        "\n",
        "# Call the method to describe the architecture in more detail\n",
        "model.describe_model_architecture()\n",
        "\n",
        "# Create an input tensor (batch size=1, 1 input channel, 12x12 pixels)\n",
        "x = torch.randn(1, 1, 12, 12)\n",
        "\n",
        "# Forward pass through the network\n",
        "output = model(x)\n",
        "print(f\"\\nFinal Output Shape: {output.shape}\")\n",
        "\n",
        "# Observing Layer Weights and Biases for further understanding\n",
        "print(\"\\n==== Layer Weights and Biases Observation ====\")\n",
        "print(f\"Conv1 Weights:\\n{model.conv1.weight}\")\n",
        "print(f\"Conv1 Biases:\\n{model.conv1.bias}\")\n",
        "print(f\"Conv2 Weights:\\n{model.conv2.weight}\")\n",
        "print(f\"Conv2 Biases:\\n{model.conv2.bias}\")\n",
        "print(\"===============================================\")\n",
        "\n",
        "# Demonstration with larger batch of inputs\n",
        "batch_input = torch.randn(5, 1, 12, 12)  # Simulating a batch of 5 input samples\n",
        "print(\"\\nForward Pass with Batch of Inputs\")\n",
        "print(f\"Input Shape (Batch of 5 samples): {batch_input.shape}\")\n",
        "\n",
        "# Forward pass for the batch\n",
        "batch_output = model(batch_input)\n",
        "print(f\"Output Shape after Forward Pass with Batch: {batch_output.shape}\\n\")\n",
        "\n",
        "# Visualizing the weight distribution for both convolutional layers (Optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_weights(layer, layer_name):\n",
        "    # Plot the weights of the given convolutional layer\n",
        "    plt.hist(layer.weight.detach().numpy().flatten(), bins=20, alpha=0.7)\n",
        "    plt.title(f\"Weight Distribution in {layer_name}\")\n",
        "    plt.xlabel(\"Weight values\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the weight distribution for Conv1 and Conv2\n",
        "plot_weights(model.conv1, \"Conv Layer 1\")\n",
        "plot_weights(model.conv2, \"Conv Layer 2\")\n",
        "\n",
        "# Analyzing the effect of input scaling\n",
        "scaled_input = torch.randn(1, 1, 12, 12) * 100  # Creating a scaled-up version of the input\n",
        "print(\"\\nForward Pass with Scaled Input\")\n",
        "print(f\"Scaled Input:\\n{scaled_input}\")\n",
        "\n",
        "# Pass scaled input through the network\n",
        "scaled_output = model(scaled_input)\n",
        "print(f\"Output for Scaled Input: {scaled_output}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JawFKkZZCF0_",
        "outputId": "819c9010-06c6-43a5-f82d-9cdaa4f4a147"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing CNN with two convolutional and two pooling layers.\n",
            "Layer 1: Convolutional Layer with 1 input channel and 6 output channels, kernel size 3x3.\n",
            "Layer 2: Max Pooling Layer with 2x2 window and stride 2.\n",
            "Layer 3: Convolutional Layer with 6 input channels and 12 output channels, kernel size 3x3.\n",
            "Layer 4: Max Pooling Layer with 2x2 window and stride 2.\n",
            "\n",
            "CNN Model Initialized with two convolutional and two pooling layers.\n",
            "\n",
            "==== Model Architecture Description ====\n",
            "First Convolutional Layer:\n",
            "\tInput: 1 channel, Output: 6 channels, Kernel: 3x3.\n",
            "First layer detects basic features such as edges in the image.\n",
            "First Pooling Layer:\n",
            "\tMax Pooling with 2x2 window. Reduces spatial dimensions while preserving important features.\n",
            "Second Convolutional Layer:\n",
            "\tInput: 6 channels, Output: 12 channels, Kernel: 3x3.\n",
            "Second layer detects more complex patterns formed by combinations of edges, such as shapes.\n",
            "Second Pooling Layer:\n",
            "\tMax Pooling with 2x2 window. Further reduces spatial dimensions and enhances features.\n",
            "========================================\n",
            "\n",
            "\n",
            "Forward Pass - Input Tensor Shape: torch.Size([1, 1, 12, 12])\n",
            "After Layer 1 (Conv1 -> ReLU -> Pool1) - Tensor Shape: torch.Size([1, 6, 5, 5])\n",
            "After Layer 2 (Conv2 -> ReLU -> Pool2) - Tensor Shape: torch.Size([1, 12, 1, 1])\n",
            "\n",
            "Final Output Shape: torch.Size([1, 12, 1, 1])\n",
            "\n",
            "==== Layer Weights and Biases Observation ====\n",
            "Conv1 Weights:\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0391,  0.3233, -0.0247],\n",
            "          [-0.0132, -0.1770, -0.0428],\n",
            "          [ 0.1147,  0.0669, -0.0178]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2482, -0.3037,  0.3268],\n",
            "          [-0.3222,  0.3197, -0.1788],\n",
            "          [-0.1937, -0.2078,  0.2807]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2823,  0.1371,  0.3131],\n",
            "          [ 0.0566,  0.0562, -0.2940],\n",
            "          [-0.2084, -0.0345,  0.1050]]],\n",
            "\n",
            "\n",
            "        [[[-0.1689, -0.0641,  0.1600],\n",
            "          [-0.1388,  0.3121, -0.2660],\n",
            "          [ 0.2545, -0.2278,  0.1606]]],\n",
            "\n",
            "\n",
            "        [[[-0.2646,  0.1692,  0.0293],\n",
            "          [ 0.1814,  0.2657, -0.3147],\n",
            "          [ 0.2188,  0.2504,  0.2959]]],\n",
            "\n",
            "\n",
            "        [[[-0.2641,  0.2858,  0.1270],\n",
            "          [ 0.2095, -0.0788, -0.0654],\n",
            "          [-0.1840, -0.2919, -0.2966]]]], requires_grad=True)\n",
            "Conv1 Biases:\n",
            "Parameter containing:\n",
            "tensor([ 0.3206, -0.1213, -0.0428, -0.1573, -0.2298, -0.2473],\n",
            "       requires_grad=True)\n",
            "Conv2 Weights:\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.4056e-02, -9.3984e-02,  5.7630e-02],\n",
            "          [ 4.4324e-02, -3.1718e-02, -7.3027e-02],\n",
            "          [ 8.1154e-02,  1.3087e-01, -1.1119e-01]],\n",
            "\n",
            "         [[-8.7618e-02,  4.9851e-02,  1.6879e-03],\n",
            "          [ 1.1283e-01, -9.9810e-02, -9.8224e-03],\n",
            "          [-5.9990e-05, -4.7354e-02, -5.3917e-02]],\n",
            "\n",
            "         [[-5.7283e-02, -7.2137e-02, -9.4699e-02],\n",
            "          [ 5.8821e-02,  4.1894e-02,  1.2924e-01],\n",
            "          [ 4.2302e-03, -5.6755e-02, -9.3831e-02]],\n",
            "\n",
            "         [[-4.3030e-02, -1.6080e-02, -1.1687e-01],\n",
            "          [-7.9159e-02,  6.1376e-02, -9.7079e-02],\n",
            "          [ 5.7954e-02, -1.1747e-01, -3.3076e-02]],\n",
            "\n",
            "         [[-8.6012e-02, -8.2033e-02, -8.3619e-02],\n",
            "          [-4.0060e-02,  8.9363e-02,  6.4852e-02],\n",
            "          [ 1.1024e-01, -3.6155e-02,  1.3027e-01]],\n",
            "\n",
            "         [[-5.5343e-02, -9.3004e-02,  1.2339e-01],\n",
            "          [-7.9332e-02, -7.0507e-02, -3.4256e-02],\n",
            "          [-4.5895e-02, -6.1653e-03,  4.0548e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1202e-01,  1.0499e-01,  7.3088e-02],\n",
            "          [-1.1064e-01,  8.9216e-02,  1.2855e-01],\n",
            "          [ 1.1745e-01, -2.9812e-02,  1.0277e-01]],\n",
            "\n",
            "         [[-4.4116e-02, -1.1526e-01,  4.9970e-02],\n",
            "          [ 2.6427e-02, -3.6617e-02,  4.2337e-02],\n",
            "          [-7.6417e-02, -8.8004e-02,  8.9344e-02]],\n",
            "\n",
            "         [[-1.1428e-01, -8.2196e-02, -8.5401e-02],\n",
            "          [-1.8553e-02,  6.3036e-02, -7.3131e-03],\n",
            "          [ 1.1709e-01,  4.5833e-02, -1.1086e-01]],\n",
            "\n",
            "         [[ 8.0419e-02, -6.7539e-02, -1.3205e-01],\n",
            "          [-3.9044e-02, -1.0804e-01, -1.2932e-01],\n",
            "          [ 1.2493e-01, -9.8468e-02, -3.9713e-02]],\n",
            "\n",
            "         [[ 1.3271e-01,  1.3097e-01,  1.0671e-01],\n",
            "          [ 6.3265e-02, -7.6522e-02,  2.8801e-02],\n",
            "          [-5.4127e-02,  2.3118e-02,  2.0196e-02]],\n",
            "\n",
            "         [[-7.0943e-02,  5.7270e-02,  2.4057e-02],\n",
            "          [-1.3409e-01,  9.5171e-02,  9.3025e-02],\n",
            "          [ 1.0787e-01, -7.2582e-02, -1.1615e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9674e-02, -1.6691e-02, -1.3123e-01],\n",
            "          [-1.1659e-01,  1.0141e-01, -9.3298e-02],\n",
            "          [ 6.0742e-02, -2.6595e-02, -7.1723e-02]],\n",
            "\n",
            "         [[-7.8432e-02, -4.4640e-03, -9.1915e-02],\n",
            "          [ 8.7307e-02,  4.0397e-02,  5.7871e-02],\n",
            "          [ 6.4067e-02,  7.1297e-02, -1.2216e-01]],\n",
            "\n",
            "         [[-7.0615e-02, -1.3577e-01,  7.5631e-03],\n",
            "          [-4.6242e-02, -9.5203e-02, -5.6332e-03],\n",
            "          [-2.7512e-02,  6.4838e-02, -2.5890e-02]],\n",
            "\n",
            "         [[ 9.9272e-02, -1.1129e-01,  1.1473e-02],\n",
            "          [-8.0150e-02,  5.2302e-02, -5.5825e-02],\n",
            "          [-6.2388e-02,  7.4775e-02, -1.3505e-01]],\n",
            "\n",
            "         [[ 1.0392e-01,  6.7980e-02, -1.1753e-01],\n",
            "          [ 1.0817e-01, -1.0855e-01, -7.0493e-02],\n",
            "          [ 7.3776e-03,  4.2529e-02,  5.6786e-02]],\n",
            "\n",
            "         [[ 2.5962e-02,  1.1646e-01, -5.6105e-02],\n",
            "          [ 4.5814e-03,  1.1614e-01, -2.6382e-02],\n",
            "          [-2.8491e-02, -7.8220e-03,  7.0567e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7947e-02,  9.4970e-02,  5.9480e-02],\n",
            "          [-5.4093e-02,  5.8983e-02, -1.1303e-01],\n",
            "          [ 1.0876e-01,  3.3799e-02,  7.8592e-02]],\n",
            "\n",
            "         [[-2.6302e-02, -1.1609e-01,  1.9012e-02],\n",
            "          [-2.7684e-02,  1.2616e-01,  9.0259e-02],\n",
            "          [-1.2965e-01, -1.0090e-01,  7.6463e-02]],\n",
            "\n",
            "         [[-2.8927e-02,  1.2786e-01,  9.0790e-02],\n",
            "          [ 1.0717e-01,  9.1709e-02,  8.0674e-02],\n",
            "          [ 3.1145e-02,  9.0039e-02, -8.8910e-02]],\n",
            "\n",
            "         [[ 4.7022e-03,  1.1420e-01, -1.3556e-01],\n",
            "          [ 5.9509e-02, -4.4800e-02,  1.0123e-01],\n",
            "          [-4.7015e-02, -8.8703e-02,  5.8157e-02]],\n",
            "\n",
            "         [[-9.0910e-02,  5.4982e-02,  1.7892e-02],\n",
            "          [ 8.0444e-02, -3.4777e-03,  6.1098e-02],\n",
            "          [-6.2895e-02,  5.0621e-02,  2.7811e-02]],\n",
            "\n",
            "         [[-1.3314e-01,  2.3774e-02,  6.3321e-03],\n",
            "          [-7.7420e-02,  1.6255e-02, -1.0676e-01],\n",
            "          [ 1.3363e-02, -1.1450e-02, -1.1638e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.3578e-01, -5.5548e-02,  1.7684e-02],\n",
            "          [-8.2412e-02, -5.2189e-02, -1.4315e-02],\n",
            "          [ 1.1426e-01, -6.8262e-02,  6.5827e-02]],\n",
            "\n",
            "         [[-3.7238e-02,  7.2921e-02, -7.3853e-02],\n",
            "          [-5.1895e-02,  1.2125e-01,  3.0861e-02],\n",
            "          [ 7.3696e-02,  2.6405e-02, -4.1684e-02]],\n",
            "\n",
            "         [[-7.5773e-02, -5.8666e-02,  5.3635e-02],\n",
            "          [-1.6153e-02,  5.3888e-02, -1.0370e-01],\n",
            "          [-3.3307e-02,  9.5519e-02, -1.3513e-01]],\n",
            "\n",
            "         [[-7.8162e-03,  9.4590e-02, -9.4380e-02],\n",
            "          [-4.5035e-02, -4.8228e-02,  7.1140e-02],\n",
            "          [-1.0703e-01, -1.2423e-02,  1.3348e-01]],\n",
            "\n",
            "         [[-6.1532e-02, -2.5091e-02, -1.1859e-01],\n",
            "          [ 7.9025e-02, -3.2244e-02, -1.2213e-01],\n",
            "          [ 7.1814e-02, -4.7590e-02,  6.4429e-02]],\n",
            "\n",
            "         [[-3.3767e-02,  6.1888e-02, -1.3335e-01],\n",
            "          [-5.6155e-02,  1.4002e-02, -6.6723e-02],\n",
            "          [-5.7339e-03,  1.2215e-01,  1.0564e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.1112e-02, -1.2978e-01, -1.2969e-01],\n",
            "          [-6.9668e-02, -2.7107e-02,  1.0683e-01],\n",
            "          [ 5.5438e-02, -4.1892e-02, -4.1408e-03]],\n",
            "\n",
            "         [[-1.0649e-01,  1.0448e-01,  3.9401e-02],\n",
            "          [ 1.2583e-01,  1.2948e-01,  1.1104e-01],\n",
            "          [-9.0857e-02,  9.0628e-04,  3.7393e-02]],\n",
            "\n",
            "         [[ 4.6576e-02,  1.0053e-01,  1.3429e-01],\n",
            "          [-1.2340e-01,  8.0026e-02,  3.0633e-02],\n",
            "          [ 1.1133e-01,  5.4258e-02,  2.4621e-02]],\n",
            "\n",
            "         [[ 3.0976e-02,  2.0032e-02,  3.1132e-02],\n",
            "          [ 1.0148e-01,  6.2007e-02,  1.1281e-01],\n",
            "          [ 4.9966e-02, -1.3539e-01, -3.6575e-02]],\n",
            "\n",
            "         [[-1.1527e-01, -9.8010e-02,  1.8622e-02],\n",
            "          [-6.3343e-02, -7.7754e-02, -1.0049e-01],\n",
            "          [ 1.0636e-01,  6.9895e-02,  2.5497e-02]],\n",
            "\n",
            "         [[ 1.0716e-01,  1.2870e-01, -3.2422e-02],\n",
            "          [ 2.5080e-02,  1.1339e-02, -9.4211e-03],\n",
            "          [ 1.8016e-02,  8.0671e-02,  2.6247e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.5837e-02,  5.4782e-02, -7.5230e-02],\n",
            "          [-7.8285e-02,  3.2503e-02,  1.1989e-01],\n",
            "          [-1.4378e-02, -1.9729e-02, -1.3931e-02]],\n",
            "\n",
            "         [[-2.0212e-02,  8.3322e-02,  1.5674e-02],\n",
            "          [-3.6835e-02, -8.1281e-02,  8.6502e-02],\n",
            "          [ 4.0543e-02, -1.8936e-02,  1.6380e-04]],\n",
            "\n",
            "         [[-7.2874e-02,  6.6920e-02, -1.0435e-01],\n",
            "          [-4.8562e-02,  1.2241e-01,  4.9236e-02],\n",
            "          [ 4.1335e-02,  1.0260e-01, -7.3028e-02]],\n",
            "\n",
            "         [[ 6.6337e-02,  8.8328e-02, -2.1656e-02],\n",
            "          [ 8.2382e-02,  4.4269e-02, -9.5599e-02],\n",
            "          [-8.7465e-02, -2.0782e-02,  1.0280e-01]],\n",
            "\n",
            "         [[-1.1732e-01,  1.5552e-02,  9.3525e-02],\n",
            "          [-6.7486e-02, -7.8851e-02,  6.2557e-02],\n",
            "          [-3.8351e-02, -1.0318e-01,  1.6665e-03]],\n",
            "\n",
            "         [[ 6.8476e-02,  1.2558e-01,  1.2026e-01],\n",
            "          [-6.6697e-02,  4.1262e-02,  9.3795e-02],\n",
            "          [ 1.3311e-01,  6.7106e-02, -1.3057e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.7711e-02, -5.5315e-04,  9.1702e-02],\n",
            "          [-8.1990e-02,  2.8831e-02, -4.9263e-02],\n",
            "          [ 6.6397e-02,  9.8155e-02, -2.8103e-02]],\n",
            "\n",
            "         [[-7.8688e-04,  1.2942e-01,  2.2469e-02],\n",
            "          [-9.7828e-02, -3.7994e-02, -6.3959e-02],\n",
            "          [ 1.0645e-01,  6.7384e-02,  1.3013e-02]],\n",
            "\n",
            "         [[-8.6618e-02,  3.7133e-02, -8.0544e-02],\n",
            "          [-6.0318e-02,  6.8667e-02, -8.1051e-02],\n",
            "          [-5.1735e-02, -4.8907e-02, -2.2229e-02]],\n",
            "\n",
            "         [[ 5.7812e-02, -5.3474e-02, -1.1892e-01],\n",
            "          [-1.1860e-01,  9.5780e-03, -7.0496e-02],\n",
            "          [ 9.1667e-02,  3.4979e-02, -1.8317e-02]],\n",
            "\n",
            "         [[ 7.5158e-02, -1.2197e-01,  1.2143e-01],\n",
            "          [ 4.4975e-02,  1.1846e-01,  6.3861e-02],\n",
            "          [ 2.8572e-02,  7.2011e-02, -2.7942e-02]],\n",
            "\n",
            "         [[ 4.4462e-03,  4.4568e-02,  2.7876e-02],\n",
            "          [-4.4741e-02,  1.1992e-01,  1.1808e-01],\n",
            "          [ 8.8178e-02,  4.9098e-02,  1.1282e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.3802e-02,  1.1084e-01,  2.8877e-03],\n",
            "          [ 8.0901e-03, -1.9256e-03,  2.3089e-02],\n",
            "          [-4.5932e-02, -1.0073e-01, -1.2018e-01]],\n",
            "\n",
            "         [[-1.4016e-02,  1.2882e-01,  1.0289e-01],\n",
            "          [-1.1449e-01, -4.8752e-02,  8.6479e-02],\n",
            "          [ 5.3007e-02,  1.2594e-01, -8.6410e-02]],\n",
            "\n",
            "         [[-8.7975e-02,  5.4361e-02,  1.9362e-02],\n",
            "          [ 1.2365e-01, -7.8452e-02,  1.3344e-01],\n",
            "          [-1.1371e-01,  2.5255e-02, -1.3197e-01]],\n",
            "\n",
            "         [[ 1.0823e-01, -1.0867e-02, -1.1980e-01],\n",
            "          [ 1.9918e-02,  9.8563e-02, -9.3183e-02],\n",
            "          [-1.8219e-02, -1.3489e-01,  6.7003e-02]],\n",
            "\n",
            "         [[ 5.7257e-02, -1.3552e-01,  2.5272e-02],\n",
            "          [ 2.4398e-02,  7.1954e-02,  1.0770e-01],\n",
            "          [ 3.5221e-02, -9.0970e-02,  3.4545e-02]],\n",
            "\n",
            "         [[-1.1632e-01, -1.3068e-01,  1.0747e-01],\n",
            "          [ 7.2979e-02, -6.1066e-02, -8.7871e-02],\n",
            "          [ 9.0305e-02, -1.1236e-01, -9.1649e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1731e-01,  9.4528e-02,  1.1748e-01],\n",
            "          [ 4.8755e-02, -1.0257e-01, -5.5245e-02],\n",
            "          [-4.7694e-02,  1.2826e-01, -1.2218e-01]],\n",
            "\n",
            "         [[ 7.3237e-02, -1.3214e-01,  5.2684e-02],\n",
            "          [ 9.5231e-02, -8.7604e-02, -1.0351e-01],\n",
            "          [ 4.9795e-02, -3.9516e-02, -1.3037e-02]],\n",
            "\n",
            "         [[-1.1545e-01,  1.0226e-01,  6.2355e-03],\n",
            "          [ 8.1144e-02, -6.5582e-02,  4.9718e-02],\n",
            "          [-2.9611e-02,  1.1247e-01,  1.0279e-02]],\n",
            "\n",
            "         [[-1.1300e-01,  1.1380e-01, -5.3395e-03],\n",
            "          [-1.0208e-02,  9.3756e-02, -9.3211e-02],\n",
            "          [ 1.3531e-01,  4.2990e-02,  8.4798e-02]],\n",
            "\n",
            "         [[ 8.8352e-03,  1.5019e-02,  6.0548e-02],\n",
            "          [ 2.9599e-02,  7.7629e-02,  3.1709e-02],\n",
            "          [-2.5030e-02,  1.1380e-01, -1.6464e-02]],\n",
            "\n",
            "         [[-4.5441e-02,  7.4538e-02, -1.8794e-03],\n",
            "          [ 2.1533e-02,  1.0418e-01, -1.1816e-01],\n",
            "          [-9.3207e-02,  2.7552e-02,  8.8178e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.3465e-02,  1.0074e-01, -2.2176e-02],\n",
            "          [ 7.5155e-03, -3.1343e-02, -6.8749e-02],\n",
            "          [ 1.0293e-01, -2.7378e-02,  6.3103e-02]],\n",
            "\n",
            "         [[-5.0949e-02, -9.5466e-02, -3.6545e-03],\n",
            "          [-6.5250e-02, -6.2277e-03, -2.8938e-02],\n",
            "          [ 7.5359e-02,  1.6339e-02,  7.5096e-02]],\n",
            "\n",
            "         [[-2.3442e-02, -8.8258e-02,  7.3006e-02],\n",
            "          [ 6.4965e-02,  9.6177e-02,  2.8275e-02],\n",
            "          [-5.5289e-02, -4.9880e-03,  9.6119e-02]],\n",
            "\n",
            "         [[-5.6161e-02,  1.0934e-01,  7.1155e-02],\n",
            "          [ 1.1965e-01, -3.1592e-02,  9.4272e-02],\n",
            "          [ 5.1897e-02, -4.3285e-02, -4.9010e-02]],\n",
            "\n",
            "         [[-9.3761e-02, -3.5598e-02,  1.2030e-01],\n",
            "          [-7.5042e-02,  7.1062e-02, -5.8001e-02],\n",
            "          [ 1.3500e-01,  2.0197e-02, -2.1937e-02]],\n",
            "\n",
            "         [[ 8.0484e-02, -3.1860e-02, -3.1207e-02],\n",
            "          [ 1.0033e-01, -6.9281e-02,  8.7501e-02],\n",
            "          [ 5.2199e-02,  5.8129e-02, -2.6750e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.6595e-02, -5.2834e-02, -9.5685e-02],\n",
            "          [-3.1790e-02, -1.2470e-01, -4.2855e-02],\n",
            "          [-1.5290e-02,  5.2479e-03, -6.7991e-02]],\n",
            "\n",
            "         [[-6.4395e-02,  4.5574e-02, -1.8417e-02],\n",
            "          [-1.0545e-01, -7.7017e-02,  6.5024e-02],\n",
            "          [-1.0231e-01, -4.4795e-02, -1.8178e-02]],\n",
            "\n",
            "         [[-7.5679e-02, -3.9856e-02, -5.3972e-03],\n",
            "          [-2.7232e-02, -2.5401e-02, -1.2973e-01],\n",
            "          [ 8.1123e-03,  4.2555e-02,  5.7827e-02]],\n",
            "\n",
            "         [[-9.2113e-02,  1.5767e-02, -4.0861e-02],\n",
            "          [ 5.4029e-02,  1.1734e-01,  3.1063e-02],\n",
            "          [ 8.9048e-02, -9.7641e-02,  1.3433e-01]],\n",
            "\n",
            "         [[-1.1641e-01, -6.6889e-02, -2.7389e-02],\n",
            "          [-1.1249e-01, -1.2364e-01, -7.5145e-02],\n",
            "          [ 7.8284e-02, -9.4041e-02, -4.2110e-02]],\n",
            "\n",
            "         [[-3.6293e-02, -1.0830e-01, -1.0473e-01],\n",
            "          [-1.1119e-01,  5.6285e-04,  1.0157e-01],\n",
            "          [ 4.0057e-02, -1.7502e-02,  1.0853e-01]]]], requires_grad=True)\n",
            "Conv2 Biases:\n",
            "Parameter containing:\n",
            "tensor([-0.0331,  0.0707,  0.1292,  0.0289,  0.0413, -0.0344, -0.0890,  0.0269,\n",
            "        -0.1064, -0.0689, -0.1069,  0.0590], requires_grad=True)\n",
            "===============================================\n",
            "\n",
            "Forward Pass with Batch of Inputs\n",
            "Input Shape (Batch of 5 samples): torch.Size([5, 1, 12, 12])\n",
            "\n",
            "Forward Pass - Input Tensor Shape: torch.Size([5, 1, 12, 12])\n",
            "After Layer 1 (Conv1 -> ReLU -> Pool1) - Tensor Shape: torch.Size([5, 6, 5, 5])\n",
            "After Layer 2 (Conv2 -> ReLU -> Pool2) - Tensor Shape: torch.Size([5, 12, 1, 1])\n",
            "Output Shape after Forward Pass with Batch: torch.Size([5, 12, 1, 1])\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9VUlEQVR4nO3dfXzO9f////vBTo1tzrbxNpuTIYlCmDBEc5KcrHeSaiSlKBG906V3S6V15qze5J2vnBQ5KSp662xOYjGnSUSIKNvI2TZsZnv+/vDb8emwE9vhmGMv3a6Xy3Gp4/l6Hs/X4/XcZve9jufrddiMMUYAAAAWVM7dBQAAADiLIAMAACyLIAMAACyLIAMAACyLIAMAACyLIAMAACyLIAMAACyLIAMAACyLIAMAACyLIIPr2qBBgxQeHu70aytWrOjagpw0Z84c2Ww2HTp0qNT3dfmcHTp0SDabTW+99Vap71uSXnzxRdlstmuyr8vZbDa9+OKLbtk3AOcQZHDNLV68WDabTcuWLcu3rVmzZrLZbFq9enW+bbVr11bbtm2vRYklcu7cOb344otas2ZNsfqvWbNGNpvN/vD29lZwcLA6duyoV199VcePH3dLXddSWa6tNKxZs0b9+vVTSEiIvLy8FBQUpF69emnp0qXuLq1QeeF5y5Yt7i7lquXm5mrOnDm66667FBoaKj8/PzVp0kSvvPKKMjMz3V0erhJBBtdcu3btJEnr1693aE9LS9NPP/0kDw8PJSYmOmw7cuSIjhw5Yn9tcc2cOVN79+69uoKv4Ny5cxo/fnyJfyk/+eST+uCDD/Tee+9p7NixqlKliuLi4nTDDTdo1apVDn0feOABnT9/XmFhYaVel7vn7Pnnn9f58+dLdf+FOX/+vJ5//nmXjhkXF6dOnTrpp59+0qOPPqoZM2Zo7NixysjIUExMjBYsWODS/SG/c+fOafDgwTp+/LiGDRumKVOmqFWrVoqLi1P37t3FRw5am4e7C8DfT82aNVWnTp18QWbDhg0yxuif//xnvm15z0saZDw9Pa+u2FLUvn173X333Q5tO3bs0B133KGYmBjt3r1bNWrUkCSVL19e5cuXL9V6zp49Kz8/P7fPmYeHhzw83PNPk4+Pj0vH+/jjj/XSSy/p7rvv1oIFCxzmduzYsfrqq6+UnZ3t0n3+XeXm5urChQsFfg29vLyUmJjocEZ36NChCg8PV1xcnBISEtSlS5drWS5ciDMycIt27dpp+/btDn95JyYm6sYbb1T37t21ceNG5ebmOmyz2Wy67bbb7G0ffvihWrRoIV9fX1WpUkX33nuvjhw54rCfgtbInDhxQg888ID8/f0VGBio2NhY7dixQzabTXPmzMlX6x9//KE+ffqoYsWKql69usaMGaOcnBxJl9aPVK9eXZI0fvx4+9tFzq6zaNasmaZMmaLTp0/rP//5j729oDUyW7ZsUXR0tKpVqyZfX1/VqVNHDz30ULHqylv/c+DAAfXo0UOVKlXSwIEDC52zPJMnT1ZYWJh8fX0VFRWln376yWF7x44d1bFjx3yv++uYV6qtoDUyFy9e1Msvv6x69erJ29tb4eHheu6555SVleXQLzw8XHfeeafWr1+vVq1aycfHR3Xr1tW8efMKnvDLXP61y6tl//79GjRokAIDAxUQEKDBgwfr3LlzVxzv3//+t6pUqaL333+/wIAYHR2tO++80/782LFjGjJkiIKDg+Xj46NmzZpp7ty5Dq/565ql9957zz4nt956qzZv3mzv99Zbb8lms+m3337Lt99x48bJy8tLp06dKs60FOrChQt64YUX1KJFCwUEBMjPz0/t27d3eGvYGKPw8HD17t073+szMzMVEBCgRx991N6WlZWluLg41a9fX97e3goNDdUzzzyT72tts9k0YsQIzZ8/XzfeeKO8vb315ZdfFlinl5dXgW9L9+3bV5L0888/O3X8KBsIMnCLdu3aKTs7W0lJSfa2vL+Y2rZtqzNnzjj8kkxMTFSjRo1UtWpVSdKECRP04IMPKiIiQpMmTdJTTz2lhIQEdejQQadPny50v7m5uerVq5c++ugjxcbGasKECUpOTlZsbGyB/XNychQdHa2qVavqrbfeUlRUlCZOnKj33ntPklS9enW9++67ki79o/jBBx/ogw8+UL9+/Zyem7vvvlu+vr76+uuvC+1z7Ngx3XHHHTp06JCeffZZvfPOOxo4cKA2btxY7LouXryo6OhoBQUF6a233lJMTEyRdc2bN09vv/22hg8frnHjxumnn35S586dlZqaWqLjc2bOHn74Yb3wwgtq3ry5Jk+erKioKMXHx+vee+/N13f//v26++671bVrV02cOFGVK1fWoEGDtGvXrhLV+Vf33HOP0tPTFR8fr3vuuUdz5szR+PHji3zNvn37tGfPHvXp00eVKlW64j7Onz+vjh076oMPPtDAgQP15ptvKiAgQIMGDdLUqVPz9V+wYIHefPNNPfroo3rllVd06NAh9evXz36G55577pHNZtPixYvzvXbx4sW64447VLly5WLOQMHS0tL0//7f/1PHjh31+uuv68UXX9Tx48cVHR2tH374QdKlwHH//fdr5cqVOnnypMPrly9frrS0NN1///2SLv183nXXXXrrrbfUq1cvvfPOO+rTp48mT56s/v3759v/qlWrNGrUKPXv319Tp04t8cL+lJQUSVK1atVKfvAoOwzgBrt27TKSzMsvv2yMMSY7O9v4+fmZuXPnGmOMCQ4ONtOmTTPGGJOWlmbKly9vhg4daowx5tChQ6Z8+fJmwoQJDmPu3LnTeHh4OLTHxsaasLAw+/NPPvnESDJTpkyxt+Xk5JjOnTsbSWb27NkOr5VkXnrpJYf93HLLLaZFixb258ePHzeSTFxcXLGOffXq1UaSWbJkSaF9mjVrZipXrmx/Pnv2bCPJHDx40BhjzLJly4wks3nz5kLHKKquvGN79tlnC9z21zk7ePCgkWR8fX3N77//bm9PSkoyksyoUaPsbVFRUSYqKuqKYxZVW1xcnPnrP00//PCDkWQefvhhh35jxowxksyqVavsbWFhYUaS+e677+xtx44dM97e3ubpp5/Ot6/LXV5TXi0PPfSQQ7++ffuaqlWrFjnWZ599ZiSZyZMnX3G/xhgzZcoUI8l8+OGH9rYLFy6YyMhIU7FiRZOWlmaM+b+vR9WqVc3Jkyfz7W/58uX2tsjISIfvVWOM2bRpk5Fk5s2bV2Q9ed9zRX2PXbx40WRlZTm0nTp1ygQHBzvM2d69e40k8+677zr0veuuu0x4eLjJzc01xhjzwQcfmHLlypl169Y59JsxY4aRZBITE+1tkky5cuXMrl27ijyOonTp0sX4+/ubU6dOOT0G3I8zMnCLG264QVWrVrWvfdmxY4fOnj1rP/3btm1b+4LfDRs2KCcnx74+ZunSpcrNzdU999yjP//80/4ICQlRREREgVc85fnyyy/l6empoUOH2tvKlSun4cOHF/qaYcOGOTxv3769fv31V+cOvJgqVqyo9PT0QrcHBgZKklasWHFVaywee+yxYvft06eP/vGPf9ift2rVSq1bt9b//vc/p/dfHHnjjx492qH96aefliR98cUXDu2NGzdW+/bt7c+rV6+uhg0bXtXXrKDvgRMnTigtLa3Q1+RtK87ZGOnScYaEhGjAgAH2Nk9PTz355JPKyMjQ2rVrHfr379/f4YxK3jH/9Tj79++vrVu36sCBA/a2RYsWydvbu8C3ekqqfPny8vLyknTpbMrJkyd18eJFtWzZUtu2bbP3a9CggVq3bq358+fb206ePKmVK1dq4MCB9rcSlyxZohtuuEGNGjVy+Nnu3LmzJOX72Y6KilLjxo2dqv3VV1/Vt99+q9dee83+8wRrIsjALWw2m9q2bWtfC5OYmKigoCDVr19fkmOQyftvXpDZt2+fjDGKiIhQ9erVHR4///yzjh07Vuh+f/vtN9WoUUMVKlRwaM/b7+V8fHzs6znyVK5c+arXFlxJRkZGkb8Ao6KiFBMTo/Hjx6tatWrq3bu3Zs+enW8dQVE8PDxUq1atYvePiIjI19agQYNSv7fNb7/9pnLlyuX7GoWEhCgwMDDfGpDatWvnG+Nqv2aXj5kXIIoa09/fX5KKDKR/9dtvvykiIkLlyjn+s3zDDTfYt5e0pn/+858qV66cFi1aJOnSepUlS5aoe/fu9vqu1ty5c9W0aVP5+PioatWqql69ur744gudOXPGod+DDz6oxMRE+3EsWbJE2dnZeuCBB+x99u3bp127duX7uW7QoIEk5fvZrlOnjlM1L1q0SM8//7yGDBlSojCPsomrluA27dq10/Lly7Vz5858VxS0bdtWY8eO1R9//KH169erZs2aqlu3rqRLf/nZbDatXLmywCt5XHkTu9K+Uqgg2dnZ+uWXX9SkSZNC+9hsNn388cfauHGjli9frq+++koPPfSQJk6cqI0bNxZrDry9vfP90rxaNputwEtZ8xZHX+3YxVHY16yguorLmTEbNWokSdq5c6fT+73ammrWrKn27dtr8eLFeu6557Rx40YdPnxYr7/+uktq+PDDDzVo0CD16dNHY8eOVVBQkMqXL6/4+HiHs0CSdO+992rUqFGaP3++nnvuOX344Ydq2bKlGjZsaO+Tm5urm266SZMmTSpwf6GhoQ7PfX19S1zzN998owcffFA9e/bUjBkzSvx6lD0EGbjNX+8nk5iYqKeeesq+rUWLFvL29taaNWuUlJSkHj162LfVq1dPxhjVqVPH/pdacYWFhWn16tU6d+6cw1mZ/fv3O30crr4L7ccff6zz588rOjr6in3btGmjNm3aaMKECVqwYIEGDhyohQsX6uGHH3Z5Xfv27cvX9ssvvzgssKxcuXKBb+FcfjahJLWFhYUpNzdX+/bts5+dkKTU1FSdPn26RPfWuZYaNGighg0b6rPPPtPUqVOvGC7DwsL0448/Kjc31yFg7tmzx77dGf3799fjjz+uvXv3atGiRapQoYJ69erl1FiX+/jjj1W3bl0tXbrU4WsaFxeXr2+VKlXUs2dPzZ8/XwMHDlRiYqKmTJni0KdevXrasWOHbr/99lK5u3NSUpL69u2rli1bavHixW67zB+uxVtLcJuWLVvKx8dH8+fP1x9//OFwRsbb21vNmzfXtGnTdPbsWYf7x/Tr10/ly5fX+PHj8/1FbIzRiRMnCt1ndHS0srOzNXPmTHtbbm6upk2b5vRx5AWioq6WKq4dO3boqaeeUuXKlYtct3Pq1Kl8x37zzTdLkv3tJVfWJUmffvqp/vjjD/vzTZs2KSkpSd27d7e31atXT3v27HG4O/GOHTvy3eCwJLXlhdjLf+nl/dXes2fPEh3HtTR+/HidOHFCDz/8sC5evJhv+9dff60VK1ZIunScKSkp9reBpEtXlr3zzjuqWLGioqKinKohJiZG5cuX10cffaQlS5bozjvvlJ+fn3MHdJm8s0J//V5MSkrShg0bCuz/wAMPaPfu3Ro7dqzKly+f76qze+65R3/88YfDz2ee8+fP6+zZs07X+vPPP6tnz54KDw/XihUrnDqbg7KJOAq38fLy0q233qp169bJ29tbLVq0cNjetm1bTZw4UZLjjfDq1aunV155RePGjdOhQ4fsl7cePHhQy5Yt0yOPPKIxY8YUuM8+ffqoVatWevrpp7V//341atRIn3/+uf2yUGf+CvT19VXjxo21aNEiNWjQQFWqVFGTJk2KfGtIktatW6fMzEzl5OToxIkTSkxM1Oeff66AgAAtW7ZMISEhhb527ty5mj59uvr27at69eopPT1dM2fOlL+/v/0Xv7N1FaZ+/fpq166dHnvsMWVlZWnKlCmqWrWqnnnmGXufhx56SJMmTVJ0dLSGDBmiY8eOacaMGbrxxhsdFsaWpLZmzZopNjZW7733nk6fPq2oqCht2rRJc+fOVZ8+fdSpUyenjuda6N+/v3bu3KkJEyZo+/btGjBggMLCwnTixAl9+eWXSkhIsN/Z95FHHtF///tfDRo0SFu3blV4eLg+/vhj+5mL4i4avlxQUJA6deqkSZMmKT09vcDLmIvy/vvvF3h/lpEjR+rOO+/U0qVL1bdvX/Xs2VMHDx7UjBkz1LhxY2VkZOR7Tc+ePVW1alX7Op2goCCH7Q888IAWL16sYcOGafXq1brtttuUk5OjPXv2aPHixfrqq6/UsmXLkk2ALq1Tio6O1qlTpzR27Nh8C8Tr1aunyMjIEo+LMsJNV0sBxhhjxo0bZySZtm3b5tu2dOlSI8lUqlTJXLx4Md/2Tz75xLRr1874+fkZPz8/06hRIzN8+HCzd+9ee5/LL/s15tKlv/fdd5+pVKmSCQgIMIMGDTKJiYlGklm4cKHDa/38/PLt9/LLg40x5vvvvzctWrQwXl5eV7wUO+/y67yHp6enqV69uunQoYOZMGGCOXbsWL7XXH759bZt28yAAQNM7dq1jbe3twkKCjJ33nmn2bJlS7HqKuzYCpqzvMt933zzTTNx4kQTGhpqvL29Tfv27c2OHTvyvf7DDz80devWNV5eXubmm282X331VYFfh8JqK2h+s7Ozzfjx402dOnWMp6enCQ0NNePGjTOZmZkO/cLCwkzPnj3z1VTYZeGXu/xrl1fL8ePHHfpd/vW4koSEBNO7d28TFBRkPDw8TPXq1U2vXr3MZ5995tAvNTXVDB482FSrVs14eXmZm266yeGWAMY4fj2uVH+emTNn2n+Wzp8/X6ya846xsMeRI0dMbm6uefXVV01YWJjx9vY2t9xyi1mxYkWBX+88jz/+uJFkFixYUOD2CxcumNdff93ceOONxtvb21SuXNm0aNHCjB8/3pw5c8bhWIcPH16sY8mbs8IesbGxxRoHZZPNGD5kAvj000/Vt29frV+/3uHuwQBca9SoUZo1a5ZSUlLyXT0IOIM1MvjbufwDCXNycvTOO+/I399fzZs3d1NVwPUvMzNTH374oWJiYggxcBnWyOBv54knntD58+cVGRmprKwsLV26VN9//71effVVFgACpeDYsWP69ttv9fHHH+vEiRMaOXKku0vCdYQgg7+dzp07a+LEiVqxYoUyMzNVv359vfPOOxoxYoS7SwOuS7t379bAgQMVFBSkt99+236FHeAKrJEBAACWxRoZAABgWQQZAABgWdf9Gpnc3FwdPXpUlSpVKpVbXgMAANczxig9PV01a9Ys8nPhrvsgc/To0XwfNAYAAKzhyJEjqlWrVqHbr/sgk3db7yNHjrjsY+sBAEDpSktLU2ho6BU/nuO6DzJ5byf5+/sTZAAAsJgrLQthsS8AALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsD3cXAAAo+4bM2VxqY88adGupjY3rH2dkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZZWZIPPaa6/JZrPpqaeesrdlZmZq+PDhqlq1qipWrKiYmBilpqa6r0gAAFCmlIkgs3nzZv33v/9V06ZNHdpHjRql5cuXa8mSJVq7dq2OHj2qfv36ualKAABQ1rg9yGRkZGjgwIGaOXOmKleubG8/c+aMZs2apUmTJqlz585q0aKFZs+ere+//14bN250Y8UAAKCscHuQGT58uHr27KkuXbo4tG/dulXZ2dkO7Y0aNVLt2rW1YcOGQsfLyspSWlqawwMAAFyfPNy584ULF2rbtm3avHlzvm0pKSny8vJSYGCgQ3twcLBSUlIKHTM+Pl7jx493dakAAKAMctsZmSNHjmjkyJGaP3++fHx8XDbuuHHjdObMGfvjyJEjLhsbAACULW4LMlu3btWxY8fUvHlzeXh4yMPDQ2vXrtXbb78tDw8PBQcH68KFCzp9+rTD61JTUxUSElLouN7e3vL393d4AACA65Pb3lq6/fbbtXPnToe2wYMHq1GjRvrXv/6l0NBQeXp6KiEhQTExMZKkvXv36vDhw4qMjHRHyQAAoIxxW5CpVKmSmjRp4tDm5+enqlWr2tuHDBmi0aNHq0qVKvL399cTTzyhyMhItWnTxh0lAwCAMsati32vZPLkySpXrpxiYmKUlZWl6OhoTZ8+3d1lAQCAMsJmjDHuLqI0paWlKSAgQGfOnGG9DAA4acic/FeXusqsQbeW2tiwruL+/nb7fWQAAACcVabfWgKQH38ZA8D/4YwMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLC6/BgCghErrNgjcAqHkOCMDAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsy8PdBQDFMWTO5lIZd9agW0tlXKn0agYA/B/OyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMviPjJlVGneg6S07p3CfVNwPbHiz6BVWfE+USg7OCMDAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsi/vIALDjfh4ArIYzMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLK4jwwAAGVEad3LSbp+7+fEGRkAAGBZBBkAAGBZBBkAAGBZBBkAAGBZBBkAAGBZBBkAAGBZXH4NANeR0rx8FyiLOCMDAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsiyADAAAsi/vIAACuS9xT5++BMzIAAMCyCDIAAMCyCDIAAMCy3Bpk3n33XTVt2lT+/v7y9/dXZGSkVq5cad+emZmp4cOHq2rVqqpYsaJiYmKUmprqxooBAEBZ4tYgU6tWLb322mvaunWrtmzZos6dO6t3797atWuXJGnUqFFavny5lixZorVr1+ro0aPq16+fO0sGAABliFuvWurVq5fD8wkTJujdd9/Vxo0bVatWLc2aNUsLFixQ586dJUmzZ8/WDTfcoI0bN6pNmzbuKBkAAJQhZWaNTE5OjhYuXKizZ88qMjJSW7duVXZ2trp06WLv06hRI9WuXVsbNmwodJysrCylpaU5PAAAwPXJ7UFm586dqlixory9vTVs2DAtW7ZMjRs3VkpKiry8vBQYGOjQPzg4WCkpKYWOFx8fr4CAAPsjNDS0lI8AAAC4i9uDTMOGDfXDDz8oKSlJjz32mGJjY7V7926nxxs3bpzOnDljfxw5csSF1QIAgLLE7Xf29fLyUv369SVJLVq00ObNmzV16lT1799fFy5c0OnTpx3OyqSmpiokJKTQ8by9veXt7V3aZQMAgDLA7WdkLpebm6usrCy1aNFCnp6eSkhIsG/bu3evDh8+rMjISDdWCAAAygq3npEZN26cunfvrtq1ays9PV0LFizQmjVr9NVXXykgIEBDhgzR6NGjVaVKFfn7++uJJ55QZGQkVywBAABJbg4yx44d04MPPqjk5GQFBASoadOm+uqrr9S1a1dJ0uTJk1WuXDnFxMQoKytL0dHRmj59ujtLBgAAZYhbg8ysWbOK3O7j46Np06Zp2rRp16giAABgJWVujQwAAEBxEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBluf3Tr3HtDZmz2d0llBnMBYC/i9L6927WoFtLZdzicuqMzK+//urqOgAAAErMqSBTv359derUSR9++KEyMzNdXRMAAECxOBVktm3bpqZNm2r06NEKCQnRo48+qk2bNrm6NgAAgCI5FWRuvvlmTZ06VUePHtX777+v5ORktWvXTk2aNNGkSZN0/PhxV9cJAACQz1VdteTh4aF+/fppyZIlev3117V//36NGTNGoaGhevDBB5WcnOyqOgEAAPK5qiCzZcsWPf7446pRo4YmTZqkMWPG6MCBA/rmm2909OhR9e7d21V1AgAA5OPU5deTJk3S7NmztXfvXvXo0UPz5s1Tjx49VK7cpVxUp04dzZkzR+Hh4a6sFQAAwIFTQebdd9/VQw89pEGDBqlGjRoF9gkKCtKsWbOuqjgAAICiOBVk9u3bd8U+Xl5eio2NdWZ4AACAYnFqjczs2bO1ZMmSfO1LlizR3Llzr7ooAACA4nAqyMTHx6tatWr52oOCgvTqq69edVEAAADF4VSQOXz4sOrUqZOvPSwsTIcPH77qogAAAIrDqSATFBSkH3/8MV/7jh07VLVq1asuCgAAoDicCjIDBgzQk08+qdWrVysnJ0c5OTlatWqVRo4cqXvvvdfVNQIAABTIqauWXn75ZR06dEi33367PDwuDZGbm6sHH3yQNTIAAOCacSrIeHl5adGiRXr55Ze1Y8cO+fr66qabblJYWJir6wMAACiUU0EmT4MGDdSgQQNX1QIAAFAiTgWZnJwczZkzRwkJCTp27Jhyc3Mdtq9atcolxQEAABTFqSAzcuRIzZkzRz179lSTJk1ks9lcXRcAAMAVORVkFi5cqMWLF6tHjx6urgcAAKDYnLr82svLS/Xr13d1LQAAACXiVJB5+umnNXXqVBljXF0PAABAsTn11tL69eu1evVqrVy5UjfeeKM8PT0dti9dutQlxZV1Q+ZsdncJgCXws+KI+QBcx6kgExgYqL59+7q6FgAAgBJxKsjMnj3b1XUAAACUmFNrZCTp4sWL+vbbb/Xf//5X6enpkqSjR48qIyPDZcUBAAAUxakzMr/99pu6deumw4cPKysrS127dlWlSpX0+uuvKysrSzNmzHB1nQAAAPk4dUZm5MiRatmypU6dOiVfX197e9++fZWQkOCy4gAAAIri1BmZdevW6fvvv5eXl5dDe3h4uP744w+XFAYAAHAlTp2Ryc3NVU5OTr7233//XZUqVbrqogAAAIrDqSBzxx13aMqUKfbnNptNGRkZiouL42MLAADANePUW0sTJ05UdHS0GjdurMzMTN13333at2+fqlWrpo8++sjVNQIAABTIqSBTq1Yt7dixQwsXLtSPP/6ojIwMDRkyRAMHDnRY/AsAAFCanAoykuTh4aH777/flbUAAACUiFNBZt68eUVuf/DBB50qBgAAoCScCjIjR450eJ6dna1z587Jy8tLFSpUIMgAAIBrwqmrlk6dOuXwyMjI0N69e9WuXTsW+wIAgGvG6c9aulxERIRee+21fGdrAAAASovLgox0aQHw0aNHXTkkAABAoZxaI/P55587PDfGKDk5Wf/5z3902223uaQwAACAK3EqyPTp08fhuc1mU/Xq1dW5c2dNnDjRFXUBAABckVNBJjc319V1AAAAlJhL18gAAABcS06dkRk9enSx+06aNMmZXQAAAFyRU0Fm+/bt2r59u7Kzs9WwYUNJ0i+//KLy5curefPm9n42m801VQIAABTAqSDTq1cvVapUSXPnzlXlypUlXbpJ3uDBg9W+fXs9/fTTLi0SAACgIE6tkZk4caLi4+PtIUaSKleurFdeeYWrlgAAwDXjVJBJS0vT8ePH87UfP35c6enpV10UAABAcTgVZPr27avBgwdr6dKl+v333/X777/rk08+0ZAhQ9SvXz9X1wgAAFAgp9bIzJgxQ2PGjNF9992n7OzsSwN5eGjIkCF68803XVogAABAYZwKMhUqVND06dP15ptv6sCBA5KkevXqyc/Pz6XFAQAAFOWqboiXnJys5ORkRUREyM/PT8YYV9UFAABwRU4FmRMnTuj2229XgwYN1KNHDyUnJ0uShgwZwqXXAADgmnEqyIwaNUqenp46fPiwKlSoYG/v37+/vvzyS5cVBwAAUBSn1sh8/fXX+uqrr1SrVi2H9oiICP32228uKQwAAOBKnDojc/bsWYczMXlOnjwpb2/vqy4KAACgOJwKMu3bt9e8efPsz202m3Jzc/XGG2+oU6dOLisOAACgKE4FmTfeeEPvvfeeunfvrgsXLuiZZ55RkyZN9N133+n1118v9jjx8fG69dZbValSJQUFBalPnz7au3evQ5/MzEwNHz5cVatWVcWKFRUTE6PU1FRnygYAANcZp4JMkyZN9Msvv6hdu3bq3bu3zp49q379+mn79u2qV69escdZu3athg8fro0bN+qbb75Rdna27rjjDp09e9beZ9SoUVq+fLmWLFmitWvX6ujRo9w9GAAASJJspoQ3f8nOzla3bt00Y8YMRUREuLSY48ePKygoSGvXrlWHDh105swZVa9eXQsWLNDdd98tSdqzZ49uuOEGbdiwQW3atLnimGlpaQoICNCZM2fk7+/v0nqHzNns0vEAALCaWYNuLZVxi/v7u8RnZDw9PfXjjz9eVXGFOXPmjCSpSpUqkqStW7cqOztbXbp0sfdp1KiRateurQ0bNhQ4RlZWltLS0hweAADg+uTUW0v333+/Zs2a5dJCcnNz9dRTT+m2225TkyZNJEkpKSny8vJSYGCgQ9/g4GClpKQUOE58fLwCAgLsj9DQUJfWCQAAyg6n7iNz8eJFvf/++/r222/VokWLfJ+xNGnSpBKPOXz4cP30009av369MyXZjRs3TqNHj7Y/T0tLI8wAAHCdKlGQ+fXXXxUeHq6ffvpJzZs3lyT98ssvDn1sNluJixgxYoRWrFih7777zuEmeyEhIbpw4YJOnz7tcFYmNTVVISEhBY7l7e3NvWwAAPibKFGQiYiIUHJyslavXi3p0kcSvP322woODnZq58YYPfHEE1q2bJnWrFmjOnXqOGxv0aKFPD09lZCQoJiYGEnS3r17dfjwYUVGRjq1TwAAcP0oUZC5/AKnlStXOlwqXVLDhw/XggUL9Nlnn6lSpUr2dS8BAQHy9fVVQECAhgwZotGjR6tKlSry9/fXE088ocjIyGJdsQQAAK5vTq2RyVPCK7fzeffddyVJHTt2dGifPXu2Bg0aJEmaPHmyypUrp5iYGGVlZSk6OlrTp0+/qv0CAIDrQ4mCjM1my7cGxpk1MXmKE4R8fHw0bdo0TZs2zen9AACA61OJ31oaNGiQfTFtZmamhg0blu+qpaVLl7quQgAAgEKUKMjExsY6PL///vtdWgwAAEBJlCjIzJ49u7TqAAAAKDGn7uwLAABQFhBkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZRFkAACAZbk1yHz33Xfq1auXatasKZvNpk8//dRhuzFGL7zwgmrUqCFfX1916dJF+/btc0+xAACgzHFrkDl79qyaNWumadOmFbj9jTfe0Ntvv60ZM2YoKSlJfn5+io6OVmZm5jWuFAAAlEUe7tx59+7d1b179wK3GWM0ZcoUPf/88+rdu7ckad68eQoODtann36qe++991qWCgAAyqAyu0bm4MGDSklJUZcuXextAQEBat26tTZs2ODGygAAQFnh1jMyRUlJSZEkBQcHO7QHBwfbtxUkKytLWVlZ9udpaWmlUyAAAHC7MntGxlnx8fEKCAiwP0JDQ91dEgAAKCVlNsiEhIRIklJTUx3aU1NT7dsKMm7cOJ05c8b+OHLkSKnWCQAA3KfMBpk6deooJCRECQkJ9ra0tDQlJSUpMjKy0Nd5e3vL39/f4QEAAK5Pbl0jk5GRof3799ufHzx4UD/88IOqVKmi2rVr66mnntIrr7yiiIgI1alTR//+979Vs2ZN9enTx31FAwCAMsOtQWbLli3q1KmT/fno0aMlSbGxsZozZ46eeeYZnT17Vo888ohOnz6tdu3a6csvv5SPj4+7SgYAAGWIzRhj3F1EaUpLS1NAQIDOnDnj8reZhszZ7NLxAACwmlmDbi2VcYv7+7vMrpEBAAC4EoIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLIIMAACwLEsEmWnTpik8PFw+Pj5q3bq1Nm3a5O6SAABAGVDmg8yiRYs0evRoxcXFadu2bWrWrJmio6N17Ngxd5cGAADcrMwHmUmTJmno0KEaPHiwGjdurBkzZqhChQp6//333V0aAABwszIdZC5cuKCtW7eqS5cu9rZy5cqpS5cu2rBhgxsrAwAAZYGHuwsoyp9//qmcnBwFBwc7tAcHB2vPnj0FviYrK0tZWVn252fOnJEkpaWluby+C+czXD4mAABWUhq/X/86rjGmyH5lOsg4Iz4+XuPHj8/XHhoa6oZqAAC4vn34eOmOn56eroCAgEK3l+kgU61aNZUvX16pqakO7ampqQoJCSnwNePGjdPo0aPtz3Nzc3Xy5ElVrVpVNputVOu9nqWlpSk0NFRHjhyRv7+/u8uxNObSdZhL12EuXYv5vHrGGKWnp6tmzZpF9ivTQcbLy0stWrRQQkKC+vTpI+lSMElISNCIESMKfI23t7e8vb0d2gIDA0u50r8Pf39/fihdhLl0HebSdZhL12I+r05RZ2LylOkgI0mjR49WbGysWrZsqVatWmnKlCk6e/asBg8e7O7SAACAm5X5INO/f38dP35cL7zwglJSUnTzzTfryy+/zLcAGAAA/P2U+SAjSSNGjCj0rSRcG97e3oqLi8v3th1Kjrl0HebSdZhL12I+rx2budJ1TQAAAGVUmb4hHgAAQFEIMgAAwLIIMgAAwLIIMgAAwLIIMpAknTx5UgMHDpS/v78CAwM1ZMgQZWQU/VlS7733njp27Ch/f3/ZbDadPn3aJeNanTPHnJmZqeHDh6tq1aqqWLGiYmJi8t3R2maz5XssXLiwNA/FLaZNm6bw8HD5+PiodevW2rRpU5H9lyxZokaNGsnHx0c33XST/ve//zlsN8bohRdeUI0aNeTr66suXbpo3759pXkIZYar53LQoEH5vge7detWmodQZpRkLnft2qWYmBiFh4fLZrNpypQpVz0mimAAY0y3bt1Ms2bNzMaNG826detM/fr1zYABA4p8zeTJk018fLyJj483ksypU6dcMq7VOXPMw4YNM6GhoSYhIcFs2bLFtGnTxrRt29ahjyQze/Zsk5ycbH+cP3++NA/lmlu4cKHx8vIy77//vtm1a5cZOnSoCQwMNKmpqQX2T0xMNOXLlzdvvPGG2b17t3n++eeNp6en2blzp73Pa6+9ZgICAsynn35qduzYYe666y5Tp06d627uLlcacxkbG2u6devm8D148uTJa3VIblPSudy0aZMZM2aM+eijj0xISIiZPHnyVY+JwhFkYHbv3m0kmc2bN9vbVq5caWw2m/njjz+u+PrVq1cXGGSudlwrcuaYT58+bTw9Pc2SJUvsbT///LORZDZs2GBvk2SWLVtWarWXBa1atTLDhw+3P8/JyTE1a9Y08fHxBfa/5557TM+ePR3aWrdubR599FFjjDG5ubkmJCTEvPnmm/btp0+fNt7e3uajjz4qhSMoO1w9l8ZcCjK9e/culXrLspLO5V+FhYUVGGSuZkw44q0laMOGDQoMDFTLli3tbV26dFG5cuWUlJRU5sYty5w55q1btyo7O1tdunSxtzVq1Ei1a9fWhg0bHPoOHz5c1apVU6tWrfT+++9f8ePtreTChQvaunWrwzyUK1dOXbp0yTcPeTZs2ODQX5Kio6Pt/Q8ePKiUlBSHPgEBAWrdunWhY14PSmMu86xZs0ZBQUFq2LChHnvsMZ04ccL1B1CGODOX7hjz78wSd/ZF6UpJSVFQUJBDm4eHh6pUqaKUlJQyN25Z5swxp6SkyMvLK9+HmwYHBzu85qWXXlLnzp1VoUIFff3113r88ceVkZGhJ5980uXH4Q5//vmncnJy8n38SHBwsPbs2VPga1JSUgrsnzdvef8tqs/1qDTmUpK6deumfv36qU6dOjpw4ICee+45de/eXRs2bFD58uVdfyBlgDNz6Y4x/84IMtexZ599Vq+//nqRfX7++edrVI21lYW5/Pe//23//1tuuUVnz57Vm2++ed0EGZR99957r/3/b7rpJjVt2lT16tXTmjVrdPvtt7uxMvydEWSuY08//bQGDRpUZJ+6desqJCREx44dc2i/ePGiTp48qZCQEKf3X1rjukNpzmVISIguXLig06dPO5yVSU1NLXKeWrdurZdffllZWVnXxee5VKtWTeXLl893tVZR8xASElJk/7z/pqamqkaNGg59br75ZhdWX7aUxlwWpG7duqpWrZr2799/3QYZZ+bSHWP+nbFG5jpWvXp1NWrUqMiHl5eXIiMjdfr0aW3dutX+2lWrVik3N1etW7d2ev+lNa47lOZctmjRQp6enkpISLC37d27V4cPH1ZkZGShNf3www+qXLnydRFiJMnLy0stWrRwmIfc3FwlJCQUOg+RkZEO/SXpm2++sfevU6eOQkJCHPqkpaUpKSmpyLm1utKYy4L8/vvvOnHihENIvN44M5fuGPNvzd2rjVE2dOvWzdxyyy0mKSnJrF+/3kRERDhcMvz777+bhg0bmqSkJHtbcnKy2b59u5k5c6aRZL777juzfft2c+LEiWKPez1yZi6HDRtmateubVatWmW2bNliIiMjTWRkpH37559/bmbOnGl27txp9u3bZ6ZPn24qVKhgXnjhhWt6bKVt4cKFxtvb28yZM8fs3r3bPPLIIyYwMNCkpKQYY4x54IEHzLPPPmvvn5iYaDw8PMxbb71lfv75ZxMXF1fg5deBgYHms88+Mz/++KPp3bv33+bya1fOZXp6uhkzZozZsGGDOXjwoPn2229N8+bNTUREhMnMzHTLMV4rJZ3LrKwss337drN9+3ZTo0YNM2bMGLN9+3azb9++Yo+J4iPIwBhjzIkTJ8yAAQNMxYoVjb+/vxk8eLBJT0+3bz948KCRZFavXm1vi4uLM5LyPWbPnl3sca9Hzszl+fPnzeOPP24qV65sKlSoYPr27WuSk5Pt21euXGluvvlmU7FiRePn52eaNWtmZsyYYXJycq7loV0T77zzjqldu7bx8vIyrVq1Mhs3brRvi4qKMrGxsQ79Fy9ebBo0aGC8vLzMjTfeaL744guH7bm5uebf//63CQ4ONt7e3ub22283e/fuvRaH4naunMtz586ZO+64w1SvXt14enqasLAwM3To0L/NL96SzGXez/jlj6ioqGKPieKzGXMdXb8JAAD+VlgjAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgA8Dl1qxZI5vNptOnTxf7NS+++GKZ+vwjm82mTz/91N1lALgCggzwNzZjxgxVqlRJFy9etLdlZGTI09NTHTt2dOibF04OHDhwxXHbtm2r5ORkBQQEuLTejh076qmnnnLpmACsjSAD/I116tRJGRkZ2rJli71t3bp1CgkJUVJSkjIzM+3tq1evVu3atVWvXr0rjuvl5aWQkBDZbLZSqRsA8hBkgL+xhg0bqkaNGlqzZo29bc2aNerdu7fq1KmjjRs3OrR36tRJ0qVP6o2Pj1edOnXk6+urZs2a6eOPP3boe/lbSzNnzlRoaKgqVKigvn37atKkSQoMDMxX0wcffKDw8HAFBATo3nvvVXp6uiRp0KBBWrt2raZOnSqbzSabzaZDhw7le/1zzz1X4CeNN2vWTC+99JIkafPmzeratauqVaumgIAARUVFadu2bYXOU0HH88MPP+SrYf369Wrfvr18fX0VGhqqJ598UmfPnrVvnz59uiIiIuTj46Pg4GDdfffdhe4TQPEQZIC/uU6dOmn16tX256tXr1bHjh0VFRVlbz9//rySkpLsQSY+Pl7z5s3TjBkztGvXLo0aNUr333+/1q5dW+A+EhMTNWzYMI0cOVI//PCDunbtqgkTJuTrd+DAAX366adasWKFVqxYobVr1+q1116TJE2dOlWRkZEaOnSokpOTlZycrNDQ0HxjDBw4UJs2bXJ4C2zXrl368ccfdd9990mS0tPTFRsbq/Xr12vjxo2KiIhQjx497KHJGQcOHFC3bt0UExOjH3/8UYsWLdL69es1YsQISdKWLVv05JNP6qWXXtLevXv15ZdfqkOHDk7vD8D/z92fWgnAvWbOnGn8/PxMdna2SUtLMx4eHubYsWNmwYIFpkOHDsYYYxISEowk89tvv5nMzExToUIF8/333zuMM2TIEDNgwABjjDGrV682ksypU6eMMcb079/f9OzZ06H/wIEDTUBAgP15XFycqVChgklLS7O3jR071rRu3dr+PCoqyowcOfKKx9SsWTPz0ksv2Z+PGzfOYZzL5eTkmEqVKpnly5fb2ySZZcuWFXg8xhizfft2I8kcPHjQfvyPPPKIw7jr1q0z5cqVM+fPnzeffPKJ8ff3dzg+AFePMzLA31zHjh119uxZbd68WevWrVODBg1UvXp1RUVF2dfJrFmzRnXr1lXt2rW1f/9+nTt3Tl27dlXFihXtj3nz5hW6EHjv3r1q1aqVQ9vlzyUpPDxclSpVsj+vUaOGjh07VuJjGjhwoBYsWCBJMsboo48+0sCBA+3bU1NTNXToUEVERCggIED+/v7KyMjQ4cOHS7yvPDt27NCcOXMc5iQ6Olq5ubk6ePCgunbtqrCwMNWtW1cPPPCA5s+fr3Pnzjm9PwCXeLi7AADuVb9+fdWqVUurV6/WqVOnFBUVJUmqWbOmQkND9f3332v16tXq3LmzpEtXNUnSF198oX/84x8OY3l7e19VLZ6eng7PbTabcnNzSzzOgAED9K9//Uvbtm3T+fPndeTIEfXv39++PTY2VidOnNDUqVMVFhYmb29vRUZG6sKFCwWOV67cpb/5jDH2tuzsbIc+GRkZevTRR/Xkk0/me33t2rXl5eWlbdu2ac2aNfr666/1wgsv6MUXX9TmzZsLXCsEoHgIMgDUqVMnrVmzRqdOndLYsWPt7R06dNDKlSu1adMmPfbYY5Kkxo0by9vbW4cPH7aHnitp2LChNm/e7NB2+fPi8PLyUk5OzhX71apVS1FRUZo/f77Onz+vrl27KigoyL49MTFR06dPV48ePSRJR44c0Z9//lnoeNWrV5ckJScnq3LlypIuLfb9q+bNm2v37t2qX79+oeN4eHioS5cu6tKli+Li4hQYGKhVq1apX79+VzwmAAUjyABQp06dNHz4cGVnZzuEk6ioKI0YMUIXLlywL/StVKmSxowZo1GjRik3N1ft2rXTmTNnlJiYKH9/f8XGxuYb/4knnlCHDh00adIk9erVS6tWrdLKlStLfHl2eHi4kpKSdOjQIVWsWFFVqlSxny253MCBAxUXF6cLFy5o8uTJDtsiIiL0wQcfqGXLlkpLS9PYsWPl6+tb6H7r16+v0NBQvfjii5owYYJ++eUXTZw40aHPv/71L7Vp00YjRozQww8/LD8/P+3evVvffPON/vOf/2jFihX69ddf1aFDB1WuXFn/+9//lJubq4YNG5ZoDgBcxt2LdAC438GDB40k06hRI4f2Q4cOGUmmYcOGDu25ublmypQppmHDhsbT09NUr17dREdHm7Vr1xpjCl4c+95775l//OMfxtfX1/Tp08e88sorJiQkxL49Li7ONGvWzGE/kydPNmFhYfbne/fuNW3atDG+vr4OC20LcurUKePt7W0qVKhg0tPTHbZt27bNtGzZ0vj4+JiIiAizZMkSExYWZiZPnmzvo78s9jXGmPXr15ubbrrJ+Pj4mPbt25slS5bkq2HTpk2ma9eupmLFisbPz880bdrUTJgwwRhzaeFvVFSUqVy5svH19TVNmzY1ixYtKrR+AMVjM+Yvb/oCwDUydOhQ7dmzR+vWrXN3KQAsjLeWAFwTb731lrp27So/Pz+tXLlSc+fO1fTp091dFgCL44wMgGvinnvu0Zo1a5Senq66devqiSee0LBhw9xdFgCLI8gAAADL4oZ4AADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsv4/UWu2iHEnSBgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forward Pass with Scaled Input\n",
            "Scaled Input:\n",
            "tensor([[[[ 8.5879e+00,  1.8717e+02, -9.4950e+01, -2.1315e+01,  5.1511e+00,\n",
            "            3.3121e+01,  4.1350e+01, -7.5559e+01, -1.0287e+01,  1.2373e+02,\n",
            "            1.1978e+01, -1.8765e+02],\n",
            "          [ 1.4907e+02,  2.2726e+02,  5.2586e+01, -1.6160e+02, -2.1773e+02,\n",
            "           -1.7862e+02, -2.5021e+01,  9.4239e+01,  4.4940e+01,  1.3293e+02,\n",
            "            2.7630e+01, -3.4190e+01],\n",
            "          [-9.2466e+00, -1.0746e+02, -1.8250e+02,  1.0506e+02,  1.7623e+02,\n",
            "            9.0912e+01, -7.2970e+01, -3.7718e+01, -1.2182e+02,  8.5874e+01,\n",
            "           -1.2910e+02, -3.7221e+01],\n",
            "          [-8.4600e+01, -6.9826e+01, -1.1024e+02, -2.6193e+01,  1.5840e+02,\n",
            "           -1.4580e+02, -1.0437e+02,  3.1008e+01, -7.6572e+01,  4.0558e+01,\n",
            "           -1.8609e+02,  1.6276e+01],\n",
            "          [-1.2846e+02,  1.0148e+02,  1.3994e+02, -2.1786e+01, -6.2665e+01,\n",
            "            2.0207e+01,  4.7728e+01, -1.3362e+01,  3.2497e+02,  6.4790e+01,\n",
            "           -8.9799e+01, -4.5963e+01],\n",
            "          [ 4.3139e+01, -5.5942e+01, -7.0452e+01, -8.3103e+01,  1.2204e+02,\n",
            "           -1.2493e+02, -1.7497e+02,  7.6288e+01, -1.1510e+02,  4.0604e+01,\n",
            "           -7.2180e+01, -8.9137e+01],\n",
            "          [-1.8491e+01,  7.4449e+01,  1.4800e+02,  1.5372e+02, -1.7579e+02,\n",
            "            5.6148e+01, -6.5287e+01, -7.7047e+00,  4.0288e+00, -1.7053e+02,\n",
            "           -9.7684e+00,  5.6694e+01],\n",
            "          [-1.4502e+02,  1.4899e+02, -3.3437e+01, -1.0545e+02,  3.3869e+01,\n",
            "           -1.7456e+02, -2.0810e+01,  1.8970e+02, -1.3460e+02, -1.3997e+02,\n",
            "            1.1130e+02, -1.0047e+00],\n",
            "          [-3.4578e+01, -6.6524e+01, -1.4736e+02,  7.4908e+00,  7.0549e+01,\n",
            "           -1.0501e+02,  1.9437e+02,  1.6316e+01, -1.3430e+02, -3.8036e+01,\n",
            "           -4.1095e+01, -2.3476e+02],\n",
            "          [-1.0109e+02, -1.6736e+02,  5.3499e+01,  1.5399e+02, -8.7454e+01,\n",
            "           -4.1633e+01, -8.9339e+01, -4.6806e+01, -1.3089e+02,  1.4312e+01,\n",
            "            2.2325e+01,  1.0272e+01],\n",
            "          [-7.6372e+01, -2.6759e+01,  1.5177e+02,  7.7516e+01, -8.9564e-01,\n",
            "           -2.8671e+01, -2.1937e+01,  3.1832e-01,  4.7868e+01, -1.0622e+02,\n",
            "            8.8616e+01, -1.5124e+01],\n",
            "          [ 8.3396e+01, -4.6030e+01,  1.4221e+01, -5.6270e+01, -2.5601e+00,\n",
            "           -5.8262e+01,  1.6041e+02,  7.5658e+01, -1.2297e+02,  5.1698e+01,\n",
            "            5.0013e+01, -1.3710e+02]]]])\n",
            "\n",
            "Forward Pass - Input Tensor Shape: torch.Size([1, 1, 12, 12])\n",
            "After Layer 1 (Conv1 -> ReLU -> Pool1) - Tensor Shape: torch.Size([1, 6, 5, 5])\n",
            "After Layer 2 (Conv2 -> ReLU -> Pool2) - Tensor Shape: torch.Size([1, 12, 1, 1])\n",
            "Output for Scaled Input: tensor([[[[  0.0000]],\n",
            "\n",
            "         [[ 13.8887]],\n",
            "\n",
            "         [[  0.0000]],\n",
            "\n",
            "         [[ 61.7800]],\n",
            "\n",
            "         [[ 11.6980]],\n",
            "\n",
            "         [[125.9330]],\n",
            "\n",
            "         [[ 71.1017]],\n",
            "\n",
            "         [[ 77.1215]],\n",
            "\n",
            "         [[ 19.8001]],\n",
            "\n",
            "         [[ 70.2441]],\n",
            "\n",
            "         [[102.7522]],\n",
            "\n",
            "         [[  0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 15. **Observation: Using Non-linear Activation Functions Like ReLU Introduces Non-linearity**\n",
        "   - Non-linear activation functions, such as **ReLU (Rectified Linear Unit)**, are applied after each convolution to introduce non-linearity into the model, which allows CNNs to learn complex, non-linear relationships.\n",
        "   \n",
        "   - **Demonstration**:\n"
      ],
      "metadata": {
        "id": "_T28CttkAtZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a convolutional layer\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
        "\n",
        "# Create an input tensor\n",
        "x = torch.randn(1, 1, 5, 5)\n",
        "\n",
        "# Apply convolution\n",
        "conv_output = conv(x)\n",
        "print(\"Convolution output before ReLU:\", conv_output)\n",
        "\n",
        "# Apply ReLU activation\n",
        "relu_output = F.relu(conv_output)\n",
        "print(\"Convolution output after ReLU:\", relu_output)\n",
        "\n",
        "# After the convolution, some negative values are produced. The **ReLU activation** function sets all negative values to zero, allowing the model to focus only on positive activations, which helps with efficient learning.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E81kDBnCGG0",
        "outputId": "7023a401-3d76-46b7-952b-e503c7500392"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolution output before ReLU: tensor([[[[ 0.4818, -0.7846, -0.2429],\n",
            "          [-1.0451, -0.4941, -0.5844],\n",
            "          [-1.3912, -0.3852,  0.2174]],\n",
            "\n",
            "         [[-0.6647, -0.1456,  0.1431],\n",
            "          [ 0.0416, -0.7185,  0.1041],\n",
            "          [-0.2716,  0.5564, -0.6613]],\n",
            "\n",
            "         [[ 0.9184, -0.3555,  0.1617],\n",
            "          [-0.3226, -0.2905, -0.3486],\n",
            "          [-1.0145, -0.6723,  0.1885]]]], grad_fn=<ConvolutionBackward0>)\n",
            "Convolution output after ReLU: tensor([[[[0.4818, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.2174]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.1431],\n",
            "          [0.0416, 0.0000, 0.1041],\n",
            "          [0.0000, 0.5564, 0.0000]],\n",
            "\n",
            "         [[0.9184, 0.0000, 0.1617],\n",
            "          [0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.1885]]]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    }
  ]
}