{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7KB3w0+aOfp2NV+RoFSeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L06-Feed%20Forward%20Networks%20for%20Natural%20Language%20Processing/03_MLP_Forward_Pass_and_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. **Introduction**\n",
        "\n",
        "- **The Forward Pass in Neural Networks**:\n",
        "  - **Forward pass**: The process of passing input data through the network's layers (from input to output).\n",
        "  - Each layer transforms the input by applying **weights**, **biases**, and **activation functions**.\n",
        "  - The forward pass computes the final predictions of the network.\n",
        "\n",
        "- **Activation Functions**:\n",
        "  - Introduce **activation functions** which are essential for adding non-linearity to the network.\n",
        "  - Without activation functions, the network would behave like a linear model regardless of the number of layers.\n",
        "  - Common activation functions:\n",
        "    - **ReLU (Rectified Linear Unit)**: Outputs the input if it’s positive, otherwise outputs 0. Helps the network learn complex patterns.\n",
        "    - **Softmax**: Converts the raw scores (logits) from the network into probabilities, often used in the final layer for classification tasks.\n",
        "\n",
        "  **Observation**:\n",
        "  - Activation functions are what give neural networks the ability to learn and model non-linear data. They enable the network to capture more complex relationships in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wcq3ml1-J13T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fEjZeNfbJ18o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. **Forward Pass**\n",
        "\n",
        "- **Defining the Forward Pass in PyTorch**:\n",
        "  - In PyTorch, the forward pass is implemented in the `forward()` method of the model class (e.g., `MultilayerPerceptron`).\n",
        "  - During the forward pass, input data flows through each layer, transformations are applied, and the final output is produced.\n",
        "\n",
        "- **Code Breakdown**:\n",
        "  - Here’s how the forward pass works in the `MultilayerPerceptron` class:\n"
      ],
      "metadata": {
        "id": "NXZ40Up4J2Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # First layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x, apply_softmax=False):\n",
        "        # Apply ReLU activation after the first layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply the second layer\n",
        "        output = self.fc2(x)\n",
        "\n",
        "        # Optionally apply Softmax if needed for classification\n",
        "        if apply_softmax:\n",
        "            output = F.softmax(output, dim=1)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Q5_M8AXPx1oS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Explanation**:\n",
        "  - **First Layer (`fc1`)**: Transforms the input using a linear transformation and then applies the ReLU activation.\n",
        "  - **Second Layer (`fc2`)**: Produces the final output of the network (logits or raw scores).\n",
        "  - **Softmax**: Can be optionally applied to convert the logits into probabilities when performing classification tasks.\n",
        "\n",
        "  **Observation**:\n",
        "  - The forward pass defines how data moves through the network, applying transformations in each layer. Adding activation functions like ReLU introduces non-linearity between layers.\n",
        "\n",
        "  **Demonstration**:\n"
      ],
      "metadata": {
        "id": "UjGtIoGpJ2Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#Create an instance of the model and define the input data:\n",
        "input_dim = 3\n",
        "hidden_dim = 100\n",
        "output_dim = 4\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Create random input tensor\n",
        "x_input = torch.rand(2, input_dim)\n"
      ],
      "metadata": {
        "id": "xi_3oSEnyDr2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pass the input tensor through the model and print the output:\n",
        "output = mlp(x_input)\n",
        "print(\"Output of the network (without Softmax):\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnZ4AihByE4v",
        "outputId": "c1860f72-90a8-4886-e53b-2211e7243bd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network (without Softmax):\n",
            " tensor([[ 0.0118,  0.0963,  0.0022,  0.0604],\n",
            "        [ 0.1121, -0.0793,  0.0499,  0.0396]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-1fgr4DGJ2Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. **Understanding Outputs**\n",
        "\n",
        "- **Raw Output (Logits)**:\n",
        "  - When performing the forward pass without Softmax, the network produces **logits**. Logits are unnormalized values that the model outputs before applying any probability distribution.\n",
        "  - Logits can be positive or negative, and they are typically used as inputs for loss functions like **Cross-Entropy Loss** (which applies Softmax internally).\n",
        "\n",
        "- **Applying Softmax**:\n",
        "  - **Softmax** normalizes the logits into probabilities. This is useful when the network is performing **classification tasks** and we want to interpret the output as the likelihood of each class.\n",
        "  - Softmax converts logits into values between 0 and 1, where the sum of the probabilities is 1.\n",
        "\n",
        "  **Observation**:\n",
        "  - If you’re working on a classification problem, the output logits should be passed through Softmax to interpret them as probabilities.\n",
        "\n",
        "  **Demonstration**:\n"
      ],
      "metadata": {
        "id": "0qACGUIXJ2Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the forward pass with Softmax and print the resulting probabilities:\n",
        "output_with_softmax = mlp(x_input, apply_softmax=True)\n",
        "print(\"Output of the network (with Softmax):\\n\", output_with_softmax)\n",
        "\n",
        "# Show that the sum of probabilities for each sample is 1:\n",
        "print(\"Sum of probabilities for each sample:\\n\", output_with_softmax.sum(dim=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjJU7HXfyecN",
        "outputId": "5dfe09b9-0c6a-45f9-a746-99b64d634bda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network (with Softmax):\n",
            " tensor([[0.2422, 0.2636, 0.2399, 0.2543],\n",
            "        [0.2706, 0.2235, 0.2543, 0.2517]], grad_fn=<SoftmaxBackward0>)\n",
            "Sum of probabilities for each sample:\n",
            " tensor([1., 1.], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oe0qgLVCJ3QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. **Exercise**\n",
        "\n",
        "- **Turn Softmax On/Off**:\n",
        "  - Modify the `apply_softmax` flag and observe how the output changes when Softmax is applied vs. when it isn’t.\n",
        "\n",
        "  **Task**:\n",
        "  - Compare the output with and without Softmax:\n"
      ],
      "metadata": {
        "id": "_W91bPV7J3QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_without_softmax = mlp(x_input, apply_softmax=False)\n",
        "output_with_softmax = mlp(x_input, apply_softmax=True)\n",
        "\n",
        "print(\"Without Softmax:\\n\", output_without_softmax)\n",
        "print(\"With Softmax:\\n\", output_with_softmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhxQJppsyli4",
        "outputId": "9b6cb20c-da36-4b3b-9f80-f4558ad64bbc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Softmax:\n",
            " tensor([[ 0.0118,  0.0963,  0.0022,  0.0604],\n",
            "        [ 0.1121, -0.0793,  0.0499,  0.0396]], grad_fn=<AddmmBackward0>)\n",
            "With Softmax:\n",
            " tensor([[0.2422, 0.2636, 0.2399, 0.2543],\n",
            "        [0.2706, 0.2235, 0.2543, 0.2517]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  - Explain the differences between the logits (raw output) and the probabilities (Softmax output).\n",
        "\n",
        "- **Print Intermediate Activations**:\n",
        "  - Print the activations from the hidden layer to visualize how the input is transformed as it moves through the network.\n",
        "\n",
        "  **Task**:\n",
        "  - Modify the `forward()` method to print intermediate activations after the ReLU function:\n"
      ],
      "metadata": {
        "id": "tnzSDAL0J3QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x, apply_softmax=False):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    print(\"Intermediate activations after ReLU:\\n\", x)\n",
        "    output = self.fc2(x)\n",
        "    if apply_softmax:\n",
        "        output = F.softmax(output, dim=1)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "mBwfflyFyq5p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Observation**:\n",
        "  - The intermediate activations show how the data is transformed after applying ReLU, demonstrating how the network processes input at each stage.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Conclusion**\n",
        "\n",
        "- **Recap of the Forward Pass**:\n",
        "  - The forward pass is the process of feeding input through the network to obtain the output.\n",
        "  - The **ReLU** activation function introduces non-linearity, enabling the model to learn more complex patterns.\n",
        "  - **Softmax** is used when working with classification tasks, converting logits into interpretable probabilities.\n",
        "\n",
        "- **Key Takeaways**:\n",
        "  - The forward pass is a critical part of neural networks that defines how data flows through the layers.\n",
        "  - Activation functions like ReLU and Softmax play essential roles in the learning process by transforming data into meaningful representations.\n",
        "\n",
        "  **Demonstration**:\n",
        "  - Run the entire model on a new set of inputs and print both intermediate activations and final outputs to summarize the flow through the network:\n"
      ],
      "metadata": {
        "id": "p2g8pnHqJ3QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = torch.rand(3, input_dim)  # New input data\n",
        "output = mlp(x_input, apply_softmax=True)\n",
        "print(\"Final output with Softmax:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqMCj28jyxBT",
        "outputId": "75552cc6-ec53-4160-e16f-b6093e68268a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output with Softmax:\n",
            " tensor([[0.2705, 0.2742, 0.2289, 0.2264],\n",
            "        [0.2550, 0.2493, 0.2630, 0.2328],\n",
            "        [0.2558, 0.2477, 0.2470, 0.2494]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}