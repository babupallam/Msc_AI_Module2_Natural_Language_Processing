{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs5YfRQex/O3TS6etP7e6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L06-Feed%20Forward%20Networks%20for%20Natural%20Language%20Processing/04_CNN_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EcXoFeC62Ty_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Terminology of CNN\n",
        "\n",
        "Before diving into the practical implementation, it’s essential to understand the key terminologies that are fundamental to Convolutional Neural Networks.\n"
      ],
      "metadata": {
        "id": "NqHIatZp2XbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1. **Convolution**\n",
        "   - **Definition**: A convolution is a mathematical operation that takes two inputs: an input matrix (e.g., an image or sequence) and a filter (also called a kernel). The filter slides over the input, performing an element-wise multiplication and summing the results to produce a feature map.\n",
        "   - **In CNNs**: In a convolutional layer, filters are used to detect features such as edges, textures, or more complex patterns in the data. These filters slide over the input with a specific stride and produce feature maps that represent the presence of these patterns in different regions of the input.\n",
        "   \n",
        "   **1D Convolution Example**:\n",
        "   - For 1D convolution, the filter slides over a 1D sequence (e.g., a time series or text data) and captures local dependencies between adjacent elements in the sequence.\n",
        "   - **Formula**:\n",
        "\n",
        "      ![image.png](data:image/png;base64,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)\n",
        "      \n",
        "     Where `y(t)` is the output at position `t`, `x(t-i)` is the input value at position `t-i`, `w(i)` is the filter weight, and `b` is the bias term.\n"
      ],
      "metadata": {
        "id": "KNuFKO8l2XXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2. **Kernel (Filter)**\n",
        "   - **Definition**: A kernel (or filter) is a small matrix used to perform the convolution operation. For 1D convolutions, a kernel is a vector that is applied to a sliding window of the input sequence.\n",
        "   - **In CNNs**: The kernel captures specific patterns in the input sequence. For example, in text processing, a kernel might detect important words or sequences of words.\n"
      ],
      "metadata": {
        "id": "kwrPtpwt2XUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3. **Stride**\n",
        "   - **Definition**: Stride refers to how much the filter moves across the input data in each step of the convolution.\n",
        "   - **In CNNs**: A larger stride results in fewer overlaps between the regions that the filter covers, leading to a smaller output size. A stride of 1 means the filter moves one element at a time.\n"
      ],
      "metadata": {
        "id": "lo9xnuIX2XR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4. **Padding**\n",
        "   - **Definition**: Padding refers to adding extra elements (usually zeros) around the borders of the input sequence or matrix.\n",
        "   - **In CNNs**: Padding ensures that the convolution operation does not reduce the size of the output too much. This is particularly useful when we want to preserve the size of the input data after the convolution.\n"
      ],
      "metadata": {
        "id": "Hr_iFcIa2XOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 5. **Pooling**\n",
        "   - **Definition**: Pooling is a down-sampling technique that reduces the spatial dimensions (for 2D data) or the length (for 1D data) of the feature maps.\n",
        "   - **Types**:\n",
        "     - **Max Pooling**: Takes the maximum value from each region of the feature map.\n",
        "     - **Average Pooling**: Averages the values from each region.\n",
        "   - **In CNNs**: Pooling is applied to reduce the size of the feature map, which helps to decrease computational costs and prevent overfitting.\n"
      ],
      "metadata": {
        "id": "x8Dy4ycK2XLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 6. **Activation Function**\n",
        "   - **Definition**: Activation functions introduce non-linearity to the network, allowing it to learn more complex patterns.\n",
        "   - **In CNNs**: The most commonly used activation function is **ReLU (Rectified Linear Unit)**, which outputs the input value if it’s positive and zero otherwise.\n"
      ],
      "metadata": {
        "id": "mbysAItr2XIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 7. **Feature Map**\n",
        "   - **Definition**: A feature map is the output of a convolutional layer, representing the detected features after the filter is applied to the input.\n",
        "   - **In CNNs**: Multiple feature maps are generated by applying different filters, each detecting various patterns in the input.\n"
      ],
      "metadata": {
        "id": "8zjLZCVQ2XFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 8. **Fully Connected Layer**\n",
        "   - **Definition**: A fully connected (FC) layer connects every neuron from the previous layer to every neuron in the next layer.\n",
        "   - **In CNNs**: FC layers are typically used at the end of the CNN to combine the features detected by convolutional and pooling layers and make predictions.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XMf05Epo2XCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Section 2: Understanding 1D Convolutions\n",
        "\n",
        "**1D Convolutions** are used when the input data is sequential, such as time-series, audio data, or text. In contrast to 2D convolutions that operate on two-dimensional data (like images), **1D convolutions** slide a kernel across one dimension (e.g., time or sequence steps).\n"
      ],
      "metadata": {
        "id": "49WQkCvh2W_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Why Use 1D Convolutions?\n",
        "\n",
        "1. **Sequential Data**: 1D convolutions are particularly useful for data where the order of elements matters, such as:\n",
        "   - Time-series (e.g., stock prices or sensor data).\n",
        "   - Natural Language Processing (NLP) tasks, such as text classification or sentiment analysis.\n",
        "   - Audio data, such as speech recognition.\n",
        "   \n",
        "2. **Efficient Local Feature Extraction**: By sliding a kernel over the sequence, 1D convolutions efficiently capture local patterns in the data. For example, a 1D convolution in a text sequence might detect the presence of specific word combinations or n-grams.\n"
      ],
      "metadata": {
        "id": "-HKyoCKN2W8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Example of 1D Convolution in PyTorch:\n",
        "\n"
      ],
      "metadata": {
        "id": "WG6pOF7q2W5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a 1D convolutional layer\n",
        "# - in_channels: Number of input channels (1 in this case)\n",
        "# - out_channels: Number of output channels/filters (3 filters in this case)\n",
        "# - kernel_size: Size of the convolution filter (3 in this case)\n",
        "# - stride: Step size for sliding the filter (1 in this case)\n",
        "# - padding: Number of zero-padding elements added to both sides of the input (1 in this case, to preserve input size)\n",
        "conv1d_layer = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# Print details of the convolutional layer to understand its architecture\n",
        "print(\"Conv1D Layer Details:\")\n",
        "print(f\" - Input Channels: {conv1d_layer.in_channels}\")\n",
        "print(f\" - Output Channels (Filters): {conv1d_layer.out_channels}\")\n",
        "print(f\" - Kernel Size: {conv1d_layer.kernel_size}\")\n",
        "print(f\" - Stride: {conv1d_layer.stride}\")\n",
        "print(f\" - Padding: {conv1d_layer.padding}\")\n",
        "print(f\" - Weight Shape: {conv1d_layer.weight.shape} (out_channels, in_channels, kernel_size)\")\n",
        "print(f\" - Bias Shape: {conv1d_layer.bias.shape}\\n\")\n",
        "\n",
        "# Create a sample 1D input tensor\n",
        "# - Shape of input_data: (batch_size=1, in_channels=1, length=10)\n",
        "# - batch_size = 1: Single sample in this batch\n",
        "# - in_channels = 1: One channel (e.g., a single audio signal or time series)\n",
        "# - length = 10: The length of the 1D input (e.g., 10 time steps in a time series)\n",
        "input_data = torch.randn(1, 1, 10)\n",
        "\n",
        "# Print the input details\n",
        "print(\"Input Data Details:\")\n",
        "print(f\" - Shape: {input_data.shape} (batch_size={input_data.shape[0]}, channels={input_data.shape[1]}, length={input_data.shape[2]})\")\n",
        "print(f\" - Values:\\n{input_data}\\n\")\n",
        "\n",
        "# Apply the 1D convolution\n",
        "output_data = conv1d_layer(input_data)\n",
        "\n",
        "# Print the output details\n",
        "print(\"Output Data Details after 1D Convolution:\")\n",
        "print(f\" - Shape: {output_data.shape} (batch_size={output_data.shape[0]}, out_channels={output_data.shape[1]}, length={output_data.shape[2]})\")\n",
        "print(f\" - Values:\\n{output_data}\\n\")\n",
        "\n",
        "# Explanation of the output shape:\n",
        "# The output shape is determined by the convolution operation and depends on:\n",
        "# - Input size\n",
        "# - Filter size (kernel_size)\n",
        "# - Stride (how much the filter moves at each step)\n",
        "# - Padding (how much zero-padding is added to the input to control the output size)\n",
        "\n",
        "# The **Conv1d** layer takes a 1D sequence and applies three filters (kernels) of size 3, resulting in an output of shape `(1, 3, 10)`. The stride of 1 and padding of 1 ensure that the output length remains the same as the input length.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yPoCaSb4e6Z",
        "outputId": "9be880fa-6e2b-47d4-98c1-b510429713ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv1D Layer Details:\n",
            " - Input Channels: 1\n",
            " - Output Channels (Filters): 3\n",
            " - Kernel Size: (3,)\n",
            " - Stride: (1,)\n",
            " - Padding: (1,)\n",
            " - Weight Shape: torch.Size([3, 1, 3]) (out_channels, in_channels, kernel_size)\n",
            " - Bias Shape: torch.Size([3])\n",
            "\n",
            "Input Data Details:\n",
            " - Shape: torch.Size([1, 1, 10]) (batch_size=1, channels=1, length=10)\n",
            " - Values:\n",
            "tensor([[[-1.2321,  1.1069,  2.3936, -0.5780,  0.1051,  0.1725, -2.4462,\n",
            "          -0.7196, -0.5829,  2.4627]]])\n",
            "\n",
            "Output Data Details after 1D Convolution:\n",
            " - Shape: torch.Size([1, 3, 10]) (batch_size=1, out_channels=3, length=10)\n",
            " - Values:\n",
            "tensor([[[ 0.7723, -0.2456, -1.4338, -0.1986,  0.2529,  0.1783,  1.4993,\n",
            "           1.2737,  0.5368, -1.0330],\n",
            "         [-0.8062, -0.4079,  1.5984,  0.2241,  0.0353,  1.1050, -0.6754,\n",
            "          -0.3637, -1.1107,  1.1556],\n",
            "         [ 1.4265,  0.7982, -1.0312,  0.6513,  0.4323, -0.6116,  1.3293,\n",
            "           0.6113,  1.6412, -0.7945]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Section 3: Components of a 1D CNN\n",
        "\n",
        "Now, let’s explore the essential components of a 1D Convolutional Neural Network by building a simple 1D CNN in PyTorch. We’ll use these layers:\n",
        "1. **1D Convolutional Layer**.\n",
        "2. **Activation Function (ReLU)**.\n",
        "3. **Pooling Layer** (Max Pooling).\n",
        "4. **Fully Connected Layer**.\n"
      ],
      "metadata": {
        "id": "bkrnIgm52W27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 1: 1D Convolutional Layer\n",
        "A 1D convolutional layer will extract local patterns from the input sequence by sliding the kernel over the input.\n",
        "\n",
        "**Demonstration**:\n"
      ],
      "metadata": {
        "id": "Gfbp5fJ72W0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple 1D Convolutional Neural Network (CNN) with one convolutional layer\n",
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple1DCNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        # - in_channels=1: Input has 1 channel (e.g., a single time series or audio signal)\n",
        "        # - out_channels=4: The layer will output 4 feature maps (using 4 convolutional filters)\n",
        "        # - kernel_size=3: The size of the 1D convolution kernel is 3\n",
        "        # - padding=1: Padding of 1 ensures that the input length is preserved after convolution\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        # - kernel_size=2: Reduces the length of the feature maps by half\n",
        "        # - stride=2: Moves the pooling window by 2 steps, downsampling the feature maps\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layer (linear layer)\n",
        "        # - input features = 4 * 5 (since after max pooling, the length of each feature map is halved to 5)\n",
        "        # - output features = 1: Single output, assuming it's a regression task or binary classification\n",
        "        self.fc1 = nn.Linear(4 * 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first convolutional layer, followed by ReLU activation and max pooling\n",
        "        # Input shape: (batch_size, in_channels=1, length=10)\n",
        "        # Output shape after conv1: (batch_size, out_channels=4, length=10)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        print(f\"After Conv1 + ReLU: {x.shape}\")\n",
        "\n",
        "        # Output shape after max pooling: (batch_size, out_channels=4, length=5)\n",
        "        x = self.pool(x)\n",
        "        print(f\"After Max Pooling: {x.shape}\")\n",
        "\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        # Output shape: (batch_size, 4 * 5), where 4 is the number of feature maps, and 5 is the reduced length\n",
        "        x = x.view(-1, 4 * 5)\n",
        "        print(f\"After Flattening: {x.shape}\")\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        # Output shape: (batch_size, 1)\n",
        "        x = self.fc1(x)\n",
        "        print(f\"After Fully Connected Layer: {x.shape}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = Simple1DCNN()\n",
        "\n",
        "# Generate a random 1D input tensor\n",
        "# Shape: (batch_size=1, channels=1, length=10)\n",
        "input_data = torch.randn(1, 1, 10)\n",
        "\n",
        "# Perform the forward pass through the model\n",
        "output = model(input_data)\n",
        "\n",
        "# Print the output shape after passing through the model\n",
        "print(f\"Final Output Shape: {output.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aOtf8jb5It9",
        "outputId": "50113acc-7f64-4764-e66b-10b3e8bae4b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Conv1 + ReLU: torch.Size([1, 4, 10])\n",
            "After Max Pooling: torch.Size([1, 4, 5])\n",
            "After Flattening: torch.Size([1, 20])\n",
            "After Fully Connected Layer: torch.Size([1, 1])\n",
            "Final Output Shape: torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Detailed Explanation:\n",
        "\n",
        "1. **Architecture**:\n",
        "   - The model consists of:\n",
        "     - A **1D convolutional layer** (`conv1`): It takes 1 input channel (e.g., 1D time series), applies 4 filters of size 3, and outputs 4 feature maps. Padding of 1 ensures the output length matches the input.\n",
        "     - A **max-pooling layer** (`pool`): This reduces the length of the feature maps by half (from 10 to 5 in this case).\n",
        "     - A **fully connected layer** (`fc1`): After flattening the feature maps, the model uses a fully connected layer to output a single value (useful for regression or binary classification tasks).\n",
        "\n",
        "2. **Forward Pass**:\n",
        "   - **Convolution + ReLU Activation**: The input passes through the convolutional layer, followed by ReLU activation, which introduces non-linearity.\n",
        "     - Input shape: `(batch_size=1, in_channels=1, length=10)`\n",
        "     - Output shape after `conv1`: `(batch_size=1, out_channels=4, length=10)` because of the 4 filters, and padding preserves the length.\n",
        "   - **Max Pooling**: After applying max pooling, the length is halved.\n",
        "     - Output shape after max pooling: `(batch_size=1, out_channels=4, length=5)`.\n",
        "\n",
        "3. **Flattening**:\n",
        "   - The output is flattened to prepare it for the fully connected layer.\n",
        "     - Flattened shape: `(batch_size=1, 4 * 5)`, where `4` is the number of filters (output channels) and `5` is the reduced length after pooling.\n",
        "   \n",
        "4. **Fully Connected Layer**:\n",
        "   - The fully connected layer outputs a single value.\n",
        "     - Output shape: `(batch_size=1, 1)`.\n"
      ],
      "metadata": {
        "id": "BJPL0rFZ2WxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 2: Activation Function\n",
        "The **ReLU activation** is applied after the convolution to introduce non-linearity.\n",
        "\n",
        "**Demonstration**:\n"
      ],
      "metadata": {
        "id": "oV0aSePL2WuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply ReLU to the output of the convolution\n",
        "conv_output = model.conv1(input_data)\n",
        "\n",
        "\n",
        "relu_output = F.relu(conv_output)\n",
        "print(\"Output after ReLU activation:\\n\", relu_output)\n",
        "\n",
        "# ReLU zeroes out all negative values, allowing the model to focus on essential features of the input data.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj_TiFtb59I5",
        "outputId": "a98175f4-338a-46e0-ef23-0f3cca332f9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output after ReLU activation:\n",
            " tensor([[[0.0000, 0.0000, 0.0000, 0.1004, 0.4222, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6767, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.7493, 0.9256, 0.7444, 0.3106, 0.0000, 1.2430, 1.6322, 1.7780,\n",
            "          1.3620, 1.0226],\n",
            "         [0.5886, 0.7098, 0.2266, 0.0000, 0.0000, 1.4504, 1.8061, 0.0000,\n",
            "          0.6995, 0.4260]]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 3: Pooling Layer\n",
        "Max Pooling is applied to reduce the length of the feature maps and focus on the most prominent features.\n",
        "\n",
        "**Demonstration**:\n"
      ],
      "metadata": {
        "id": "OLzqUIE12Wra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply max pooling to the ReLU output\n",
        "pooled_output = model.pool(relu_output)\n",
        "print(\"Output after max pooling:\\n\", pooled_output)\n",
        "# Max pooling reduces the dimensionality of the feature map by taking the maximum value from non-overlapping regions of the feature map. This reduces computational complexity while retaining essential information.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z27_kDA06GrF",
        "outputId": "55702492-c684-4d77-b956-6c6c8f11d6b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output after max pooling:\n",
            " tensor([[[0.0000, 0.1004, 0.4222, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.6767, 0.0000, 0.0000],\n",
            "         [0.9256, 0.7444, 1.2430, 1.7780, 1.3620],\n",
            "         [0.7098, 0.2266, 1.4504, 1.8061, 0.6995]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 4: Fully Connected Layer\n",
        "Finally, the output from the pooled feature map is flattened and passed through a fully connected layer.\n",
        "\n",
        "**Demonstration**:\n",
        "\n"
      ],
      "metadata": {
        "id": "1EAwuz-L2Wow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the pooled output and apply the fully connected layer\n",
        "flattened = pooled_output.view(-1, 4 * 5)  # Assuming input length 10, after pooling it's 5\n",
        "fc_output = model.fc1(flattened)\n",
        "print(\"Output from the fully connected layer:\\n\", fc_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVzS8KRn6K9M",
        "outputId": "7134f648-695d-4305-f6f5-4b1ff1c3bf8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from the fully connected layer:\n",
            " tensor([[0.0499]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- The fully connected layer combines the features detected by the previous layers to produce the final output. In this case, the output is a single value, which could represent a prediction for a regression task.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RzQ09Svb2Wl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Section 4: Building a More Complex 1D CNN\n",
        "\n",
        "In this section, we will build a more complex 1D CNN architecture that consists of multiple convolutional layers, activation functions, pooling layers, and fully connected layers. This architecture can be used for tasks like **time-series classification** or **text classification**.\n",
        "\n",
        "**Demonstration**:\n"
      ],
      "metadata": {
        "id": "AtXCpIe42WjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a more complex 1D CNN architecture with two convolutional layers and two fully connected layers\n",
        "class Complex1DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Complex1DCNN, self).__init__()\n",
        "\n",
        "        # First 1D convolutional layer\n",
        "        # - in_channels=1: Single input channel (e.g., a 1D time series)\n",
        "        # - out_channels=16: 16 filters (neurons), each generating an output feature map\n",
        "        # - kernel_size=3: Filter size of 3 (each filter spans 3 time steps)\n",
        "        # - padding=1: Padding added to ensure output length matches input length\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second 1D convolutional layer\n",
        "        # - in_channels=16: Takes the 16 output channels from the first layer\n",
        "        # - out_channels=32: 32 filters (neurons)\n",
        "        # - kernel_size=3: Filter size of 3\n",
        "        # - padding=1: Ensures length remains the same after convolution\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        # - kernel_size=2: Reduces the size of the input by half (downsampling)\n",
        "        # - stride=2: Pooling window moves 2 steps at a time\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # - First fully connected layer\n",
        "        # - Input size: 32 * 5 (32 channels, reduced length to 5 after pooling twice)\n",
        "        # - Output size: 100 neurons\n",
        "        self.fc1 = nn.Linear(32 * 5, 100)\n",
        "\n",
        "        # Output layer\n",
        "        # - Input size: 100 neurons from the previous fully connected layer\n",
        "        # - Output size: 10 neurons (for classification into 10 classes)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial input dimensions\n",
        "        print(f\"Input Data:\\n{x}\")\n",
        "        print(f\" - Shape: {x.shape} (batch_size, channels, length)\")\n",
        "\n",
        "        # First convolution -> ReLU -> Max Pooling\n",
        "        print(f\"\\nApplying Conv1: (in_channels={self.conv1.in_channels}, out_channels={self.conv1.out_channels}, kernel_size={self.conv1.kernel_size})\")\n",
        "        print(f\" - Weight shape: {self.conv1.weight.shape} (out_channels, in_channels, kernel_size)\")\n",
        "        print(f\" - Bias shape: {self.conv1.bias.shape} (out_channels)\")\n",
        "        x = self.conv1(x)\n",
        "        print(f\"Values after Conv1:\\n{x}\")\n",
        "        print(f\" - Output shape after Conv1: {x.shape} (batch_size, out_channels=16, length=20)\")\n",
        "\n",
        "        # ReLU activation after Conv1\n",
        "        x = F.relu(x)\n",
        "        print(f\"Values after ReLU (Conv1):\\n{x}\")\n",
        "\n",
        "        # Max pooling after Conv1\n",
        "        x = self.pool(x)\n",
        "        print(f\"Values after Max Pooling (Conv1):\\n{x}\")\n",
        "        print(f\" - Output shape after Max Pooling (Conv1): {x.shape} (batch_size, out_channels=16, length=10)\")\n",
        "\n",
        "        # Second convolution -> ReLU -> Max Pooling\n",
        "        print(f\"\\nApplying Conv2: (in_channels={self.conv2.in_channels}, out_channels={self.conv2.out_channels}, kernel_size={self.conv2.kernel_size})\")\n",
        "        print(f\" - Weight shape: {self.conv2.weight.shape} (out_channels, in_channels, kernel_size)\")\n",
        "        print(f\" - Bias shape: {self.conv2.bias.shape} (out_channels)\")\n",
        "        x = self.conv2(x)\n",
        "        print(f\"Values after Conv2:\\n{x}\")\n",
        "        print(f\" - Output shape after Conv2: {x.shape} (batch_size, out_channels=32, length=10)\")\n",
        "\n",
        "        # ReLU activation after Conv2\n",
        "        x = F.relu(x)\n",
        "        print(f\"Values after ReLU (Conv2):\\n{x}\")\n",
        "\n",
        "        # Max pooling after Conv2\n",
        "        x = self.pool(x)\n",
        "        print(f\"Values after Max Pooling (Conv2):\\n{x}\")\n",
        "        print(f\" - Output shape after Max Pooling (Conv2): {x.shape} (batch_size, out_channels=32, length=5)\")\n",
        "\n",
        "        # Flatten the tensor before passing into fully connected layers\n",
        "        print(f\"\\nFlattening the tensor for Fully Connected layers\")\n",
        "        x = x.view(-1, 32 * 5)\n",
        "        print(f\"Values after Flattening:\\n{x}\")\n",
        "        print(f\" - Shape after flattening: {x.shape} (batch_size, flattened size)\")\n",
        "\n",
        "        # Fully connected layer 1\n",
        "        print(f\"\\nApplying FC1: (in_features={self.fc1.in_features}, out_features={self.fc1.out_features})\")\n",
        "        print(f\" - Weight shape: {self.fc1.weight.shape} (out_features, in_features)\")\n",
        "        print(f\" - Bias shape: {self.fc1.bias.shape} (out_features)\")\n",
        "        x = self.fc1(x)\n",
        "        print(f\"Values after FC1:\\n{x}\")\n",
        "        print(f\" - Output shape after FC1: {x.shape} (batch_size, neurons=100)\")\n",
        "\n",
        "        # ReLU activation after FC1\n",
        "        x = F.relu(x)\n",
        "        print(f\"Values after ReLU (FC1):\\n{x}\")\n",
        "\n",
        "        # Fully connected layer 2 (Output layer)\n",
        "        print(f\"\\nApplying FC2 (Output Layer): (in_features={self.fc2.in_features}, out_features={self.fc2.out_features})\")\n",
        "        print(f\" - Weight shape: {self.fc2.weight.shape} (out_features, in_features)\")\n",
        "        print(f\" - Bias shape: {self.fc2.bias.shape} (out_features)\")\n",
        "        x = self.fc2(x)\n",
        "        print(f\"Values after FC2 (Output):\\n{x}\")\n",
        "        print(f\" - Output shape after FC2: {x.shape} (batch_size, neurons=10)\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the complex CNN model\n",
        "complex_model = Complex1DCNN()\n",
        "\n",
        "# Generate a random 1D input tensor\n",
        "input_data = torch.randn(1, 1, 20)\n",
        "print(f\"Input Data:\\n{input_data}\\n\")\n",
        "\n",
        "# Forward pass through the model\n",
        "output = complex_model(input_data)\n",
        "\n",
        "# Print the final output shape and values\n",
        "print(f\"\\nFinal Output Shape: {output.shape}\")\n",
        "print(f\"Final Output Values:\\n{output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS7f4yMx6Ql7",
        "outputId": "e0f21813-be49-449d-8516-59557ce278b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data:\n",
            "tensor([[[-0.1224, -1.2369, -0.4172, -0.2618,  0.4049, -1.7903, -0.5131,\n",
            "          -0.1033, -0.5494,  0.2583,  0.0935, -1.0770,  0.3812,  0.6636,\n",
            "          -0.6359, -1.3385, -0.5934,  0.1702,  0.2764,  0.8674]]])\n",
            "\n",
            "Input Data:\n",
            "tensor([[[-0.1224, -1.2369, -0.4172, -0.2618,  0.4049, -1.7903, -0.5131,\n",
            "          -0.1033, -0.5494,  0.2583,  0.0935, -1.0770,  0.3812,  0.6636,\n",
            "          -0.6359, -1.3385, -0.5934,  0.1702,  0.2764,  0.8674]]])\n",
            " - Shape: torch.Size([1, 1, 20]) (batch_size, channels, length)\n",
            "\n",
            "Applying Conv1: (in_channels=1, out_channels=16, kernel_size=(3,))\n",
            " - Weight shape: torch.Size([16, 1, 3]) (out_channels, in_channels, kernel_size)\n",
            " - Bias shape: torch.Size([16]) (out_channels)\n",
            "Values after Conv1:\n",
            "tensor([[[ 0.3453,  0.0129,  0.1545, -0.1268,  0.5882, -0.0610,  0.1578,\n",
            "           0.1908, -0.1397,  0.0270,  0.2860, -0.2431, -0.0811,  0.1845,\n",
            "           0.2616,  0.1144,  0.0178, -0.0320, -0.2880,  0.0170],\n",
            "         [-0.4925, -0.7661, -0.8851, -0.5989, -0.4579, -0.7493, -1.0598,\n",
            "          -0.6146, -0.5815, -0.5228, -0.3646, -0.6461, -0.6279, -0.1844,\n",
            "          -0.4279, -0.9399, -0.9434, -0.5510, -0.2943, -0.1492],\n",
            "         [ 0.0249,  0.2618, -0.5626, -0.1944, -0.2506,  0.7002, -0.8290,\n",
            "          -0.2667,  0.0567, -0.4060,  0.0940,  0.3089, -0.7321, -0.0170,\n",
            "           0.5233,  0.0264, -0.5758, -0.4082, -0.0533, -0.1465],\n",
            "         [-0.5038, -0.5815, -0.6836, -0.5605, -0.5111, -0.5430, -0.7633,\n",
            "          -0.5688, -0.5356, -0.5495, -0.4555, -0.5392, -0.6109, -0.4035,\n",
            "          -0.4445, -0.6577, -0.7056, -0.5597, -0.4474, -0.4027],\n",
            "         [ 0.3515,  0.4114,  0.4613,  0.2955,  0.3752,  0.4051,  0.5275,\n",
            "           0.3628,  0.2944,  0.2858,  0.2872,  0.3106,  0.2978,  0.1856,\n",
            "           0.3249,  0.4979,  0.4587,  0.2861,  0.1375,  0.1325],\n",
            "         [ 0.1092, -0.3906,  0.2589, -0.5169,  0.6080, -0.7085,  0.4727,\n",
            "           0.0455, -0.6548, -0.2148, -0.0873, -0.8974, -0.1966, -0.3051,\n",
            "          -0.2763, -0.0286,  0.0710, -0.2956, -1.0080, -0.5447],\n",
            "         [-0.0261,  0.1167,  0.7968,  0.5234,  0.0378, -0.2236,  1.1098,\n",
            "           0.3864,  0.3056,  0.5700, -0.1089,  0.1851,  0.9741, -0.0330,\n",
            "          -0.4351,  0.3383,  0.9256,  0.6239,  0.3597,  0.1718],\n",
            "         [-0.0704,  0.1545, -0.2532, -0.2128, -0.2155,  0.3938, -0.3253,\n",
            "          -0.2042, -0.0755, -0.3525, -0.0993,  0.0896, -0.5084, -0.2649,\n",
            "           0.1799,  0.1136, -0.2516, -0.3483, -0.2982, -0.3808],\n",
            "         [ 0.3785,  0.8266, -0.2133,  0.1314, -0.0032,  1.4028, -0.4819,\n",
            "           0.0476,  0.4580, -0.1869,  0.4061,  0.8258, -0.5719,  0.1662,\n",
            "           1.0110,  0.6004, -0.2039, -0.1767,  0.1653, -0.0270],\n",
            "         [ 0.9608,  0.7439,  0.3628,  0.3047,  1.0478,  0.9465,  0.1890,\n",
            "           0.6017,  0.4528,  0.3377,  0.9454,  0.5010,  0.0072,  0.7701,\n",
            "           1.1931,  0.6965,  0.2042,  0.2715,  0.2311,  0.5065],\n",
            "         [-0.3130,  0.2632, -0.3147,  0.3315, -0.8045,  0.5602, -0.4577,\n",
            "          -0.2058,  0.4562,  0.0150, -0.1684,  0.7107,  0.0374, -0.0198,\n",
            "           0.0320, -0.0236, -0.1127,  0.1030,  0.6984,  0.2009],\n",
            "         [ 0.2986,  0.1544,  0.0786, -0.4548,  0.5109,  0.2413,  0.1174,\n",
            "           0.0254, -0.3660, -0.3909,  0.1173, -0.3409, -0.6245, -0.2710,\n",
            "           0.3546,  0.3630, -0.0880, -0.4581, -0.9300, -0.6340],\n",
            "         [-0.0826, -0.8851, -1.2226, -0.9492,  0.1811, -0.7835, -1.6197,\n",
            "          -0.5997, -0.8403, -0.7035,  0.0879, -0.9868, -1.1454,  0.2259,\n",
            "           0.1115, -1.1438, -1.5019, -0.8277, -0.5777,  0.0241],\n",
            "         [ 0.3481,  0.7787,  0.2459,  0.0858,  0.1567,  1.1233,  0.2337,\n",
            "           0.1767,  0.2856, -0.1400,  0.2183,  0.5550, -0.3453, -0.1616,\n",
            "           0.6807,  0.8355,  0.2516, -0.1304, -0.2533, -0.3971],\n",
            "         [ 0.2200,  0.8939,  0.8702,  0.5061,  0.0138,  0.9935,  1.1322,\n",
            "           0.4233,  0.5397,  0.2641,  0.0136,  0.7553,  0.4128, -0.3238,\n",
            "           0.2475,  1.1326,  1.0176,  0.3342,  0.0147, -0.3729],\n",
            "         [ 0.3182, -0.0093,  0.4435,  0.1969,  0.5725, -0.2599,  0.5406,\n",
            "           0.3702,  0.0682,  0.3887,  0.2856, -0.1194,  0.4825,  0.3371,\n",
            "           0.0437,  0.1158,  0.3768,  0.3575,  0.1224,  0.3365]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n",
            " - Output shape after Conv1: torch.Size([1, 16, 20]) (batch_size, out_channels=16, length=20)\n",
            "Values after ReLU (Conv1):\n",
            "tensor([[[0.3453, 0.0129, 0.1545, 0.0000, 0.5882, 0.0000, 0.1578, 0.1908,\n",
            "          0.0000, 0.0270, 0.2860, 0.0000, 0.0000, 0.1845, 0.2616, 0.1144,\n",
            "          0.0178, 0.0000, 0.0000, 0.0170],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0249, 0.2618, 0.0000, 0.0000, 0.0000, 0.7002, 0.0000, 0.0000,\n",
            "          0.0567, 0.0000, 0.0940, 0.3089, 0.0000, 0.0000, 0.5233, 0.0264,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3515, 0.4114, 0.4613, 0.2955, 0.3752, 0.4051, 0.5275, 0.3628,\n",
            "          0.2944, 0.2858, 0.2872, 0.3106, 0.2978, 0.1856, 0.3249, 0.4979,\n",
            "          0.4587, 0.2861, 0.1375, 0.1325],\n",
            "         [0.1092, 0.0000, 0.2589, 0.0000, 0.6080, 0.0000, 0.4727, 0.0455,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0710, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.1167, 0.7968, 0.5234, 0.0378, 0.0000, 1.1098, 0.3864,\n",
            "          0.3056, 0.5700, 0.0000, 0.1851, 0.9741, 0.0000, 0.0000, 0.3383,\n",
            "          0.9256, 0.6239, 0.3597, 0.1718],\n",
            "         [0.0000, 0.1545, 0.0000, 0.0000, 0.0000, 0.3938, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0896, 0.0000, 0.0000, 0.1799, 0.1136,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3785, 0.8266, 0.0000, 0.1314, 0.0000, 1.4028, 0.0000, 0.0476,\n",
            "          0.4580, 0.0000, 0.4061, 0.8258, 0.0000, 0.1662, 1.0110, 0.6004,\n",
            "          0.0000, 0.0000, 0.1653, 0.0000],\n",
            "         [0.9608, 0.7439, 0.3628, 0.3047, 1.0478, 0.9465, 0.1890, 0.6017,\n",
            "          0.4528, 0.3377, 0.9454, 0.5010, 0.0072, 0.7701, 1.1931, 0.6965,\n",
            "          0.2042, 0.2715, 0.2311, 0.5065],\n",
            "         [0.0000, 0.2632, 0.0000, 0.3315, 0.0000, 0.5602, 0.0000, 0.0000,\n",
            "          0.4562, 0.0150, 0.0000, 0.7107, 0.0374, 0.0000, 0.0320, 0.0000,\n",
            "          0.0000, 0.1030, 0.6984, 0.2009],\n",
            "         [0.2986, 0.1544, 0.0786, 0.0000, 0.5109, 0.2413, 0.1174, 0.0254,\n",
            "          0.0000, 0.0000, 0.1173, 0.0000, 0.0000, 0.0000, 0.3546, 0.3630,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1811, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0879, 0.0000, 0.0000, 0.2259, 0.1115, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0241],\n",
            "         [0.3481, 0.7787, 0.2459, 0.0858, 0.1567, 1.1233, 0.2337, 0.1767,\n",
            "          0.2856, 0.0000, 0.2183, 0.5550, 0.0000, 0.0000, 0.6807, 0.8355,\n",
            "          0.2516, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2200, 0.8939, 0.8702, 0.5061, 0.0138, 0.9935, 1.1322, 0.4233,\n",
            "          0.5397, 0.2641, 0.0136, 0.7553, 0.4128, 0.0000, 0.2475, 1.1326,\n",
            "          1.0176, 0.3342, 0.0147, 0.0000],\n",
            "         [0.3182, 0.0000, 0.4435, 0.1969, 0.5725, 0.0000, 0.5406, 0.3702,\n",
            "          0.0682, 0.3887, 0.2856, 0.0000, 0.4825, 0.3371, 0.0437, 0.1158,\n",
            "          0.3768, 0.3575, 0.1224, 0.3365]]], grad_fn=<ReluBackward0>)\n",
            "Values after Max Pooling (Conv1):\n",
            "tensor([[[0.3453, 0.1545, 0.5882, 0.1908, 0.0270, 0.2860, 0.1845, 0.2616,\n",
            "          0.0178, 0.0170],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2618, 0.0000, 0.7002, 0.0000, 0.0567, 0.3089, 0.0000, 0.5233,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4114, 0.4613, 0.4051, 0.5275, 0.2944, 0.3106, 0.2978, 0.4979,\n",
            "          0.4587, 0.1375],\n",
            "         [0.1092, 0.2589, 0.6080, 0.4727, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0710, 0.0000],\n",
            "         [0.1167, 0.7968, 0.0378, 1.1098, 0.5700, 0.1851, 0.9741, 0.3383,\n",
            "          0.9256, 0.3597],\n",
            "         [0.1545, 0.0000, 0.3938, 0.0000, 0.0000, 0.0896, 0.0000, 0.1799,\n",
            "          0.0000, 0.0000],\n",
            "         [0.8266, 0.1314, 1.4028, 0.0476, 0.4580, 0.8258, 0.1662, 1.0110,\n",
            "          0.0000, 0.1653],\n",
            "         [0.9608, 0.3628, 1.0478, 0.6017, 0.4528, 0.9454, 0.7701, 1.1931,\n",
            "          0.2715, 0.5065],\n",
            "         [0.2632, 0.3315, 0.5602, 0.0000, 0.4562, 0.7107, 0.0374, 0.0320,\n",
            "          0.1030, 0.6984],\n",
            "         [0.2986, 0.0786, 0.5109, 0.1174, 0.0000, 0.1173, 0.0000, 0.3630,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.1811, 0.0000, 0.0000, 0.0879, 0.2259, 0.1115,\n",
            "          0.0000, 0.0241],\n",
            "         [0.7787, 0.2459, 1.1233, 0.2337, 0.2856, 0.5550, 0.0000, 0.8355,\n",
            "          0.2516, 0.0000],\n",
            "         [0.8939, 0.8702, 0.9935, 1.1322, 0.5397, 0.7553, 0.4128, 1.1326,\n",
            "          1.0176, 0.0147],\n",
            "         [0.3182, 0.4435, 0.5725, 0.5406, 0.3887, 0.2856, 0.4825, 0.1158,\n",
            "          0.3768, 0.3365]]], grad_fn=<SqueezeBackward1>)\n",
            " - Output shape after Max Pooling (Conv1): torch.Size([1, 16, 10]) (batch_size, out_channels=16, length=10)\n",
            "\n",
            "Applying Conv2: (in_channels=16, out_channels=32, kernel_size=(3,))\n",
            " - Weight shape: torch.Size([32, 16, 3]) (out_channels, in_channels, kernel_size)\n",
            " - Bias shape: torch.Size([32]) (out_channels)\n",
            "Values after Conv2:\n",
            "tensor([[[-0.1529,  0.0947, -0.1568,  0.2619, -0.0642, -0.0869,  0.2490,\n",
            "          -0.0327,  0.1206, -0.1342],\n",
            "         [ 0.1314,  0.1561,  0.1596, -0.1923,  0.0655,  0.1327,  0.1155,\n",
            "           0.1554,  0.0075, -0.0322],\n",
            "         [-0.1646,  0.0122, -0.0758, -0.1824, -0.0542, -0.2390, -0.2790,\n",
            "          -0.0516,  0.0149,  0.0074],\n",
            "         [-0.0125, -0.1205, -0.1610, -0.3380, -0.0177, -0.1941, -0.1546,\n",
            "          -0.1193, -0.3119, -0.0202],\n",
            "         [ 0.1121,  0.4464,  0.1668,  0.5566,  0.3267,  0.1588,  0.4158,\n",
            "           0.1609,  0.4316, -0.0077],\n",
            "         [-0.4766, -0.5325, -0.5702, -0.5493, -0.6130, -0.4483, -0.4175,\n",
            "          -0.6080, -0.3862, -0.1855],\n",
            "         [ 0.3046,  0.1295,  0.2484,  0.2900,  0.0108,  0.2045,  0.1561,\n",
            "           0.3558,  0.2303,  0.0751],\n",
            "         [ 0.0164, -0.2863, -0.0032, -0.2639, -0.0708, -0.0811,  0.0143,\n",
            "           0.1575, -0.2539, -0.1634],\n",
            "         [-0.0091, -0.4897, -0.0743, -0.3154, -0.3776, -0.0114, -0.2999,\n",
            "          -0.0023, -0.3020, -0.2748],\n",
            "         [-0.1434, -0.4387, -0.3474, -0.0240, -0.1576, -0.1000, -0.1378,\n",
            "          -0.2828, -0.0136,  0.0608],\n",
            "         [ 0.1177,  0.0412,  0.1618, -0.1098,  0.0734,  0.1256, -0.0807,\n",
            "           0.1508, -0.1438, -0.0953],\n",
            "         [-0.1623,  0.0845, -0.2576, -0.0232,  0.0770, -0.3135,  0.0617,\n",
            "          -0.2685,  0.0096,  0.0327],\n",
            "         [ 0.2496,  0.5743,  0.4863,  0.4182,  0.2335,  0.2122,  0.5261,\n",
            "           0.4304,  0.2564,  0.0393],\n",
            "         [-0.1543,  0.2598, -0.2737,  0.3272, -0.0467, -0.1810,  0.0878,\n",
            "          -0.1664,  0.2151, -0.2264],\n",
            "         [-0.2440, -0.5873, -0.3009, -0.2604, -0.5491, -0.2600, -0.3498,\n",
            "          -0.4650, -0.4987, -0.2018],\n",
            "         [ 0.0077, -0.2288,  0.0381, -0.0245, -0.2041,  0.0165, -0.1673,\n",
            "          -0.0415, -0.1078,  0.0314],\n",
            "         [ 0.2083,  0.8010,  0.4749,  0.4706,  0.5136,  0.2531,  0.4831,\n",
            "           0.4830,  0.4152,  0.1639],\n",
            "         [ 0.3333,  0.3140,  0.5422,  0.7468,  0.1541,  0.2773,  0.2880,\n",
            "           0.4303,  0.5059,  0.1466],\n",
            "         [-0.2127, -0.2907, -0.4099, -0.3497, -0.3229, -0.3073, -0.4601,\n",
            "          -0.4331, -0.3199, -0.2567],\n",
            "         [ 0.1794, -0.3013,  0.0388, -0.1624, -0.2270, -0.1828, -0.3164,\n",
            "           0.1231, -0.3258, -0.3079],\n",
            "         [ 0.2303,  0.4329,  0.5187,  0.1115,  0.5591,  0.3305,  0.2563,\n",
            "           0.2569,  0.1919,  0.3083],\n",
            "         [ 0.1590,  0.4064,  0.2319,  0.1245,  0.4013,  0.3569,  0.3861,\n",
            "           0.2091,  0.1717,  0.2965],\n",
            "         [-0.2486,  0.1303, -0.4064,  0.4131,  0.1764, -0.1135,  0.2058,\n",
            "          -0.2257,  0.2621, -0.0596],\n",
            "         [ 0.2301,  0.1164,  0.3080,  0.2413,  0.1758,  0.2570,  0.2584,\n",
            "           0.3851,  0.1494,  0.0942],\n",
            "         [ 0.1956,  0.2952,  0.1091,  0.0472,  0.2889,  0.1243,  0.2621,\n",
            "           0.2735, -0.0881, -0.0200],\n",
            "         [ 0.0238,  0.1453,  0.3030,  0.4332,  0.2433,  0.2422,  0.1912,\n",
            "           0.1330,  0.2687,  0.1872],\n",
            "         [-0.0025,  0.3244, -0.1065, -0.0969,  0.0673, -0.1289,  0.1971,\n",
            "          -0.1439, -0.1610, -0.1919],\n",
            "         [-0.3037, -0.4475, -0.4574, -0.6932, -0.2783, -0.5388, -0.5734,\n",
            "          -0.4978, -0.4505, -0.0463],\n",
            "         [ 0.0195,  0.2088, -0.1936, -0.3368,  0.0720, -0.3513, -0.0991,\n",
            "          -0.1397, -0.2229, -0.3529],\n",
            "         [ 0.0895,  0.1090,  0.2189,  0.2345,  0.1240,  0.2204,  0.0730,\n",
            "           0.3801,  0.3151,  0.0235],\n",
            "         [ 0.1131, -0.5548, -0.0630, -0.4450, -0.2536,  0.1739, -0.3338,\n",
            "           0.0997, -0.2615,  0.0452],\n",
            "         [ 0.1829,  0.1286,  0.3428,  0.1809,  0.1271,  0.1442,  0.2192,\n",
            "           0.2444,  0.1201,  0.1329]]], grad_fn=<ConvolutionBackward0>)\n",
            " - Output shape after Conv2: torch.Size([1, 32, 10]) (batch_size, out_channels=32, length=10)\n",
            "Values after ReLU (Conv2):\n",
            "tensor([[[0.0000, 0.0947, 0.0000, 0.2619, 0.0000, 0.0000, 0.2490, 0.0000,\n",
            "          0.1206, 0.0000],\n",
            "         [0.1314, 0.1561, 0.1596, 0.0000, 0.0655, 0.1327, 0.1155, 0.1554,\n",
            "          0.0075, 0.0000],\n",
            "         [0.0000, 0.0122, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0149, 0.0074],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1121, 0.4464, 0.1668, 0.5566, 0.3267, 0.1588, 0.4158, 0.1609,\n",
            "          0.4316, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.3046, 0.1295, 0.2484, 0.2900, 0.0108, 0.2045, 0.1561, 0.3558,\n",
            "          0.2303, 0.0751],\n",
            "         [0.0164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0143, 0.1575,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0608],\n",
            "         [0.1177, 0.0412, 0.1618, 0.0000, 0.0734, 0.1256, 0.0000, 0.1508,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0845, 0.0000, 0.0000, 0.0770, 0.0000, 0.0617, 0.0000,\n",
            "          0.0096, 0.0327],\n",
            "         [0.2496, 0.5743, 0.4863, 0.4182, 0.2335, 0.2122, 0.5261, 0.4304,\n",
            "          0.2564, 0.0393],\n",
            "         [0.0000, 0.2598, 0.0000, 0.3272, 0.0000, 0.0000, 0.0878, 0.0000,\n",
            "          0.2151, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0077, 0.0000, 0.0381, 0.0000, 0.0000, 0.0165, 0.0000, 0.0000,\n",
            "          0.0000, 0.0314],\n",
            "         [0.2083, 0.8010, 0.4749, 0.4706, 0.5136, 0.2531, 0.4831, 0.4830,\n",
            "          0.4152, 0.1639],\n",
            "         [0.3333, 0.3140, 0.5422, 0.7468, 0.1541, 0.2773, 0.2880, 0.4303,\n",
            "          0.5059, 0.1466],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1794, 0.0000, 0.0388, 0.0000, 0.0000, 0.0000, 0.0000, 0.1231,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2303, 0.4329, 0.5187, 0.1115, 0.5591, 0.3305, 0.2563, 0.2569,\n",
            "          0.1919, 0.3083],\n",
            "         [0.1590, 0.4064, 0.2319, 0.1245, 0.4013, 0.3569, 0.3861, 0.2091,\n",
            "          0.1717, 0.2965],\n",
            "         [0.0000, 0.1303, 0.0000, 0.4131, 0.1764, 0.0000, 0.2058, 0.0000,\n",
            "          0.2621, 0.0000],\n",
            "         [0.2301, 0.1164, 0.3080, 0.2413, 0.1758, 0.2570, 0.2584, 0.3851,\n",
            "          0.1494, 0.0942],\n",
            "         [0.1956, 0.2952, 0.1091, 0.0472, 0.2889, 0.1243, 0.2621, 0.2735,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0238, 0.1453, 0.3030, 0.4332, 0.2433, 0.2422, 0.1912, 0.1330,\n",
            "          0.2687, 0.1872],\n",
            "         [0.0000, 0.3244, 0.0000, 0.0000, 0.0673, 0.0000, 0.1971, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0195, 0.2088, 0.0000, 0.0000, 0.0720, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0895, 0.1090, 0.2189, 0.2345, 0.1240, 0.2204, 0.0730, 0.3801,\n",
            "          0.3151, 0.0235],\n",
            "         [0.1131, 0.0000, 0.0000, 0.0000, 0.0000, 0.1739, 0.0000, 0.0997,\n",
            "          0.0000, 0.0452],\n",
            "         [0.1829, 0.1286, 0.3428, 0.1809, 0.1271, 0.1442, 0.2192, 0.2444,\n",
            "          0.1201, 0.1329]]], grad_fn=<ReluBackward0>)\n",
            "Values after Max Pooling (Conv2):\n",
            "tensor([[[0.0947, 0.2619, 0.0000, 0.2490, 0.1206],\n",
            "         [0.1561, 0.1596, 0.1327, 0.1554, 0.0075],\n",
            "         [0.0122, 0.0000, 0.0000, 0.0000, 0.0149],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4464, 0.5566, 0.3267, 0.4158, 0.4316],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3046, 0.2900, 0.2045, 0.3558, 0.2303],\n",
            "         [0.0164, 0.0000, 0.0000, 0.1575, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0608],\n",
            "         [0.1177, 0.1618, 0.1256, 0.1508, 0.0000],\n",
            "         [0.0845, 0.0000, 0.0770, 0.0617, 0.0327],\n",
            "         [0.5743, 0.4863, 0.2335, 0.5261, 0.2564],\n",
            "         [0.2598, 0.3272, 0.0000, 0.0878, 0.2151],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0077, 0.0381, 0.0165, 0.0000, 0.0314],\n",
            "         [0.8010, 0.4749, 0.5136, 0.4831, 0.4152],\n",
            "         [0.3333, 0.7468, 0.2773, 0.4303, 0.5059],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1794, 0.0388, 0.0000, 0.1231, 0.0000],\n",
            "         [0.4329, 0.5187, 0.5591, 0.2569, 0.3083],\n",
            "         [0.4064, 0.2319, 0.4013, 0.3861, 0.2965],\n",
            "         [0.1303, 0.4131, 0.1764, 0.2058, 0.2621],\n",
            "         [0.2301, 0.3080, 0.2570, 0.3851, 0.1494],\n",
            "         [0.2952, 0.1091, 0.2889, 0.2735, 0.0000],\n",
            "         [0.1453, 0.4332, 0.2433, 0.1912, 0.2687],\n",
            "         [0.3244, 0.0000, 0.0673, 0.1971, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.0000, 0.0720, 0.0000, 0.0000],\n",
            "         [0.1090, 0.2345, 0.2204, 0.3801, 0.3151],\n",
            "         [0.1131, 0.0000, 0.1739, 0.0997, 0.0452],\n",
            "         [0.1829, 0.3428, 0.1442, 0.2444, 0.1329]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            " - Output shape after Max Pooling (Conv2): torch.Size([1, 32, 5]) (batch_size, out_channels=32, length=5)\n",
            "\n",
            "Flattening the tensor for Fully Connected layers\n",
            "Values after Flattening:\n",
            "tensor([[0.0947, 0.2619, 0.0000, 0.2490, 0.1206, 0.1561, 0.1596, 0.1327, 0.1554,\n",
            "         0.0075, 0.0122, 0.0000, 0.0000, 0.0000, 0.0149, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.4464, 0.5566, 0.3267, 0.4158, 0.4316, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.3046, 0.2900, 0.2045, 0.3558, 0.2303, 0.0164,\n",
            "         0.0000, 0.0000, 0.1575, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0608, 0.1177, 0.1618, 0.1256, 0.1508,\n",
            "         0.0000, 0.0845, 0.0000, 0.0770, 0.0617, 0.0327, 0.5743, 0.4863, 0.2335,\n",
            "         0.5261, 0.2564, 0.2598, 0.3272, 0.0000, 0.0878, 0.2151, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0077, 0.0381, 0.0165, 0.0000, 0.0314, 0.8010,\n",
            "         0.4749, 0.5136, 0.4831, 0.4152, 0.3333, 0.7468, 0.2773, 0.4303, 0.5059,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1794, 0.0388, 0.0000, 0.1231,\n",
            "         0.0000, 0.4329, 0.5187, 0.5591, 0.2569, 0.3083, 0.4064, 0.2319, 0.4013,\n",
            "         0.3861, 0.2965, 0.1303, 0.4131, 0.1764, 0.2058, 0.2621, 0.2301, 0.3080,\n",
            "         0.2570, 0.3851, 0.1494, 0.2952, 0.1091, 0.2889, 0.2735, 0.0000, 0.1453,\n",
            "         0.4332, 0.2433, 0.1912, 0.2687, 0.3244, 0.0000, 0.0673, 0.1971, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2088, 0.0000, 0.0720, 0.0000,\n",
            "         0.0000, 0.1090, 0.2345, 0.2204, 0.3801, 0.3151, 0.1131, 0.0000, 0.1739,\n",
            "         0.0997, 0.0452, 0.1829, 0.3428, 0.1442, 0.2444, 0.1329]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            " - Shape after flattening: torch.Size([1, 160]) (batch_size, flattened size)\n",
            "\n",
            "Applying FC1: (in_features=160, out_features=100)\n",
            " - Weight shape: torch.Size([100, 160]) (out_features, in_features)\n",
            " - Bias shape: torch.Size([100]) (out_features)\n",
            "Values after FC1:\n",
            "tensor([[ 0.2045, -0.1151, -0.0477,  0.0244,  0.0877,  0.0389,  0.1812,  0.0899,\n",
            "          0.0309,  0.2461,  0.0460, -0.0550, -0.1382, -0.0533,  0.0073,  0.0736,\n",
            "         -0.0130, -0.0304, -0.2073, -0.1291, -0.0270, -0.1951,  0.2610, -0.2628,\n",
            "          0.0137,  0.2315,  0.0270,  0.0513,  0.0945,  0.0441, -0.1052,  0.1188,\n",
            "         -0.0620,  0.3138, -0.1970, -0.0760, -0.0878, -0.0316,  0.2127,  0.1227,\n",
            "         -0.1863,  0.1265,  0.0688,  0.2178, -0.1225,  0.2197, -0.2142, -0.0773,\n",
            "         -0.0962, -0.0205,  0.0225,  0.0163,  0.0976, -0.0871,  0.0644, -0.3380,\n",
            "          0.1357, -0.0725, -0.0841,  0.1395,  0.0941,  0.0238,  0.0140,  0.0306,\n",
            "          0.3881,  0.0036, -0.1347, -0.3854,  0.0659, -0.3193, -0.1294,  0.2269,\n",
            "         -0.1110,  0.1493, -0.0773, -0.0335,  0.0295, -0.0072,  0.0329, -0.2770,\n",
            "         -0.0970,  0.1116, -0.0257,  0.1384, -0.0047, -0.0850, -0.0932,  0.0569,\n",
            "         -0.0407, -0.0868,  0.2382,  0.1005,  0.0419, -0.0157, -0.1092,  0.2017,\n",
            "          0.0835,  0.1966,  0.1532,  0.1083]], grad_fn=<AddmmBackward0>)\n",
            " - Output shape after FC1: torch.Size([1, 100]) (batch_size, neurons=100)\n",
            "Values after ReLU (FC1):\n",
            "tensor([[0.2045, 0.0000, 0.0000, 0.0244, 0.0877, 0.0389, 0.1812, 0.0899, 0.0309,\n",
            "         0.2461, 0.0460, 0.0000, 0.0000, 0.0000, 0.0073, 0.0736, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.2610, 0.0000, 0.0137, 0.2315, 0.0270,\n",
            "         0.0513, 0.0945, 0.0441, 0.0000, 0.1188, 0.0000, 0.3138, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.2127, 0.1227, 0.0000, 0.1265, 0.0688, 0.2178, 0.0000,\n",
            "         0.2197, 0.0000, 0.0000, 0.0000, 0.0000, 0.0225, 0.0163, 0.0976, 0.0000,\n",
            "         0.0644, 0.0000, 0.1357, 0.0000, 0.0000, 0.1395, 0.0941, 0.0238, 0.0140,\n",
            "         0.0306, 0.3881, 0.0036, 0.0000, 0.0000, 0.0659, 0.0000, 0.0000, 0.2269,\n",
            "         0.0000, 0.1493, 0.0000, 0.0000, 0.0295, 0.0000, 0.0329, 0.0000, 0.0000,\n",
            "         0.1116, 0.0000, 0.1384, 0.0000, 0.0000, 0.0000, 0.0569, 0.0000, 0.0000,\n",
            "         0.2382, 0.1005, 0.0419, 0.0000, 0.0000, 0.2017, 0.0835, 0.1966, 0.1532,\n",
            "         0.1083]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Applying FC2 (Output Layer): (in_features=100, out_features=10)\n",
            " - Weight shape: torch.Size([10, 100]) (out_features, in_features)\n",
            " - Bias shape: torch.Size([10]) (out_features)\n",
            "Values after FC2 (Output):\n",
            "tensor([[-0.0673,  0.0661,  0.0615, -0.1170, -0.0386, -0.0466, -0.0809, -0.1540,\n",
            "          0.0964,  0.0009]], grad_fn=<AddmmBackward0>)\n",
            " - Output shape after FC2: torch.Size([1, 10]) (batch_size, neurons=10)\n",
            "\n",
            "Final Output Shape: torch.Size([1, 10])\n",
            "Final Output Values:\n",
            "tensor([[-0.0673,  0.0661,  0.0615, -0.1170, -0.0386, -0.0466, -0.0809, -0.1540,\n",
            "          0.0964,  0.0009]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explanation:\n",
        "\n",
        "1. **Architecture**:\n",
        "   - The model consists of:\n",
        "     - **2 convolutional layers**:\n",
        "       - `conv1`: 1 input channel, 16 output feature maps (filters), kernel size 3, padding of 1.\n",
        "       - `conv2`: 16 input feature maps, 32 output feature maps (filters), kernel size 3, padding of 1.\n",
        "     - **Max pooling**: The pooling layer reduces the feature map length by half (from 20 → 10 → 5).\n",
        "     - **2 fully connected layers**:\n",
        "       - `fc1`: After flattening, the fully connected layer transforms the input to 100 units.\n",
        "       - `fc2`: The final output layer maps to 10 units (for 10 classes, e.g., classification task).\n",
        "\n",
        "2. **Forward Pass**:\n",
        "   - **Conv1 + ReLU + Pooling**:\n",
        "     - Input shape: `(batch_size=1, channels=1, length=20)`\n",
        "     - Output after `conv1`: `(batch_size=1, out_channels=16, length=20)`\n",
        "     - After max pooling: `(batch_size=1, out_channels=16, length=10)`\n",
        "   - **Conv2 + ReLU + Pooling**:\n",
        "     - Output after `conv2`: `(batch_size=1, out_channels=32, length=10)`\n",
        "     - After max pooling: `(batch_size=1, out_channels=32, length=5)`\n",
        "   - **Flattening**:\n",
        "     - The output is reshaped to a flat tensor of size `(batch_size, 32 * 5)` to be passed into the fully connected layers.\n",
        "   - **Fully Connected Layer 1 (fc1)**:\n",
        "     - After passing through the first fully connected layer with ReLU, the output shape is `(batch_size, 100)`.\n",
        "   - **Output Layer (fc2)**:\n",
        "     - The final output has 10 units (for classification into 10 classes), with shape `(batch_size, 10)`.\n"
      ],
      "metadata": {
        "id": "QGXXZhU_2Wgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Section 5: Regularization in CNNs\n"
      ],
      "metadata": {
        "id": "LoHC4nGJ2WfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Dropout\n",
        "\n",
        "To prevent overfitting in deep CNNs, **dropout** is a commonly used regularization technique. Dropout randomly deactivates a portion of the neurons during training, forcing the network to learn redundant representations of the data.\n",
        "\n",
        "**Demonstration**:\n"
      ],
      "metadata": {
        "id": "p_--huoz2WcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a more complex 1D CNN architecture with two convolutional layers, dropout, and two fully connected layers\n",
        "class Complex1DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Complex1DCNN, self).__init__()\n",
        "\n",
        "        # First 1D convolutional layer\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second 1D convolutional layer\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dropout layer to reduce overfitting (p=0.5 means 50% chance of dropping a neuron)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 5, 100)  # 32 channels with length reduced to 5\n",
        "        self.fc2 = nn.Linear(100, 10)  # 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial input dimensions\n",
        "        print(f\"Input Data:\\n{x}\")\n",
        "        print(f\" - Shape: {x.shape} (batch_size, channels, length)\")\n",
        "\n",
        "        # First convolution -> ReLU -> Max Pooling\n",
        "        x = self.conv1(x)\n",
        "        print(f\"Values after Conv1:\\n{x}\")\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        print(f\"Values after Max Pooling (Conv1):\\n{x}\")\n",
        "        print(f\" - Output shape after Max Pooling (Conv1): {x.shape} (batch_size, out_channels=16, length=10)\")\n",
        "\n",
        "        # Second convolution -> ReLU -> Max Pooling\n",
        "        x = self.conv2(x)\n",
        "        print(f\"Values after Conv2:\\n{x}\")\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        print(f\"Values after Max Pooling (Conv2):\\n{x}\")\n",
        "        print(f\" - Output shape after Max Pooling (Conv2): {x.shape} (batch_size, out_channels=32, length=5)\")\n",
        "\n",
        "        # Flatten the tensor before passing into fully connected layers\n",
        "        print(f\"\\nFlattening the tensor for Fully Connected layers\")\n",
        "        x = x.view(-1, 32 * 5)\n",
        "        print(f\"Values after Flattening:\\n{x}\")\n",
        "        print(f\" - Shape after flattening: {x.shape} (batch_size, flattened size)\")\n",
        "\n",
        "        # Fully connected layer 1\n",
        "        x = self.fc1(x)\n",
        "        print(f\"Values after FC1:\\n{x}\")\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Apply dropout after the first fully connected layer\n",
        "        x = self.dropout(x)\n",
        "        print(f\"Values after Dropout:\\n{x} (note: some neurons will have values set to zero)\")\n",
        "\n",
        "        # Fully connected layer 2 (Output layer)\n",
        "        x = self.fc2(x)\n",
        "        print(f\"Values after FC2 (Output):\\n{x}\")\n",
        "        print(f\" - Output shape after FC2: {x.shape} (batch_size, neurons=10)\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the complex CNN model with dropout\n",
        "complex_model = Complex1DCNN()\n",
        "\n",
        "# Generate a random 1D input tensor\n",
        "input_data = torch.randn(1, 1, 20)\n",
        "print(f\"Input Data:\\n{input_data}\\n\")\n",
        "\n",
        "# Forward pass through the model\n",
        "output = complex_model(input_data)\n",
        "\n",
        "# Print the final output shape and values\n",
        "print(f\"\\nFinal Output Shape: {output.shape}\")\n",
        "print(f\"Final Output Values:\\n{output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff9a6U-x8CaE",
        "outputId": "b7f466ad-8de2-499b-e994-e1a7982e9ec9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data:\n",
            "tensor([[[ 1.5091, -1.0224,  0.8826, -0.2454,  0.9845,  0.0733, -0.4648,\n",
            "          -0.5518, -0.0188, -1.2759, -0.0076,  1.3832,  0.1292,  0.0409,\n",
            "           1.4141,  0.2515, -1.5029,  1.2123,  0.0559,  1.4383]]])\n",
            "\n",
            "Input Data:\n",
            "tensor([[[ 1.5091, -1.0224,  0.8826, -0.2454,  0.9845,  0.0733, -0.4648,\n",
            "          -0.5518, -0.0188, -1.2759, -0.0076,  1.3832,  0.1292,  0.0409,\n",
            "           1.4141,  0.2515, -1.5029,  1.2123,  0.0559,  1.4383]]])\n",
            " - Shape: torch.Size([1, 1, 20]) (batch_size, channels, length)\n",
            "Values after Conv1:\n",
            "tensor([[[ 0.2020, -1.0325,  0.4979, -0.5932,  0.1778, -0.5461, -0.2661,\n",
            "          -0.0595,  0.1132, -0.4108,  0.3896,  0.1604, -0.7147, -0.2103,\n",
            "           0.1442, -0.6781, -0.5960,  0.7736, -0.6777,  0.1468],\n",
            "         [ 0.6254,  0.4534, -0.0502,  0.4288,  0.3337,  0.5887, -0.0027,\n",
            "          -0.2780, -0.1330, -0.3180, -0.4705,  0.5741,  0.7852,  0.1813,\n",
            "           0.6057,  0.8479, -0.2777, -0.1591,  0.6762,  0.6219],\n",
            "         [-1.0569,  0.1933,  0.0191,  0.1716, -0.2491, -0.6254, -0.0018,\n",
            "           0.5068, -0.2203,  0.6345,  1.2188, -0.5146, -0.6215,  0.5668,\n",
            "          -0.4996, -1.3370,  1.1238,  0.2161,  0.0746, -0.6227],\n",
            "         [ 0.5710, -1.4557,  0.6916, -0.8981,  0.2777, -0.5314, -0.2525,\n",
            "          -0.1306,  0.3632, -0.6048,  0.1625,  0.2880, -0.8147, -0.5183,\n",
            "           0.2491, -0.4617, -1.0742,  0.9794, -1.0509,  0.3032],\n",
            "         [ 0.2119,  0.1757, -0.2988,  0.0367, -0.0775,  0.3070, -0.0366,\n",
            "          -0.2974, -0.0937, -0.2281, -0.7039,  0.0345,  0.3729, -0.2413,\n",
            "           0.0365,  0.6126, -0.3342, -0.4646,  0.1129,  0.0793],\n",
            "         [ 0.3640, -0.1014,  0.4367,  0.3583,  0.6049, -0.1415, -0.4196,\n",
            "          -0.2029, -0.5009, -0.5783,  0.7352,  0.8226,  0.1017,  0.7239,\n",
            "           0.8919, -0.5373, -0.1495,  0.7563,  0.7040,  0.7896],\n",
            "         [ 0.7102,  0.1733, -0.1477, -0.1101, -0.0141,  0.7487,  0.4200,\n",
            "          -0.0938,  0.5888,  0.0563, -1.1832,  0.0605,  0.6262, -0.6411,\n",
            "           0.0124,  1.5041, -0.5240, -0.4973, -0.2240,  0.1595],\n",
            "         [ 0.8545, -0.2939,  0.8689,  0.1988,  0.8296,  0.0956, -0.0083,\n",
            "           0.1332,  0.1953, -0.2842,  0.7602,  0.9832,  0.1325,  0.5372,\n",
            "           1.0089, -0.0656, -0.2415,  1.1580,  0.3548,  0.9768],\n",
            "         [ 0.1986, -0.2052,  0.9792,  0.3713,  0.7779, -0.3773, -0.0951,\n",
            "           0.4450, -0.0482,  0.0354,  1.7547,  0.7887, -0.2959,  1.0627,\n",
            "           0.8375, -1.1095,  0.4934,  1.4727,  0.5236,  0.7009],\n",
            "         [ 0.1398, -0.4337, -0.1309, -0.2511, -0.0113, -0.1900, -0.4509,\n",
            "          -0.5139, -0.3985, -0.6942, -0.3789,  0.1382, -0.1161, -0.2229,\n",
            "           0.1567, -0.1263, -0.6974, -0.0663, -0.1096,  0.1573],\n",
            "         [ 1.4598, -1.2734,  1.1746, -0.6359,  0.8300,  0.0616,  0.1151,\n",
            "           0.0580,  0.8748, -0.5263,  0.1342,  0.9525, -0.2432, -0.3601,\n",
            "           0.9062,  0.4027, -1.2190,  1.4251, -0.7542,  1.0103],\n",
            "         [ 0.6827, -1.1063,  0.7983, -0.7477,  0.3077, -0.1889,  0.2230,\n",
            "           0.2644,  0.8655, -0.0445,  0.1734,  0.2185, -0.5747, -0.5158,\n",
            "           0.1505,  0.0320, -0.6829,  0.9533, -1.0499,  0.2451],\n",
            "         [ 0.2015,  0.4136, -0.2758,  0.0225, -0.2011,  0.5713,  0.4578,\n",
            "           0.1039,  0.4736,  0.3852, -0.8181, -0.2407,  0.4700, -0.4326,\n",
            "          -0.2829,  1.0789,  0.0179, -0.6088, -0.1308, -0.1817],\n",
            "         [ 0.0047, -0.4349,  0.3567, -0.5619, -0.1830, -0.0470,  0.6359,\n",
            "           0.6513,  1.0586,  0.7783,  0.0791, -0.4939, -0.4916, -0.5710,\n",
            "          -0.5884,  0.1889,  0.1709,  0.2713, -1.0621, -0.4838],\n",
            "         [-0.1757,  0.1557, -0.3721, -0.0572, -0.2883,  0.0977,  0.0144,\n",
            "          -0.1328, -0.0592,  0.0290, -0.5071, -0.3012,  0.0990, -0.2735,\n",
            "          -0.3112,  0.2764, -0.0447, -0.5308, -0.0907, -0.2811],\n",
            "         [-0.0186,  0.1234, -0.1716,  0.2221,  0.0816, -0.0456, -0.3736,\n",
            "          -0.3629, -0.5650, -0.4542, -0.0287,  0.2395,  0.1716,  0.2581,\n",
            "           0.2872, -0.1819, -0.1511, -0.1054,  0.4737,  0.2325]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n",
            "Values after Max Pooling (Conv1):\n",
            "tensor([[[0.2020, 0.4979, 0.1778, 0.0000, 0.1132, 0.3896, 0.0000, 0.1442,\n",
            "          0.7736, 0.1468],\n",
            "         [0.6254, 0.4288, 0.5887, 0.0000, 0.0000, 0.5741, 0.7852, 0.8479,\n",
            "          0.0000, 0.6762],\n",
            "         [0.1933, 0.1716, 0.0000, 0.5068, 0.6345, 1.2188, 0.5668, 0.0000,\n",
            "          1.1238, 0.0746],\n",
            "         [0.5710, 0.6916, 0.2777, 0.0000, 0.3632, 0.2880, 0.0000, 0.2491,\n",
            "          0.9794, 0.3032],\n",
            "         [0.2119, 0.0367, 0.3070, 0.0000, 0.0000, 0.0345, 0.3729, 0.6126,\n",
            "          0.0000, 0.1129],\n",
            "         [0.3640, 0.4367, 0.6049, 0.0000, 0.0000, 0.8226, 0.7239, 0.8919,\n",
            "          0.7563, 0.7896],\n",
            "         [0.7102, 0.0000, 0.7487, 0.4200, 0.5888, 0.0605, 0.6262, 1.5041,\n",
            "          0.0000, 0.1595],\n",
            "         [0.8545, 0.8689, 0.8296, 0.1332, 0.1953, 0.9832, 0.5372, 1.0089,\n",
            "          1.1580, 0.9768],\n",
            "         [0.1986, 0.9792, 0.7779, 0.4450, 0.0354, 1.7547, 1.0627, 0.8375,\n",
            "          1.4727, 0.7009],\n",
            "         [0.1398, 0.0000, 0.0000, 0.0000, 0.0000, 0.1382, 0.0000, 0.1567,\n",
            "          0.0000, 0.1573],\n",
            "         [1.4598, 1.1746, 0.8300, 0.1151, 0.8748, 0.9525, 0.0000, 0.9062,\n",
            "          1.4251, 1.0103],\n",
            "         [0.6827, 0.7983, 0.3077, 0.2644, 0.8655, 0.2185, 0.0000, 0.1505,\n",
            "          0.9533, 0.2451],\n",
            "         [0.4136, 0.0225, 0.5713, 0.4578, 0.4736, 0.0000, 0.4700, 1.0789,\n",
            "          0.0179, 0.0000],\n",
            "         [0.0047, 0.3567, 0.0000, 0.6513, 1.0586, 0.0791, 0.0000, 0.1889,\n",
            "          0.2713, 0.0000],\n",
            "         [0.1557, 0.0000, 0.0977, 0.0144, 0.0290, 0.0000, 0.0990, 0.2764,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1234, 0.2221, 0.0816, 0.0000, 0.0000, 0.2395, 0.2581, 0.2872,\n",
            "          0.0000, 0.4737]]], grad_fn=<SqueezeBackward1>)\n",
            " - Output shape after Max Pooling (Conv1): torch.Size([1, 16, 10]) (batch_size, out_channels=16, length=10)\n",
            "Values after Conv2:\n",
            "tensor([[[ 1.2254e-01,  1.9001e-01,  5.1447e-01,  3.4477e-01,  1.4473e-01,\n",
            "           2.4620e-01,  4.5126e-01,  4.2145e-01,  2.7761e-01,  3.5305e-01],\n",
            "         [ 2.0650e-01,  3.7405e-01,  1.8983e-01,  1.9587e-01,  1.7547e-01,\n",
            "           2.3203e-01,  2.1476e-01,  2.6446e-01,  3.7612e-01,  2.7652e-01],\n",
            "         [-3.2707e-01, -1.0840e-01, -6.0725e-02, -3.5606e-01, -9.1705e-02,\n",
            "           2.2047e-01, -2.7353e-01, -7.3793e-02,  1.1041e-01,  1.0554e-02],\n",
            "         [-4.3274e-01, -3.3433e-01, -2.3011e-01, -3.0555e-01, -3.0196e-01,\n",
            "          -2.6221e-01, -1.2296e-01, -3.0671e-01, -4.1158e-01, -7.0965e-05],\n",
            "         [ 3.9428e-01,  4.5221e-01,  4.5417e-01,  4.6743e-01,  5.4815e-01,\n",
            "           2.2010e-01,  4.8826e-01,  4.4428e-01,  4.6465e-01,  4.9566e-01],\n",
            "         [ 4.3677e-01,  1.4931e-01, -2.8086e-02,  2.2584e-01,  5.6937e-01,\n",
            "           3.2629e-01,  4.8395e-01,  7.7956e-01,  5.2057e-01, -9.3387e-02],\n",
            "         [ 7.5979e-01,  5.2966e-01,  4.7718e-01,  3.0781e-01,  6.2835e-01,\n",
            "           1.2528e-01,  4.8016e-01,  1.1623e+00,  7.3424e-01,  3.0265e-01],\n",
            "         [ 2.6788e-01,  3.7058e-01,  4.2334e-01,  2.5815e-01, -4.3603e-02,\n",
            "           3.2192e-01,  1.1192e-01, -4.7710e-02,  9.2322e-02,  3.7931e-01],\n",
            "         [-1.8087e-01, -2.1148e-01, -1.5719e-01, -3.6760e-01, -4.1638e-01,\n",
            "          -4.8306e-01, -7.5411e-01, -6.8955e-01, -6.5776e-01, -1.8736e-02],\n",
            "         [ 1.1679e-01,  6.3909e-01,  3.4211e-01,  6.4898e-01,  4.0182e-01,\n",
            "           4.9272e-01,  9.7922e-01,  1.7553e-01,  5.7502e-01,  1.8391e-01],\n",
            "         [ 1.4960e-01,  2.8016e-01,  4.3254e-01,  8.0616e-02,  7.9750e-02,\n",
            "           1.1850e-01,  3.2600e-01,  3.2445e-01,  3.5126e-01,  4.6294e-01],\n",
            "         [-4.8038e-02, -4.2690e-02, -2.6829e-01,  2.2770e-02,  3.3291e-01,\n",
            "           1.4528e-01, -3.8359e-01, -5.3267e-02,  4.7755e-02, -2.5920e-01],\n",
            "         [-3.4147e-01, -2.8238e-01, -1.3447e-01, -4.3669e-03,  1.4934e-01,\n",
            "          -9.3535e-02, -3.8105e-01, -7.7246e-02, -1.6810e-01, -3.5079e-02],\n",
            "         [ 2.2673e-01,  2.9346e-01, -9.9540e-02, -3.5867e-02,  2.1188e-01,\n",
            "           3.1366e-01, -1.2450e-01, -1.0025e-01,  2.8473e-01, -2.2751e-01],\n",
            "         [ 6.9538e-02, -4.4946e-02,  1.5252e-01, -5.9598e-02, -2.5648e-01,\n",
            "          -1.7181e-02,  2.1253e-01,  3.7229e-04,  9.5511e-02,  4.9255e-01],\n",
            "         [ 1.4158e-01, -7.6831e-02,  1.7401e-01,  1.2981e-01,  3.1401e-01,\n",
            "           3.8288e-01, -2.9943e-01,  2.8877e-01,  2.2860e-01,  1.2485e-01],\n",
            "         [ 9.8201e-02,  2.1893e-01,  1.5166e-01,  3.6458e-01,  6.4391e-02,\n",
            "          -1.0066e-01,  4.8120e-01,  1.2941e-01, -3.0990e-02,  2.3700e-01],\n",
            "         [-2.3676e-01, -4.2619e-01, -8.9541e-02, -2.2306e-02, -2.2976e-01,\n",
            "          -3.2343e-01, -3.3694e-01, -2.5589e-01, -2.3608e-01,  2.1063e-01],\n",
            "         [-6.1874e-02, -2.3277e-01,  1.1313e-01, -2.0927e-01, -1.0270e-01,\n",
            "           3.3245e-01,  1.0619e-01,  1.8034e-01, -2.7833e-02,  2.3569e-01],\n",
            "         [-3.1812e-01, -3.5106e-02, -2.8176e-01, -3.6091e-01, -1.3211e-01,\n",
            "           1.7974e-01, -1.1826e-01, -4.2808e-01, -1.6096e-01, -1.9021e-01],\n",
            "         [ 4.2467e-01,  2.6947e-01,  1.1254e-02,  6.3239e-02,  3.2242e-01,\n",
            "           5.0395e-02,  8.8654e-02,  1.0641e-01,  2.0054e-01, -8.9238e-02],\n",
            "         [-3.0425e-01,  1.1161e-02, -1.3080e-01,  1.7919e-02, -1.6790e-01,\n",
            "          -4.2993e-02,  1.4167e-01, -2.6171e-01,  2.4571e-02, -1.4007e-01],\n",
            "         [ 4.0437e-01,  7.0766e-01,  6.5364e-01,  4.8445e-01,  4.9250e-01,\n",
            "           5.2841e-01,  3.2530e-01,  2.4373e-01,  3.8493e-01,  4.1293e-01],\n",
            "         [ 9.6860e-02,  3.3008e-01,  4.9802e-01,  2.8034e-01,  1.5709e-01,\n",
            "           5.5878e-01,  4.8572e-01,  2.2107e-01,  1.6498e-01,  3.0040e-01],\n",
            "         [-1.5479e-01, -1.7632e-01, -1.7509e-01, -1.1482e-01, -2.2538e-01,\n",
            "          -3.4314e-01, -6.7279e-01, -1.8711e-01,  2.0319e-01, -1.8896e-02],\n",
            "         [ 2.1370e-01,  4.0516e-01,  3.4274e-01,  4.5932e-02,  2.4345e-01,\n",
            "           4.4269e-01,  4.1407e-01,  4.7774e-02,  8.4151e-04,  1.5088e-01],\n",
            "         [ 5.9807e-01,  4.4403e-01, -6.7814e-02,  5.2444e-01,  5.1871e-01,\n",
            "           8.5717e-02,  1.1424e-01,  5.7516e-01,  8.5976e-01, -3.5439e-01],\n",
            "         [ 4.0047e-02, -3.0578e-02,  5.1559e-02, -8.3008e-02, -1.4534e-01,\n",
            "          -1.8703e-01, -5.7146e-01,  1.0057e-01,  1.1997e-01, -2.5016e-01],\n",
            "         [ 2.7084e-01,  5.8702e-01,  4.8222e-01,  5.0560e-01,  2.8927e-01,\n",
            "           3.7457e-01,  4.5397e-01,  2.5846e-01,  5.6535e-01,  2.7303e-01],\n",
            "         [-9.8355e-02, -8.8798e-02, -1.9437e-01, -2.7799e-01, -1.0433e-01,\n",
            "          -3.9719e-01,  2.2549e-01,  5.7777e-02,  5.9493e-02,  5.7720e-02],\n",
            "         [ 2.8435e-01,  1.1741e-01, -2.7001e-02, -8.0167e-02,  1.7893e-02,\n",
            "           5.3668e-02,  3.1268e-01,  1.5139e-01, -3.2011e-02,  1.9223e-01],\n",
            "         [-3.0307e-01, -3.9758e-01, -2.6548e-01, -1.8091e-01, -5.6174e-01,\n",
            "          -3.9513e-01, -1.7913e-02, -3.4365e-01, -2.0666e-01, -2.6689e-01]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n",
            "Values after Max Pooling (Conv2):\n",
            "tensor([[[0.1900, 0.5145, 0.2462, 0.4513, 0.3531],\n",
            "         [0.3741, 0.1959, 0.2320, 0.2645, 0.3761],\n",
            "         [0.0000, 0.0000, 0.2205, 0.0000, 0.1104],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4522, 0.4674, 0.5481, 0.4883, 0.4957],\n",
            "         [0.4368, 0.2258, 0.5694, 0.7796, 0.5206],\n",
            "         [0.7598, 0.4772, 0.6283, 1.1623, 0.7342],\n",
            "         [0.3706, 0.4233, 0.3219, 0.1119, 0.3793],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6391, 0.6490, 0.4927, 0.9792, 0.5750],\n",
            "         [0.2802, 0.4325, 0.1185, 0.3260, 0.4629],\n",
            "         [0.0000, 0.0228, 0.3329, 0.0000, 0.0478],\n",
            "         [0.0000, 0.0000, 0.1493, 0.0000, 0.0000],\n",
            "         [0.2935, 0.0000, 0.3137, 0.0000, 0.2847],\n",
            "         [0.0695, 0.1525, 0.0000, 0.2125, 0.4925],\n",
            "         [0.1416, 0.1740, 0.3829, 0.2888, 0.2286],\n",
            "         [0.2189, 0.3646, 0.0644, 0.4812, 0.2370],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2106],\n",
            "         [0.0000, 0.1131, 0.3324, 0.1803, 0.2357],\n",
            "         [0.0000, 0.0000, 0.1797, 0.0000, 0.0000],\n",
            "         [0.4247, 0.0632, 0.3224, 0.1064, 0.2005],\n",
            "         [0.0112, 0.0179, 0.0000, 0.1417, 0.0246],\n",
            "         [0.7077, 0.6536, 0.5284, 0.3253, 0.4129],\n",
            "         [0.3301, 0.4980, 0.5588, 0.4857, 0.3004],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2032],\n",
            "         [0.4052, 0.3427, 0.4427, 0.4141, 0.1509],\n",
            "         [0.5981, 0.5244, 0.5187, 0.5752, 0.8598],\n",
            "         [0.0400, 0.0516, 0.0000, 0.1006, 0.1200],\n",
            "         [0.5870, 0.5056, 0.3746, 0.4540, 0.5654],\n",
            "         [0.0000, 0.0000, 0.0000, 0.2255, 0.0595],\n",
            "         [0.2843, 0.0000, 0.0537, 0.3127, 0.1922],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            " - Output shape after Max Pooling (Conv2): torch.Size([1, 32, 5]) (batch_size, out_channels=32, length=5)\n",
            "\n",
            "Flattening the tensor for Fully Connected layers\n",
            "Values after Flattening:\n",
            "tensor([[0.1900, 0.5145, 0.2462, 0.4513, 0.3531, 0.3741, 0.1959, 0.2320, 0.2645,\n",
            "         0.3761, 0.0000, 0.0000, 0.2205, 0.0000, 0.1104, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.4522, 0.4674, 0.5481, 0.4883, 0.4957, 0.4368, 0.2258,\n",
            "         0.5694, 0.7796, 0.5206, 0.7598, 0.4772, 0.6283, 1.1623, 0.7342, 0.3706,\n",
            "         0.4233, 0.3219, 0.1119, 0.3793, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6391, 0.6490, 0.4927, 0.9792, 0.5750, 0.2802, 0.4325, 0.1185, 0.3260,\n",
            "         0.4629, 0.0000, 0.0228, 0.3329, 0.0000, 0.0478, 0.0000, 0.0000, 0.1493,\n",
            "         0.0000, 0.0000, 0.2935, 0.0000, 0.3137, 0.0000, 0.2847, 0.0695, 0.1525,\n",
            "         0.0000, 0.2125, 0.4925, 0.1416, 0.1740, 0.3829, 0.2888, 0.2286, 0.2189,\n",
            "         0.3646, 0.0644, 0.4812, 0.2370, 0.0000, 0.0000, 0.0000, 0.0000, 0.2106,\n",
            "         0.0000, 0.1131, 0.3324, 0.1803, 0.2357, 0.0000, 0.0000, 0.1797, 0.0000,\n",
            "         0.0000, 0.4247, 0.0632, 0.3224, 0.1064, 0.2005, 0.0112, 0.0179, 0.0000,\n",
            "         0.1417, 0.0246, 0.7077, 0.6536, 0.5284, 0.3253, 0.4129, 0.3301, 0.4980,\n",
            "         0.5588, 0.4857, 0.3004, 0.0000, 0.0000, 0.0000, 0.0000, 0.2032, 0.4052,\n",
            "         0.3427, 0.4427, 0.4141, 0.1509, 0.5981, 0.5244, 0.5187, 0.5752, 0.8598,\n",
            "         0.0400, 0.0516, 0.0000, 0.1006, 0.1200, 0.5870, 0.5056, 0.3746, 0.4540,\n",
            "         0.5654, 0.0000, 0.0000, 0.0000, 0.2255, 0.0595, 0.2843, 0.0000, 0.0537,\n",
            "         0.3127, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            " - Shape after flattening: torch.Size([1, 160]) (batch_size, flattened size)\n",
            "Values after FC1:\n",
            "tensor([[-0.1706,  0.0052,  0.0226, -0.2940, -0.0880,  0.0989,  0.1494, -0.0023,\n",
            "         -0.2533,  0.1393,  0.0271,  0.3807, -0.3821, -0.0996,  0.1793,  0.2386,\n",
            "          0.4044, -0.2598,  0.1956, -0.0035, -0.2456, -0.1494, -0.0071,  0.2297,\n",
            "         -0.0451, -0.0833,  0.2144, -0.2508, -0.0391, -0.0618, -0.0364,  0.3523,\n",
            "         -0.2442,  0.1869, -0.2666,  0.1640,  0.0994, -0.0156, -0.3028,  0.0436,\n",
            "          0.1054, -0.2200,  0.1108,  0.1803,  0.0151, -0.0449, -0.1318, -0.1530,\n",
            "          0.1796, -0.1771,  0.1233, -0.1220,  0.0376,  0.1297, -0.1516, -0.0989,\n",
            "         -0.2041,  0.1442, -0.2150,  0.0129, -0.0132,  0.3964, -0.1814, -0.0247,\n",
            "         -0.2483, -0.4979,  0.2164, -0.1103, -0.3721,  0.3204,  0.0957,  0.3115,\n",
            "         -0.0257, -0.0727, -0.2343, -0.1545, -0.1048, -0.0301,  0.3424, -0.2433,\n",
            "         -0.0455,  0.2105,  0.0267,  0.0950, -0.2632,  0.0722, -0.5208,  0.1737,\n",
            "          0.0911, -0.2178, -0.1572, -0.1944,  0.2206, -0.3123, -0.0199, -0.0129,\n",
            "         -0.0394,  0.2681,  0.4621, -0.1046]], grad_fn=<AddmmBackward0>)\n",
            "Values after Dropout:\n",
            "tensor([[0.0000, 0.0104, 0.0000, 0.0000, 0.0000, 0.1978, 0.2988, 0.0000, 0.0000,\n",
            "         0.2787, 0.0541, 0.0000, 0.0000, 0.0000, 0.0000, 0.4772, 0.0000, 0.0000,\n",
            "         0.3913, 0.0000, 0.0000, 0.0000, 0.0000, 0.4594, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3738, 0.0000, 0.3280,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.2109, 0.0000, 0.2217, 0.3607, 0.0302,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0752, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0257, 0.0000, 0.7928, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.6229,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6847, 0.0000, 0.0000,\n",
            "         0.4209, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.1822, 0.0000,\n",
            "         0.0000, 0.0000, 0.4413, 0.0000, 0.0000, 0.0000, 0.0000, 0.5362, 0.9243,\n",
            "         0.0000]], grad_fn=<MulBackward0>) (note: some neurons will have values set to zero)\n",
            "Values after FC2 (Output):\n",
            "tensor([[ 0.0599,  0.0040, -0.1822, -0.0544, -0.1454, -0.1150,  0.0497, -0.0441,\n",
            "         -0.0004,  0.1602]], grad_fn=<AddmmBackward0>)\n",
            " - Output shape after FC2: torch.Size([1, 10]) (batch_size, neurons=10)\n",
            "\n",
            "Final Output Shape: torch.Size([1, 10])\n",
            "Final Output Values:\n",
            "tensor([[ 0.0599,  0.0040, -0.1822, -0.0544, -0.1454, -0.1150,  0.0497, -0.0441,\n",
            "         -0.0004,  0.1602]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- Dropout is applied before the fully connected layer to randomly deactivate 50% of the neurons during each forward pass. This helps the model generalize better to unseen data.\n",
        "- see the difference in the output for values after dropout.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "AGACajs02Waa"
      }
    }
  ]
}