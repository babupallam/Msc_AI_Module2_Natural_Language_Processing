{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOS4OfuQasRyA5qVzhIiXSW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction"],"metadata":{"id":"wy2Iv9pdYWUB"}},{"cell_type":"markdown","source":["Chunking is a critical task in Natural Language Processing (NLP) where the goal is to divide text into meaningful groups of tokens, also known as “chunks.” These chunks typically represent phrases like noun phrases (NP), verb phrases (VP), or prepositional phrases (PP). Unlike tokenization or part-of-speech (POS) tagging, which operates at the word level, chunking operates at a higher level, grouping words into syntactically correlated units that convey more semantic meaning.\n","\n"],"metadata":{"id":"TzqDKeplUS-s"}},{"cell_type":"markdown","source":["## 3.1 **Definition and Purpose of Chunking**\n","   - **Definition**:\n","     - Chunking is the process of segmenting and labeling multi-token sequences (chunks) in a sentence, such as noun phrases (NP), verb phrases (VP), and prepositional phrases (PP).\n","     - It focuses on identifying non-overlapping, contiguous sequences of words that form coherent phrases.\n","   \n","   - **Purpose**:\n","     - Chunking helps in breaking down sentences into smaller, semantically meaningful units, which is crucial for tasks like named entity recognition (NER), information extraction, and syntactic parsing.\n","     - It is simpler than full parsing but offers more syntactic information than tokenization or POS tagging alone.\n","\n","   - **Demonstration**:\n"],"metadata":{"id":"dyz_U16tUTEm"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample sentence\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the sentence\n","tokens = nltk.word_tokenize(sentence)\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Print the POS tags\n","print(\"POS Tags:\", pos_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNWOEbYiUuub","executionInfo":{"status":"ok","timestamp":1728831841131,"user_tz":-60,"elapsed":294,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"8d33ffd1-e0cd-478f-98ce-30b4c5777dd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","source":["## 3.2 **Types of Chunking**\n","   - **Noun Phrase Chunking (NP-Chunking)**:\n","     - Focuses on identifying noun phrases, which consist of a noun and its associated modifiers (e.g., adjectives, determiners).\n","     - NP-Chunking simplifies sentence structure into meaningful groups such as \"The quick brown fox.\"\n","     \n"],"metadata":{"id":"U9cJ4jBwUTH5"}},{"cell_type":"markdown","source":["  - **Demonstration**:\n"],"metadata":{"id":"Dt9u4QfFUTLC"}},{"cell_type":"code","source":["import nltk\n","\n","# Defining a simple rule for NP (Noun Phrase) chunking using regular expressions\n","chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # NP: determiner (optional), adjectives (optional), and noun\n","chunk_parser = nltk.RegexpParser(chunk_grammar)\n","\n","# Using previously defined POS tags for chunking\n","chunked_tree = chunk_parser.parse(pos_tags)\n","\n","# Print the chunked tree structure\n","print(chunked_tree)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkUB7dEnU15m","executionInfo":{"status":"ok","timestamp":1728833091422,"user_tz":-60,"elapsed":281,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"5c5d0269-1b31-42f5-8579-8484c875a923"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  over/IN\n","  (NP the/DT lazy/JJ dog/NN)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["   - **Verb Phrase Chunking (VP-Chunking)**:\n","     - Involves grouping verb-related components such as the main verb and its arguments (objects or complements).\n","     - This type of chunking is helpful in identifying actions and their corresponding entities.\n","\n","     - **Demonstration**:\n"],"metadata":{"id":"HJZQy_JiUTO0"}},{"cell_type":"code","source":["# Install NLTK\n","!pip install nltk\n","\n","# Import the NLTK library\n","import nltk\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')  # For tokenization\n","nltk.download('averaged_perceptron_tagger')  # For POS tagging\n","\n","# Sample sentence for demonstration\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the sentence\n","tokens = nltk.word_tokenize(sentence)\n","\n","# Generate POS tags for the tokens\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Defining a rule for VP (Verb Phrase) chunking\n","chunk_grammar_vp = \"VP: {<VB.*><NP|PP>*}\"  # VP: Verb followed by noun phrases (NP) or prepositional phrases (PP)\n","chunk_parser_vp = nltk.RegexpParser(chunk_grammar_vp)\n","\n","# Using the POS tags for chunking\n","chunked_tree_vp = chunk_parser_vp.parse(pos_tags)\n","\n","# Print the chunked tree structure for verb phrases\n","print(chunked_tree_vp)\n"],"metadata":{"id":"dDNCeCNsU5NK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833096013,"user_tz":-60,"elapsed":2584,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"4d1e94d5-20f4-41f0-8626-3f7ce7ee31c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","(S\n","  The/DT\n","  quick/JJ\n","  brown/NN\n","  fox/NN\n","  (VP jumps/VBZ)\n","  over/IN\n","  the/DT\n","  lazy/JJ\n","  dog/NN\n","  ./.)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","source":["   - **Prepositional Phrase Chunking (PP-Chunking)**:\n","     - PP-Chunking identifies prepositional phrases, which typically consist of a preposition and its noun phrase (e.g., \"over the lazy dog\").\n","     - Prepositional phrases provide information about relationships between entities, which can be crucial for tasks like relation extraction.\n","\n","     - **Demonstration**:\n"],"metadata":{"id":"pAFwESTcUTST"}},{"cell_type":"code","source":["import nltk\n","\n","# Defining a rule for PP (Prepositional Phrase) chunking\n","chunk_grammar_pp = \"PP: {<IN><NP>}\"  # PP: Preposition (IN) followed by a noun phrase (NP)\n","chunk_parser_pp = nltk.RegexpParser(chunk_grammar_pp)\n","\n","# Using the POS tags from earlier for chunking\n","chunked_tree_pp = chunk_parser_pp.parse(pos_tags)\n","\n","# Print the chunked tree structure for prepositional phrases\n","print(chunked_tree_pp)\n","\n"],"metadata":{"id":"iOOC_oSiU8iZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833098501,"user_tz":-60,"elapsed":274,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"70e5c586-2c71-416b-b18e-240280269d4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  The/DT\n","  quick/JJ\n","  brown/NN\n","  fox/NN\n","  jumps/VBZ\n","  over/IN\n","  the/DT\n","  lazy/JJ\n","  dog/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["## 3.3 **Techniques for Chunking**\n","   - **Rule-Based Chunking (Regular Expression-Based Chunking)**:\n","     - Rule-based chunking involves manually defining patterns based on POS tags to identify chunks. It uses grammar-like rules to specify the structure of phrases.\n","     - This technique is easy to implement and interpret, but it may not generalize well across different text corpora or domains.\n","   \n","   - **Chinking**:\n","     - Chinking is the inverse of chunking. Instead of specifying patterns to include in a chunk, chinking defines what to exclude from a chunk.\n","     - This is useful when the initial chunking rule is too inclusive and captures more than desired.\n","\n","     - **Demonstration**:\n","\n"],"metadata":{"id":"6P-j_TH3UTVH"}},{"cell_type":"code","source":["import nltk\n","\n","# Defining a rule for NP chunking with chinking (excluding certain parts)\n","chunk_grammar_with_chink = \"\"\"\n","NP: {<DT>?<JJ>*<NN>}  # Chunk determiners, adjectives, and nouns\n","}<VBZ>{  # Chink (exclude) verbs (VBZ) from the NP chunk\n","\"\"\"\n","chunk_parser_with_chink = nltk.RegexpParser(chunk_grammar_with_chink)\n","\n","# Using the POS tags from earlier for chunking with chinking\n","chunked_tree_with_chink = chunk_parser_with_chink.parse(pos_tags)\n","\n","# Print the chunked tree structure with chinking applied\n","print(chunked_tree_with_chink)\n","\n"],"metadata":{"id":"p78V7j2-VAk4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833110902,"user_tz":-60,"elapsed":371,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"ca213632-eba3-4551-8a52-b0e74e33a9c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  over/IN\n","  (NP the/DT lazy/JJ dog/NN)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["   - **Data-Driven Chunking**:\n","     - Data-driven chunking relies on machine learning models trained on annotated corpora to identify chunk boundaries.\n","     - Instead of defining rules manually, data-driven methods can learn patterns directly from data, making them more robust to variations in text.\n"],"metadata":{"id":"9U1ZkrksVCCc"}},{"cell_type":"markdown","source":["## 3.4 **Chunk Representation**\n","   - **IOB Tagging Format**:\n","     - The Inside-Outside-Beginning (IOB) format is commonly used to represent chunks in a sentence. Each word is labeled as either Inside (I), Outside (O), or Beginning (B) of a chunk.\n","     - This format is useful for training machine learning models that predict chunk boundaries.\n","\n","     - **Example**:\n","       - Sentence: \"The quick brown fox jumps over the lazy dog.\"\n","       - IOB Format:\n","         ```\n","         The    B-NP\n","         quick  I-NP\n","         brown  I-NP\n","         fox    I-NP\n","         jumps  B-VP\n","         over   B-PP\n","         the    B-NP\n","         lazy   I-NP\n","         dog    I-NP\n","         ```\n","\n","     - **Demonstration**:\n"],"metadata":{"id":"RNSovx0NUTaQ"}},{"cell_type":"code","source":["# Simple demonstration of assigning IOB tags manually\n","iob_tags = [\n","    ('The', 'B-NP'), ('quick', 'I-NP'), ('brown', 'I-NP'),\n","    ('fox', 'I-NP'), ('jumps', 'B-VP'), ('over', 'B-PP'),\n","    ('the', 'B-NP'), ('lazy', 'I-NP'), ('dog', 'I-NP')\n","]\n","\n","# Print each word along with its IOB tag\n","for word, tag in iob_tags:\n","    print(f\"{word}: {tag}\")\n"],"metadata":{"id":"FkczIpilVBpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833115826,"user_tz":-60,"elapsed":385,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"3c31912a-a1b8-4c0b-e62f-3cd416df8406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The: B-NP\n","quick: I-NP\n","brown: I-NP\n","fox: I-NP\n","jumps: B-VP\n","over: B-PP\n","the: B-NP\n","lazy: I-NP\n","dog: I-NP\n"]}]},{"cell_type":"markdown","source":["   - **Tree Representation**:\n","     - Chunking can also be represented using tree structures, which allow for the visualization of hierarchical relationships between chunks.\n","     - Tree representations are especially useful for linguists and NLP researchers as they visually depict the structure of a sentence.\n","\n"],"metadata":{"id":"fIoQPeWbVKdB"}},{"cell_type":"markdown","source":["## 3.5 **Developing and Evaluating Chunkers**\n","   - **Corpus-Based Development**:\n","     - Chunkers are often trained and evaluated using annotated corpora like the CoNLL-2000 chunking corpus, which provides POS-tagged and chunk-annotated sentences.\n","     - Data-driven chunkers can use supervised learning to predict chunk boundaries based on features extracted from the input data.\n","\n","   - **Evaluation Metrics**:\n","     - **Accuracy**: Measures the percentage of correctly predicted chunks.\n","     - **Precision**: Proportion of predicted chunks that are correct.\n","     - **Recall**: Proportion of actual chunks that are correctly predicted.\n","     - **F-Measure**: Harmonic mean of precision and recall, providing a balanced metric.\n","\n","     - **Demonstration**:\n","  "],"metadata":{"id":"25ws5BAdUTfD"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# True labels (ground truth IOB tags)\n","true_labels = ['B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP']\n","\n","# Predicted labels by the chunker\n","predicted_labels = ['B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP']\n","\n","# Generate and print the classification report (Precision, Recall, F1-score)\n","print(classification_report(true_labels, predicted_labels))\n"],"metadata":{"id":"ZOYIPXyYVTGk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833120974,"user_tz":-60,"elapsed":10,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"278d3782-2f42-44dc-d620-904c0e67d9fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        B-NP       1.00      1.00      1.00         2\n","        B-PP       1.00      1.00      1.00         1\n","        B-VP       1.00      1.00      1.00         1\n","        I-NP       1.00      1.00      1.00         5\n","\n","    accuracy                           1.00         9\n","   macro avg       1.00      1.00      1.00         9\n","weighted avg       1.00      1.00      1.00         9\n","\n"]}]},{"cell_type":"markdown","source":["\n","   - **Unigram and Bigram Chunkers**:\n","     - **Unigram Chunkers**: Use individual tokens and their POS tags as features to predict chunk boundaries.\n","     - **Bigram Chunkers**: Use pairs of consecutive tokens (bigrams) and their POS tags to capture local context for chunk prediction.\n","     - These models can be evaluated using standard corpora and metrics mentioned above.\n","\n"],"metadata":{"id":"vLDbkbELVToS"}},{"cell_type":"markdown","source":["## 3.6 **Creative Observations in Chunking**\n","\n","   - **Chunking as a Precursor to Full Parsing**:\n","     - Chunking can be seen as a lightweight alternative to full syntactic parsing. While chunking only identifies major phrase types (e.g., NP, VP), parsing determines the entire grammatical structure of a sentence.\n","     - In practice, chunking is computationally less expensive and faster, making it a viable solution for applications where full parsing may not be necessary.\n","\n","   - **Combining Chunking with Named Entity Recognition**:\n","     - Chunking can be integrated with Named Entity Recognition (NER) to extract meaningful entities from text. For example, NP chunks can serve as candidates for named entities, which can then be classified as PERSON, ORGANIZATION, or LOCATION.\n","     - This hybrid approach leverages the strengths of both chunking and NER.\n","\n","   - **Chunking for Relation Extraction**:\n","     - Chunking plays a vital role in relation extraction by grouping relevant entities together. Once noun phrases or verb phrases are chunked, they can be analyzed further to extract relationships between entities (e.g., \"John works at Microsoft\").\n","\n","   - **Impact of Domain-Specific Language on Chunking**:\n","     - Chunking performance can vary significantly across domains. For instance, legal or medical texts contain specialized terminology and complex sentence structures, which require customized chunking rules or domain-specific training data for optimal performance.\n"],"metadata":{"id":"tWBFAGsPUTiI"}},{"cell_type":"markdown","source":["# Observations"],"metadata":{"id":"H5X6p4O6Wz4i"}},{"cell_type":"markdown","source":["## 1 **Definition and Purpose of Chunking**\n","\n","- **Definition**: Chunking is the process of grouping tokens (words) into meaningful phrases based on syntactic patterns.\n","- **Purpose**: Chunking helps group individual tokens into semantically significant units like noun phrases (NP), verb phrases (VP), and prepositional phrases (PP).\n","\n","- **Code Demonstration**:\n","\n","This code demonstrates how to use NLTK to tokenize a sentence into individual words and assign part-of-speech (POS) tags to each token. The output will include a list of tuples where each tuple contains a word and its corresponding POS tag.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"1PxTxmtsUTlq"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sample sentence\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenizing the sentence into words\n","tokens = nltk.word_tokenize(sentence)\n","\n","# Assigning POS tags to each token\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Print the tokens with their corresponding POS tags\n","print(\"POS Tags:\", pos_tags)\n"],"metadata":{"id":"o8v1L0v9XOtc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833124526,"user_tz":-60,"elapsed":285,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"870ed17c-bde3-4624-df46-4e7746935b74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","source":["## 2 **Types of Chunking**\n","\n"],"metadata":{"id":"2CnbHHPKUTn5"}},{"cell_type":"markdown","source":["#### **3.2.1 Noun Phrase Chunking (NP-Chunking)**\n","\n","- **Goal**: Identify noun phrases based on POS tags.\n","  \n","- **Code Demonstration**:\n"],"metadata":{"id":"GgHuWH_VXWqj"}},{"cell_type":"code","source":["# Defining a grammar for NP-chunking\n","chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # NP: Determiner (optional), adjectives (optional), and noun\n","chunk_parser = nltk.RegexpParser(chunk_grammar)\n","chunked_tree = chunk_parser.parse(pos_tags)\n","\n","# Print the chunked tree structure\n","print(chunked_tree)\n"],"metadata":{"id":"lZB9a-1AXYjx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833128553,"user_tz":-60,"elapsed":262,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"32f9bd27-2370-4ca0-8043-140217314e4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  over/IN\n","  (NP the/DT lazy/JJ dog/NN)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["- **Explanation**:\n","  - The grammar `NP: {<DT>?<JJ>*<NN>}` specifies that a noun phrase (NP) can consist of:\n","    - An optional determiner (`<DT>`, e.g., \"the\"),\n","    - Zero or more adjectives (`<JJ>*`, e.g., \"quick\", \"brown\"),\n","    - Followed by a noun (`<NN>`, e.g., \"fox\").\n","  - This rule is applied to chunk parts of the sentence, grouping them as noun phrases.\n"],"metadata":{"id":"jGRqaxbVb4bp"}},{"cell_type":"markdown","source":["#### **3.2.2 Verb Phrase Chunking (VP-Chunking)**\n","\n","- **Goal**: Group together verb phrases based on POS tags and other syntactic clues.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"5S0EPeC6XWkV"}},{"cell_type":"code","source":["# Defining a grammar for VP-chunking\n","chunk_grammar_vp = \"VP: {<VB.*><NP|PP>*}\"  # VP: Verb followed by noun phrases or prepositional phrases\n","chunk_parser_vp = nltk.RegexpParser(chunk_grammar_vp)\n","chunked_tree_vp = chunk_parser_vp.parse(pos_tags)\n","\n","# Print the chunked tree structure\n","print(chunked_tree_vp)\n"],"metadata":{"id":"g07GiSQnXcA5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833132305,"user_tz":-60,"elapsed":258,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"b33dab3f-533d-4350-8f30-6b329cc348db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  The/DT\n","  quick/JJ\n","  brown/NN\n","  fox/NN\n","  (VP jumps/VBZ)\n","  over/IN\n","  the/DT\n","  lazy/JJ\n","  dog/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["- **Explanation**:\n","  - The grammar `VP: {<VB.*><NP|PP>*}` captures verb phrases (VP) by identifying:\n","    - Any verb (`<VB.*>`, such as `VB`, `VBZ`, `VBD`, etc.),\n","    - Followed by zero or more noun phrases (`<NP>`) or prepositional phrases (`<PP>`).\n","  - This rule helps to group verbs with their related noun or prepositional phrases, creating meaningful verb phrases from the sentence."],"metadata":{"id":"4gHLzmlzb_yV"}},{"cell_type":"markdown","source":["#### **3.2.3 Prepositional Phrase Chunking (PP-Chunking)**\n","\n","- **Goal**: Identify prepositional phrases.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"rBG3uNeDXWQA"}},{"cell_type":"code","source":["# Defining a grammar for PP-chunking\n","chunk_grammar_pp = \"PP: {<IN><NP>}\"  # PP: Preposition followed by a noun phrase\n","chunk_parser_pp = nltk.RegexpParser(chunk_grammar_pp)\n","chunked_tree_pp = chunk_parser_pp.parse(pos_tags)\n","\n","# Print the chunked tree structure\n","print(chunked_tree_pp)\n"],"metadata":{"id":"TR_uV54RXeol","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833136081,"user_tz":-60,"elapsed":347,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"4ea8971a-3740-4075-d88d-dcba6a989110"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  The/DT\n","  quick/JJ\n","  brown/NN\n","  fox/NN\n","  jumps/VBZ\n","  over/IN\n","  the/DT\n","  lazy/JJ\n","  dog/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["- **Explanation**:\n","  - The grammar `PP: {<IN><NP>}` captures **prepositional phrases (PP)** by identifying:\n","    - A preposition (`<IN>`, e.g., \"over\", \"in\", \"with\"),\n","    - Followed by a **noun phrase** (`<NP>`, e.g., \"the lazy dog\").\n","  - This rule allows the extraction of prepositional phrases from the sentence, such as \"over the lazy dog,\" grouping prepositions with their related noun phrases."],"metadata":{"id":"-Pfz7C8hcKk8"}},{"cell_type":"markdown","source":["## 3.3 **Techniques for Chunking**\n","\n","\n"],"metadata":{"id":"j-Q2Qpe8UTqD"}},{"cell_type":"markdown","source":["#### **3.3.1 Rule-Based Chunking (Regular Expression-Based Chunking)**\n","\n","- **Goal**: Use manually defined rules to identify chunks in a sentence.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"O1qdi4LKXkQy"}},{"cell_type":"code","source":["# Rule-based chunking using regular expressions\n","chunk_grammar_rule_based = \"\"\"\n","NP: {<DT>?<JJ>*<NN>}   # Noun Phrase: Optional determiner, adjectives, and noun\n","VP: {<VB.*><NP|PP>*}   # Verb Phrase: Verb followed by noun phrases or prepositional phrases\n","PP: {<IN><NP>}         # Prepositional Phrase: Preposition followed by noun phrase\n","\"\"\"\n","chunk_parser_rule_based = nltk.RegexpParser(chunk_grammar_rule_based)\n","chunked_tree_rule_based = chunk_parser_rule_based.parse(pos_tags)\n","\n","# Print the chunked tree structure\n","print(chunked_tree_rule_based)\n"],"metadata":{"id":"mimHToGkXk7O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833141802,"user_tz":-60,"elapsed":277,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"3e3a895e-94df-48b1-8b85-c3c84eb66a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  (VP jumps/VBZ)\n","  (PP over/IN (NP the/DT lazy/JJ dog/NN))\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["#### **3.3.2 Chinking**\n","\n","- **Goal**: Exclude certain tokens from chunks to refine chunk boundaries.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"E2BggnhoXpiE"}},{"cell_type":"code","source":["# Chunking with chinking: remove verbs from NP chunks\n","chunk_grammar_with_chink = \"\"\"\n","NP: {<DT>?<JJ>*<NN>}  # Chunk determiner, adjectives, and nouns\n","}<VBZ>{  # Exclude verbs (VBZ) from the NP chunk using chinking\n","\"\"\"\n","chunk_parser_with_chink = nltk.RegexpParser(chunk_grammar_with_chink)\n","chunked_tree_with_chink = chunk_parser_with_chink.parse(pos_tags)\n","\n","# Print the chunked tree structure with chinking applied\n","print(chunked_tree_with_chink)\n","\n"],"metadata":{"id":"F2FYH-q-Xp8m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833145190,"user_tz":-60,"elapsed":248,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"a894c9df-0104-4257-c1bb-189e6be5e0ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  over/IN\n","  (NP the/DT lazy/JJ dog/NN)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["#### **3.3.3 Data-Driven Chunking**\n","\n","- **Goal**: Use machine learning models to predict chunk boundaries.\n","\n","- **Code Demonstration Using NLTK**:\n"],"metadata":{"id":"xtXawXdMXsyk"}},{"cell_type":"code","source":["# Training a Unigram chunker on the CoNLL 2000 chunking corpus\n","import nltk\n","nltk.download('conll2000')\n","\n","from nltk.corpus import conll2000\n","from nltk.chunk.util import conlltags2tree\n","from nltk.chunk import ChunkParserI\n","from nltk.tag import UnigramTagger\n","\n","# Define a UnigramChunker class\n","class UnigramChunker(ChunkParserI):\n","    def __init__(self, train_sents):\n","        # Convert training data into a format suitable for the UnigramTagger\n","        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n","        self.tagger = UnigramTagger(train_data)  # Train a UnigramTagger on the chunk labels\n","\n","    def parse(self, sentence):\n","        # Extract POS tags from the sentence\n","        pos_tags = [pos for (word, pos) in sentence]\n","        # Use the trained UnigramTagger to predict chunk tags\n","        tagged_pos_tags = self.tagger.tag(pos_tags)\n","        # Combine words, POS tags, and chunk predictions into the format expected by conlltags2tree\n","        conll_tags = [(word, pos, chunk) for ((word, pos), (pos2, chunk)) in zip(sentence, tagged_pos_tags)]\n","        # Convert the tagged sentence into a chunk tree and return it\n","        return conlltags2tree(conll_tags)\n","\n","# Training the Unigram chunker on the CoNLL-2000 chunking corpus\n","train_sentences = conll2000.chunked_sents('train.txt')\n","unigram_chunker = UnigramChunker(train_sentences)\n","\n","# Test the chunker on a sample sentence\n","test_sentence = [(\"The\", \"DT\"), (\"quick\", \"JJ\"), (\"brown\", \"JJ\"), (\"fox\", \"NN\"),\n","                 (\"jumps\", \"VBZ\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"lazy\", \"JJ\"), (\"dog\", \"NN\")]\n","\n","# Output the chunk tree for the test sentence\n","print(unigram_chunker.parse(test_sentence))\n"],"metadata":{"id":"vokLciYSXtK6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833152691,"user_tz":-60,"elapsed":2820,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"1cd798ac-93e5-489c-9efa-622709881468"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n"]},{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/JJ fox/NN)\n","  (VP jumps/VBZ)\n","  (PP over/IN)\n","  (NP the/DT lazy/JJ dog/NN))\n"]}]},{"cell_type":"markdown","source":["## 3.4 **Chunk Representation**\n"],"metadata":{"id":"qYMt6S6dUTsd"}},{"cell_type":"markdown","source":["\n","#### **3.4.1 IOB Tagging Format**\n","\n","- **Goal**: Use the Inside-Outside-Beginning (IOB) format to label chunks.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"u6K0YrXFX1P9"}},{"cell_type":"code","source":["# Example IOB format for a sentence\n","iob_tags = [\n","    ('The', 'B-NP'), ('quick', 'I-NP'), ('brown', 'I-NP'),\n","    ('fox', 'I-NP'), ('jumps', 'B-VP'), ('over', 'B-PP'),\n","    ('the', 'B-NP'), ('lazy', 'I-NP'), ('dog', 'I-NP')\n","]\n","\n","# Print each word with its corresponding IOB tag\n","for word, tag in iob_tags:\n","    print(f\"{word}: {tag}\")\n"],"metadata":{"id":"16rOd-WDX8Py","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833152692,"user_tz":-60,"elapsed":11,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"f6ec5fe9-ed9f-495c-b7a5-063ffd3510ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The: B-NP\n","quick: I-NP\n","brown: I-NP\n","fox: I-NP\n","jumps: B-VP\n","over: B-PP\n","the: B-NP\n","lazy: I-NP\n","dog: I-NP\n"]}]},{"cell_type":"markdown","source":["- **Explanation**:\n","  - **IOB (Inside-Outside-Beginning)** tags are used to mark the boundaries of chunks in text.\n","    - **B-** (Beginning) indicates the beginning of a chunk.\n","    - **I-** (Inside) marks a token inside a chunk.\n","    - **O** (Outside) means the token is not part of any chunk.\n","  - For example, the noun phrase \"The quick brown fox\" is tagged with `B-NP` (Beginning of Noun Phrase) for \"The\" and `I-NP` (Inside Noun Phrase) for \"quick\", \"brown\", and \"fox\"."],"metadata":{"id":"uCooxWjLcr1X"}},{"cell_type":"markdown","source":["#### **3.4.2 Tree Representation**\n","\n","- **Goal**: Represent chunks as hierarchical trees for visualization.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"AfP5Td2sX0N9"}},{"cell_type":"code","source":["# Example tree structure for a chunked sentence\n","from nltk import Tree\n","\n","# Define a tree structure for the sentence \"The quick brown fox jumps over the lazy dog.\"\n","tree = Tree('S', [\n","    Tree('NP', [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN')]),  # Noun Phrase (NP)\n","    ('jumps', 'VBZ'),  # Verb\n","    Tree('PP', [('over', 'IN'),  # Prepositional Phrase (PP) with preposition 'over'\n","        Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])])  # Nested NP within the PP\n","])\n","\n","# Pretty-print the tree structure\n","tree.pretty_print()\n","\n"],"metadata":{"id":"PKVBsfiOX-YM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833157509,"user_tz":-60,"elapsed":477,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"cca8ec92-85ec-454f-e9dd-aa09b100429f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                     S                                     \n","     ________________________________|____________________                  \n","    |               |                                     PP               \n","    |               |                         ____________|_____            \n","    |               NP                       |                  NP         \n","    |        _______|________________        |       ___________|______     \n","jumps/VBZ The/DT quick/JJ brown/JJ fox/NN over/IN the/DT     lazy/JJ dog/NN\n","\n"]}]},{"cell_type":"markdown","source":["## 3.5 **Developing and Evaluating Chunkers**\n"],"metadata":{"id":"D23U4WyuUTu2"}},{"cell_type":"markdown","source":["\n","#### **3.5.1 Corpus-Based Development**\n","\n","- **Goal**: Train chunkers using annotated corpora like CoNLL-2000.\n","\n","- **Code Demonstration**:\n","\n"],"metadata":{"id":"SMJUCXToYAhX"}},{"cell_type":"code","source":["import nltk\n","nltk.download('conll2000')\n","\n","from nltk.corpus import conll2000\n","\n","# The conll2000.chunked_sents() function retrieves the chunked sentences from the dataset, where each sentence includes part-of-speech tags and chunk labels.\n","\n","# Load training and testing sentences from the CoNLL-2000 chunking corpus\n","train_sents = conll2000.chunked_sents('train.txt')\n","test_sents = conll2000.chunked_sents('test.txt')\n","\n","# train_sents[0] displays the first sentence from the training set with its chunk structure, showing how the text is annotated with phrases such as NP (noun phrase) and VP (verb phrase).\n","# Display the first sentence in the training set with chunk annotations\n","print(train_sents[0])\n"],"metadata":{"id":"6jaDNcPwYA-T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833160876,"user_tz":-60,"elapsed":333,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"96801e49-9379-48d9-e04a-7e15e7ad5b59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP Confidence/NN)\n","  (PP in/IN)\n","  (NP the/DT pound/NN)\n","  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n","  (NP another/DT sharp/JJ dive/NN)\n","  if/IN\n","  (NP trade/NN figures/NNS)\n","  (PP for/IN)\n","  (NP September/NNP)\n","  ,/,\n","  due/JJ\n","  (PP for/IN)\n","  (NP release/NN)\n","  (NP tomorrow/NN)\n","  ,/,\n","  (VP fail/VB to/TO show/VB)\n","  (NP a/DT substantial/JJ improvement/NN)\n","  (PP from/IN)\n","  (NP July/NNP and/CC August/NNP)\n","  (NP 's/POS near-record/JJ deficits/NNS)\n","  ./.)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["#### **3.5.2 Evaluation Metrics**\n","\n","- **Goal**: Evaluate chunkers using accuracy, precision, recall, and F1-score.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"UhUyEv0lYBXJ"}},{"cell_type":"code","source":["from nltk.chunk import conlltags2tree, tree2conlltags\n","from sklearn.metrics import classification_report\n","\n","# Convert a chunk tree to IOB tags\n","def tree_to_iob(tree):\n","    return tree2conlltags(tree)\n","\n","# Evaluate chunker performance\n","def evaluate_chunker(chunker, test_sents):\n","    # Convert gold standard and predicted chunk trees to IOB tags\n","    gold = [tree_to_iob(sent) for sent in test_sents]  # Gold standard IOB tags\n","    predictions = [tree_to_iob(chunker.parse(sent.leaves())) for sent in test_sents]  # Predicted IOB tags by the chunker\n","\n","    # Flatten the gold and predicted IOB tags to lists for evaluation\n","    gold_flat = [tag for sent in gold for _, _, tag in sent]  # Flatten gold standard tags\n","    pred_flat = [tag for sent in predictions for _, _, tag in sent]  # Flatten predicted tags\n","\n","    # Use classification_report to evaluate precision, recall, and F1-score\n","    return classification_report(gold_flat, pred_flat, labels=[\"B-NP\", \"I-NP\", \"O\"], zero_division=0)\n","\n","# Example evaluation on the Unigram chunker\n","print(evaluate_chunker(unigram_chunker, test_sents))\n"],"metadata":{"id":"JHjBHDe7YBvQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728833166558,"user_tz":-60,"elapsed":1378,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"167e42c9-ba77-4867-abab-523e5e1b1317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        B-NP       0.87      0.95      0.91     12422\n","        I-NP       0.97      0.86      0.91     14376\n","           O       0.86      0.83      0.85      8416\n","\n","   micro avg       0.91      0.89      0.90     35214\n","   macro avg       0.90      0.88      0.89     35214\n","weighted avg       0.91      0.89      0.90     35214\n","\n"]}]},{"cell_type":"markdown","source":["## 3.6 **Creative Observations in Chunking**\n","\n"],"metadata":{"id":"34K8H4T6Y84-"}},{"cell_type":"markdown","source":["#### **3.6.1 Multi-Tasking with Chunking and Named Entity Recognition (NER)**\n","\n","- **Observation**:\n","   - Chunking is often combined with Named Entity Recognition (NER) to improve entity extraction. Noun phrases (NPs) identified through chunking can serve as candidates for entity recognition, which can then classify these phrases as `PERSON`, `LOCATION`, `ORGANIZATION`, or other types.\n","   - This combination provides a more refined understanding of the text, allowing both syntactic (chunking) and semantic (NER) information to work together.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"bk8yR1GdY81d"}},{"cell_type":"code","source":["from nltk.chunk import ne_chunk\n","import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","# Use NLTK's ne_chunk to identify named entities in a sentence\n","sentence = [(\"Barack\", \"NNP\"), (\"Obama\", \"NNP\"), (\"was\", \"VBD\"), (\"born\", \"VBN\"),\n","            (\"in\", \"IN\"), (\"Hawaii\", \"NNP\")]\n","\n","# Perform named entity recognition (NER) and chunking on the POS-tagged sentence\n","named_entities_tree = ne_chunk(sentence)\n","\n","# Print the named entities tree structure\n","print(named_entities_tree)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMPKTYFtZpqv","executionInfo":{"status":"ok","timestamp":1728833170641,"user_tz":-60,"elapsed":832,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"b150cbd3-185b-41fc-b46f-83523d011ccb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Barack/NNP)\n","  (PERSON Obama/NNP)\n","  was/VBD\n","  born/VBN\n","  in/IN\n","  (GPE Hawaii/NNP))\n"]}]},{"cell_type":"markdown","source":["#### **3.6.2 Chunking for Relation Extraction**\n","\n","- **Observation**:\n","   - After chunking, relation extraction can be performed to identify the relationships between entities in a sentence. For example, once NP chunks (noun phrases) and VP chunks (verb phrases) are identified, the relation between entities such as \"John\" and \"Microsoft\" can be detected via a phrase like \"works at.\"\n","   - Chunking helps by simplifying sentences into basic components, making it easier to detect relationships.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"r0I9f0g5Y8yr"}},{"cell_type":"code","source":["import re\n","\n","# Sample text\n","text = \"John works at Microsoft in Seattle.\"\n","\n","# Define a regular expression pattern to extract person, organization, and location entities\n","pattern = r\"(?P<person>\\b[A-Z][a-z]+\\b) works at (?P<organization>\\b[A-Z][a-z]+\\b) in (?P<location>\\b[A-Z][a-z]+\\b)\"\n","\n","# Search for the pattern in the text\n","match = re.search(pattern, text)\n","\n","# If the pattern matches, extract and print the named entities\n","if match:\n","    print(\"Person:\", match.group(\"person\"))\n","    print(\"Organization:\", match.group(\"organization\"))\n","    print(\"Location:\", match.group(\"location\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdS4_MIMZrpG","executionInfo":{"status":"ok","timestamp":1728833177377,"user_tz":-60,"elapsed":248,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"161ce061-6173-4348-9224-ced03d1bd31b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Person: John\n","Organization: Microsoft\n","Location: Seattle\n"]}]},{"cell_type":"markdown","source":["- **Explanation**:\n","  - This example uses **regular expressions** for **relation extraction** by defining a pattern that captures:\n","    - A **person** (capitalized word like \"John\"),\n","    - An **organization** (capitalized word like \"Microsoft\"),\n","    - A **location** (capitalized word like \"Seattle\").\n","  - The regex pattern matches sentences like \"John works at Microsoft in Seattle\" and uses named capturing groups (`?P<name>`) to label different entities: **Person**, **Organization**, and **Location**.\n","  - This is a simple method for extracting structured relationships from text using patterns based on expected sentence structures."],"metadata":{"id":"lsim2TGtdTxI"}},{"cell_type":"markdown","source":["#### **3.6.3 Domain-Specific Chunking**\n","\n","- **Observation**:\n","   - Different domains, such as legal, medical, or financial texts, require customized chunking grammars due to variations in sentence structure and terminology. For example, legal texts may have longer, more complex noun phrases, while medical texts might involve specific terms like drug names or symptoms.\n","   - Adapting chunking grammars or training domain-specific models can significantly improve chunking performance in these specialized fields.\n","\n","- **Code Demonstration for Medical Text Chunking**:\n"],"metadata":{"id":"aVJgYeXwY8wB"}},{"cell_type":"code","source":["# Example chunking for medical text\n","import nltk\n","\n","# Medical text sample\n","medical_text = \"The patient was prescribed 20mg of Lisinopril for hypertension.\"\n","\n","# Tokenize and assign POS tags\n","medical_tokens = nltk.word_tokenize(medical_text)\n","medical_pos_tags = nltk.pos_tag(medical_tokens)\n","\n","# Customized NP and VP chunking for drug dosage and medical conditions\n","medical_chunk_grammar = r\"\"\"\n","NP: {<CD><NN><IN><NNP>}  # Noun Phrase: Dosage (CD), Unit (NN), Preposition (IN), Drug name (NNP)\n","VP: {<VBD><NP>}          # Verb Phrase: Verb (VBD) followed by a noun phrase (NP)\n","\"\"\"\n","\n","# Create a chunk parser with the defined grammar\n","medical_chunk_parser = nltk.RegexpParser(medical_chunk_grammar)\n","medical_chunked_tree = medical_chunk_parser.parse(medical_pos_tags)\n","\n","# Print and display the chunked tree\n","print(medical_chunked_tree)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_ZWKS2EZtd0","executionInfo":{"status":"ok","timestamp":1728833183570,"user_tz":-60,"elapsed":262,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"3a2d0091-b63c-46c7-f9a0-7d2cdfd0f8ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  The/DT\n","  patient/NN\n","  was/VBD\n","  prescribed/VBN\n","  20mg/CD\n","  of/IN\n","  Lisinopril/NNP\n","  for/IN\n","  hypertension/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["\n","- **Explanation**:\n","  - The chunking grammar is customized for **medical text**:\n","    - **NP**: Matches patterns like **\"20mg of Lisinopril\"**, where:\n","      - `<CD>` represents a cardinal number (e.g., **20mg**),\n","      - `<NN>` is a noun (e.g., **mg**),\n","      - `<IN>` is a preposition (e.g., **of**),\n","      - `<NNP>` is a proper noun (e.g., **Lisinopril**).\n","    - **VP**: Matches verb phrases like **\"was prescribed 20mg of Lisinopril\"** with a verb followed by a noun phrase.\n","  - This grammar helps capture domain-specific structures in **medical text** by identifying relationships between drug dosages and conditions. The result can be printed and visualized using **`draw()`**."],"metadata":{"id":"oZ0NQHCddeZF"}},{"cell_type":"markdown","source":["## 3.7 **Chunk Representation**\n","\n"],"metadata":{"id":"Yyh_XscFY8t3"}},{"cell_type":"markdown","source":["#### **3.7.1 IOB Tagging Format**\n","\n","- **Observation**:\n","   - The Inside-Outside-Beginning (IOB) format is widely used to represent chunked text, especially in datasets for training machine learning models. Each token is labeled as being inside (I), outside (O), or at the beginning (B) of a chunk.\n","   - The IOB format is useful for training sequence labeling models such as Hidden Markov Models (HMM) or Conditional Random Fields (CRF) for chunking tasks.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"-39coIIeY8qT"}},{"cell_type":"code","source":["from nltk.chunk import conlltags2tree, tree2conlltags\n","import nltk\n","\n","# Assuming medical_chunked_tree is the chunk tree you want to convert\n","# Replace 'medical_chunked_tree' with the actual name of your chunk tree variable if it's different\n","\n","# Convert chunk tree to IOB tags\n","iob_tags = tree2conlltags(medical_chunked_tree) # Changed chunked_tree_rule_based to medical_chunked_tree\n","print(iob_tags)\n","\n","# Convert IOB tags back to chunk tree\n","chunk_tree_from_iob = conlltags2tree(iob_tags)\n","print(chunk_tree_from_iob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtK98H6IZvfB","executionInfo":{"status":"ok","timestamp":1728833616284,"user_tz":-60,"elapsed":338,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"806358b7-2f3d-4991-c2c8-74a7d5357180"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT', 'O'), ('patient', 'NN', 'O'), ('was', 'VBD', 'O'), ('prescribed', 'VBN', 'O'), ('20mg', 'CD', 'O'), ('of', 'IN', 'O'), ('Lisinopril', 'NNP', 'O'), ('for', 'IN', 'O'), ('hypertension', 'NN', 'O'), ('.', '.', 'O')]\n","(S\n","  The/DT\n","  patient/NN\n","  was/VBD\n","  prescribed/VBN\n","  20mg/CD\n","  of/IN\n","  Lisinopril/NNP\n","  for/IN\n","  hypertension/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["#### **3.7.2 Tree Representation**\n","\n","- **Observation**:\n","   - Chunked sentences are often represented using tree structures, which visually depict the hierarchical relationships between different chunks. This tree representation is useful for linguists and NLP researchers to understand the syntactic structure of sentences.\n","   - NLTK provides functions to display tree structures, allowing for a clear visualization of chunking results.\n","\n","- **Code Demonstration**:\n"],"metadata":{"id":"FCON1R0pZQ1u"}},{"cell_type":"code","source":["from nltk import Tree\n","\n","# Example of creating a chunk tree manually\n","tree = Tree('S', [\n","    Tree('NP', [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN')]),  # Noun Phrase\n","    ('jumps', 'VBZ'),  # Verb\n","    Tree('PP', [('over', 'IN'),  # Prepositional Phrase\n","        Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])])  # Nested Noun Phrase within PP\n","])\n","\n","# Print a pretty representation of the tree structure\n","tree.pretty_print()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msnUR0HQZxa9","executionInfo":{"status":"ok","timestamp":1728833625162,"user_tz":-60,"elapsed":399,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7d7967e4-8d12-4750-f686-1039261084e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                     S                                     \n","     ________________________________|____________________                  \n","    |               |                                     PP               \n","    |               |                         ____________|_____            \n","    |               NP                       |                  NP         \n","    |        _______|________________        |       ___________|______     \n","jumps/VBZ The/DT quick/JJ brown/JJ fox/NN over/IN the/DT     lazy/JJ dog/NN\n","\n"]}]},{"cell_type":"markdown","source":["\n","- **Explanation**:\n","  - This code creates a **chunk tree** manually using NLTK's `Tree` class.\n","  - The tree structure includes:\n","    - A **Noun Phrase (NP)** consisting of a determiner (`DT`), adjectives (`JJ`), and a noun (`NN`).\n","    - A **Verb (VBZ)** for the action performed by the subject.\n","    - A **Prepositional Phrase (PP)** that includes a preposition (`IN`) followed by another **Noun Phrase (NP)**.\n","  - The methods **`pretty_print()`** and **`draw()`** are used to visualize the tree structure, making it easier to understand the relationships between different chunks and how they combine to form the overall sentence structure."],"metadata":{"id":"211N6j8jdvyB"}},{"cell_type":"markdown","source":["## 3.8 **Developing and Evaluating Chunkers**\n","\n"],"metadata":{"id":"VDXsNMxPZQ95"}},{"cell_type":"markdown","source":["#### **3.8.1 Training a Chunker with a Corpus**\n","\n","- **Observation**:\n","   - Chunkers can be trained using annotated corpora, such as the CoNLL-2000 chunking dataset, where each word is tagged with its corresponding chunk label. Supervised learning models like UnigramTagger, BigramTagger, or more advanced methods like Conditional Random Fields (CRF) can be trained to automatically predict chunk boundaries.\n","   - NLTK provides access to pre-annotated corpora, making it easy to train chunkers using real-world data.\n","\n","- **Code Demonstration (Training a Unigram Chunker)**:\n"],"metadata":{"id":"_tZxXMNnZV1E"}},{"cell_type":"code","source":["import nltk\n","nltk.download('conll2000')\n","\n","from nltk.corpus import conll2000\n","from nltk.chunk.util import conlltags2tree\n","from nltk.tag import UnigramTagger\n","\n","# Define a UnigramChunker class that inherits from ChunkParserI\n","class UnigramChunker(nltk.ChunkParserI):\n","    def __init__(self, train_sents):\n","        # Prepare training data in IOB format for the UnigramTagger\n","        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n","        self.tagger = UnigramTagger(train_data)  # Train a UnigramTagger on the chunk labels\n","\n","    def parse(self, sentence):\n","        # Extract POS tags from the sentence\n","        pos_tags = [pos for (word, pos) in sentence]\n","        # Use the trained UnigramTagger to predict chunk tags\n","        tagged_pos_tags = self.tagger.tag(pos_tags)\n","        # Combine words, POS tags, and chunk predictions into the format expected by conlltags2tree\n","        conll_tags = [(word, pos, chunk) for ((word, pos), (pos2, chunk)) in zip(sentence, tagged_pos_tags)]\n","        # Convert the tagged sentence into a chunk tree and return it\n","        return conlltags2tree(conll_tags)\n","\n","# Load training sentences from the CoNLL-2000 chunking corpus\n","train_sentences = conll2000.chunked_sents('train.txt')\n","# Initialize the UnigramChunker with the training data\n","unigram_chunker = UnigramChunker(train_sentences)\n","\n","# Test the chunker on a sample sentence\n","test_sentence = [(\"The\", \"DT\"), (\"quick\", \"JJ\"), (\"brown\", \"JJ\"), (\"fox\", \"NN\"),\n","                 (\"jumps\", \"VBZ\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"lazy\", \"JJ\"), (\"dog\", \"NN\")]\n","\n","# Output the chunk tree for the test sentence\n","print(unigram_chunker.parse(test_sentence))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7HL5XSaZ5sr","executionInfo":{"status":"ok","timestamp":1728833633183,"user_tz":-60,"elapsed":1703,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7e4d8030-8ee1-49db-ce1d-b7eedcdd9fa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/JJ fox/NN)\n","  (VP jumps/VBZ)\n","  (PP over/IN)\n","  (NP the/DT lazy/JJ dog/NN))\n"]}]},{"cell_type":"markdown","source":["#### **3.8.2 Evaluating Chunkers**\n","\n","- **Observation**:\n","   - After training a chunker, it is essential to evaluate its performance using metrics such as precision, recall, and F1-score. These metrics help determine how well the chunker identifies correct chunks, avoids false positives, and captures all\n","\n"," relevant chunks.\n","   - Evaluation is typically performed using a test set, which is separate from the training data.\n","\n","- **Code Demonstration (Evaluating a Chunker)**:\n"],"metadata":{"id":"LKm_zE-dZ3pA"}},{"cell_type":"code","source":["from nltk.chunk import conlltags2tree, tree2conlltags\n","from sklearn.metrics import classification_report\n","\n","# Convert a chunk tree to IOB tags\n","def tree_to_iob(tree):\n","    return tree2conlltags(tree)\n","\n","# Evaluate chunker performance\n","def evaluate_chunker(chunker, test_sents):\n","    # Convert the gold standard sentences to IOB tags\n","    gold = [tree_to_iob(sent) for sent in test_sents]\n","    # Generate predictions by parsing the test sentences\n","    predictions = [tree_to_iob(chunker.parse(sent.leaves())) for sent in test_sents]\n","\n","    # Flatten the lists to compare gold and predicted tags\n","    gold_flat = [tag for sent in gold for _, _, tag in sent]\n","    pred_flat = [tag for sent in predictions for _, _, tag in sent]\n","\n","    # Generate and return the classification report\n","    return classification_report(gold_flat, pred_flat, labels=[\"B-NP\", \"I-NP\", \"O\"], zero_division=0)\n","\n","# Example evaluation on the Unigram chunker using the CoNLL-2000 test set\n","test_sentences = conll2000.chunked_sents('test.txt')\n","print(evaluate_chunker(unigram_chunker, test_sentences))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goWWa1KGZzke","executionInfo":{"status":"ok","timestamp":1728833637747,"user_tz":-60,"elapsed":1323,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"eb8f1aea-4e1d-4a83-82fc-c1b7f8a5c6df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","        B-NP       0.87      0.95      0.91     12422\n","        I-NP       0.97      0.86      0.91     14376\n","           O       0.86      0.83      0.85      8416\n","\n","   micro avg       0.91      0.89      0.90     35214\n","   macro avg       0.90      0.88      0.89     35214\n","weighted avg       0.91      0.89      0.90     35214\n","\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"wYR__zMphP7s"}}]}