{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN91QcFup+ef8+3EjyfqOTQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Information Extraction (IE) is a crucial task in Natural Language Processing (NLP), focusing on transforming unstructured text data into structured formats. The goal is to identify relevant information from vast amounts of textual data, making it usable for querying, analysis, and various applications.\n","\n"],"metadata":{"id":"K-zVUe2cB8JR"}},{"cell_type":"markdown","source":["#### 1.1 **Definition and Goals of Information Extraction**\n","- **Definition**:\n","  - Information Extraction refers to the process of automatically extracting structured information, such as entities, relationships, and events, from unstructured text.\n","  - The extracted information typically fits into predefined categories like person names, organization names, dates, locations, and relationships between these entities.\n","\n","- **Goals**:\n","  - **Extracting Structured Data**: The primary aim is to convert free-form text into structured representations (e.g., database entries).\n","  - **Simplifying Text Analysis**: IE reduces the complexity of analyzing vast amounts of text by focusing on relevant information.\n","  - **Enabling Automated Processing**: Structured data extracted from text can be utilized for automated decision-making, data mining, and querying systems.\n","\n"],"metadata":{"id":"fvqLwtfrB8Pe"}},{"cell_type":"markdown","source":["- **Example Code**:\n"],"metadata":{"id":"r2Ypx4cgChpX"}},{"cell_type":"code","source":["# Simple demonstration of converting unstructured text to structured data using regex\n","import re  # Import the regular expression module\n","\n","text = \"John works at Microsoft in Seattle.\"  # Define the unstructured text\n","# Define the regex pattern with named capture groups for person, organization, and location\n","# (?P<name>...) captures the matched substring and assigns it to the group named 'name'\n","# \\b matches a word boundary\n","# [A-Z][a-z]+ matches one or more uppercase followed by lowercase letters\n","pattern = r\"(?P<person>\\b[A-Z][a-z]+\\b) works at (?P<organization>\\b[A-Z][a-z]+\\b) in (?P<location>\\b[A-Z][a-z]+\\b)\"\n","match = re.search(pattern, text)  # Search for the pattern in the text\n","\n","# If a match is found\n","if match:\n","    structured_data = match.groupdict()  # Extract the captured groups into a dictionary\n","    print(structured_data)  # Print the structured data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFGU75GACoWx","executionInfo":{"status":"ok","timestamp":1728675404403,"user_tz":-60,"elapsed":267,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"28501d9a-38fe-4e48-e666-a07f58842686"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'person': 'John', 'organization': 'Microsoft', 'location': 'Seattle'}\n"]}]},{"cell_type":"markdown","source":["#### 1.2 **Challenges in Information Extraction**\n"],"metadata":{"id":"fP2rkcelB8SN"}},{"cell_type":"markdown","source":["##### **1.2.1 Language Ambiguity**\n","   - **Challenge**: Language ambiguity occurs when a word or phrase has multiple meanings based on context. For example, \"Apple\" could refer to a fruit or the technology company.\n","   - **Solution**:\n","    - Use context to disambiguate the meaning of words.\n","    - Techniques such as Named Entity Recognition (NER) or context-aware models (e.g., BERT) can help identify the correct meaning based on surrounding text.\n","\n","   - **Code Demonstration**:\n"],"metadata":{"id":"38s_mbx5EXkN"}},{"cell_type":"code","source":["# Demonstrating how to handle ambiguous entities using regular expression patterns to detect context\n","import re\n","\n","# Input text that contains the ambiguous word \"Apple\"\n","text = \"Apple is planning to release a new iPhone. An apple a day keeps the doctor away.\"\n","\n","# Patterns for detecting the context in which the word \"Apple\" appears\n","patterns = [\n","    r\"(Apple) is planning to release\",  # Pattern to detect \"Apple\" in the context of a company\n","    r\"an (apple) a day keeps the doctor away\"  # Pattern to detect \"apple\" in the context of a fruit\n","]\n","\n","# Loop over each pattern to search for matches in the text\n","for pattern in patterns:\n","    # Use re.search() to find a match for the pattern in the input text\n","    # re.IGNORECASE allows for case-insensitive matching (e.g., \"Apple\" and \"apple\" are treated the same)\n","    match = re.search(pattern, text, re.IGNORECASE)\n","\n","    # If a match is found, extract the entity and determine the context\n","    if match:\n","        # match.group(1) extracts the first captured group from the matched pattern, which is the entity (\"Apple\" or \"apple\")\n","        entity = match.group(1)\n","\n","        # Determine the context based on the pattern that matched\n","        # If \"release\" appears in the pattern, interpret the entity as a \"company\"\n","        # Otherwise, interpret the entity as a \"fruit\"\n","        context = \"company\" if \"release\" in pattern else \"fruit\"\n","\n","        # Output the identified entity and the interpreted context\n","        print(f\"Entity: {entity}, Interpreted as: {context}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWx5H-n906US","executionInfo":{"status":"ok","timestamp":1728675767212,"user_tz":-60,"elapsed":416,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"9c7b57cd-225c-4310-c956-317c77f0cba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entity: Apple, Interpreted as: company\n","Entity: apple, Interpreted as: fruit\n"]}]},{"cell_type":"markdown","source":["##### **1.2.2 Complex Sentence Structures**\n","   - **Challenge**: Complex sentences with nested clauses, parenthetical expressions, or multiple entities can be difficult for information extraction algorithms to parse.\n","   - **Solution**:Use dependency parsing to understand the syntactic structure and relationships in the sentence.\n","\n","   - **Code Demonstration**:\n"],"metadata":{"id":"ECEHvosCEXhS"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy model for English\n","# \"en_core_web_sm\" is a small pre-trained model that includes vocabulary, syntax, entities, and word vectors for English\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Example sentence with a complex structure\n","text = \"John, who works at Google, lives in New York.\"\n","\n","# Parse the sentence using spaCy's NLP pipeline\n","# This includes tokenization, part-of-speech tagging, and dependency parsing\n","doc = nlp(text)\n","\n","# Extract and print entities and their relationships\n","# Iterate through each token (word) in the parsed sentence\n","for token in doc:\n","    # Check if the token is a subject or an object\n","    # \"nsubj\" indicates a nominal subject (the noun that performs the action)\n","    # \"dobj\" indicates a direct object (the noun that receives the action)\n","    if token.dep_ in (\"nsubj\", \"dobj\"):\n","        # Print the token (word), its dependency type, and the head word (verb it relates to)\n","        # token.text -> the actual word\n","        # token.dep_ -> the dependency label (e.g., \"nsubj\" for subject, \"dobj\" for object)\n","        # token.head.text -> the \"head\" word, which is usually the main verb of the clause\n","        print(f\"Token: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZMKCQT61Gpf","executionInfo":{"status":"ok","timestamp":1728675803102,"user_tz":-60,"elapsed":2640,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"5504a840-ec51-4df9-9b5a-2fd53a404116"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token: John, Dependency: nsubj, Head: lives\n","Token: who, Dependency: nsubj, Head: works\n"]}]},{"cell_type":"markdown","source":["- **Explanation**: The output shows how the dependencies between words are structured in the complex sentence, providing insight into which entities are related to actions or other entities.\n","\n"],"metadata":{"id":"4FGsrsQG1HHq"}},{"cell_type":"markdown","source":["##### **1.2.3 Domain-Specific Terminology**\n","   - **Challenge**: Specialized terms used in certain domains (e.g., medicine, finance) may not be recognized by general-purpose language models.\n","   - **Solution**: Use domain-specific models or fine-tune general-purpose models on domain-specific corpora.\n","\n","   - **Code Demonstration**:"],"metadata":{"id":"J9_nyHpOEXdD"}},{"cell_type":"code","source":["# Define a custom vocabulary for medical terms\n","# This list contains domain-specific terms related to medicine\n","medical_terms = [\"diabetes\", \"hypertension\", \"metformin\"]\n","\n","# Example text that contains some of the medical terms\n","text = \"The patient was prescribed metformin to manage his diabetes.\"\n","\n","# Check for domain-specific terms in the text\n","# Split the text into individual words and check if each word (converted to lowercase) is in the medical_terms list\n","recognized_terms = [word for word in text.split() if word.lower() in medical_terms]\n","\n","# Output the list of recognized medical terms found in the text\n","print(\"Recognized Medical Terms:\", recognized_terms)\n","# Expected Output: Recognized Medical Terms: ['metformin', 'diabetes']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCYJql9N1gFh","executionInfo":{"status":"ok","timestamp":1728675844130,"user_tz":-60,"elapsed":409,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"10ca87f5-a3a5-42e6-d36f-3c0463c5c041"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Recognized Medical Terms: ['metformin']\n"]}]},{"cell_type":"markdown","source":["##### **1.2.4 Inconsistency and Variability in Text**\n","   - **Challenge**: Human-generated text can vary significantly in grammar, style, and vocabulary, which makes standardization difficult.\n","   - **Solution**: Use normalization techniques such as lowercasing, stemming, and lemmatization to reduce variability. Additionally, use flexible models that generalize well across different styles.\n","\n","   - **Code Demonstration**:\n"],"metadata":{"id":"uq0FGKk1EXYm"}},{"cell_type":"code","source":["import re\n","\n","# Example texts with variations in grammar and spelling\n","texts = [\n","    \"Dr. Smith is a renowned cardiologist.\",\n","    \"Doctor Smith is an expert in the field of heart disease.\",\n","    \"Dr Smith is known for his work in cardiology.\"\n","]\n","\n","# Function to normalize text by handling variations in grammar and terminology\n","def normalize_text(text):\n","    # Convert the text to lowercase for case-insensitive matching\n","    text = text.lower()\n","\n","    # Use regular expressions to normalize different variations of \"Doctor\"\n","    # Replace occurrences of \"doctor\" or \"dr.\" (with a dot) with a consistent form \"dr\"\n","    text = re.sub(r\"doctor|dr\\.\", \"dr\", text)\n","\n","    # Normalize different terms referring to the same field of expertise\n","    # Replace occurrences of \"cardiology\" or \"heart disease\" with \"cardiology\"\n","    text = re.sub(r\"cardiology|heart disease\", \"cardiology\", text)\n","\n","    # Return the normalized text\n","    return text\n","\n","# Normalize each example text in the list\n","# Apply the normalize_text function to each string in the texts list\n","normalized_texts = [normalize_text(text) for text in texts]\n","\n","# Output the normalized versions of the texts\n","print(\"Normalized Texts:\")\n","for norm_text in normalized_texts:\n","    print(norm_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OY9eaN-41zZH","executionInfo":{"status":"ok","timestamp":1728675883164,"user_tz":-60,"elapsed":257,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"39fac00d-be3a-4817-d435-48601b835a17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Normalized Texts:\n","dr smith is a renowned cardiologist.\n","dr smith is an expert in the field of cardiology.\n","dr smith is known for his work in cardiology.\n"]}]},{"cell_type":"markdown","source":["   - **Explanation**: The normalization process reduces variability by converting text to lowercase, lemmatizing words, and removing stop words. However, inconsistencies in naming (\"Dr. Smith\" vs. \"Dr. John Smith, MD\") still present challenges.\n"],"metadata":{"id":"LgOzXWCC1zyI"}},{"cell_type":"markdown","source":["#### 1.3 **Applications of Information Extraction**\n"],"metadata":{"id":"PttHo8R5B8VC"}},{"cell_type":"markdown","source":["##### 1.3.1 **Business Intelligence**\n","   - **Objective**: Extract key insights from business documents, financial reports, or news articles to identify competitors, market trends, and risk factors.\n","   - **Approach**:\n","     - Use named entity recognition (NER) to identify entities such as companies, products, and locations.\n","     - Extract relevant phrases or sentences related to business events (e.g., mergers, acquisitions, product launches).\n","   - **Code Example**:\n"],"metadata":{"id":"DcBWyIISJEge"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy model for Named Entity Recognition (NER)\n","# \"en_core_web_sm\" is a pre-trained small English model for NLP tasks such as tokenization, NER, and dependency parsing\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Example business news text\n","text = \"\"\"\n","Apple announced a new partnership with Tesla to develop advanced battery technology.\n","The partnership is expected to revolutionize the electric vehicle market.\n","\"\"\"\n","\n","# Process the text using spaCy's NLP pipeline\n","# The nlp object applies several NLP tasks, including tokenization, part-of-speech tagging, NER, and dependency parsing\n","doc = nlp(text)\n","\n","# Extract named entities identified in the text\n","# Named entities are phrases identified by spaCy as representing specific entities (e.g., organizations, persons, locations)\n","business_entities = [(ent.text, ent.label_) for ent in doc.ents]\n","# Output the list of detected named entities and their labels\n","print(\"Named Entities:\", business_entities)\n","\n","# Extract relevant phrases using dependency parsing\n","# Loop through each sentence in the processed text (doc.sents)\n","# Check if the word \"partnership\" is present in the sentence (case-insensitively)\n","relevant_phrases = [sent.text for sent in doc.sents if \"partnership\" in sent.text.lower()]\n","# Output the sentences that mention \"partnership\"\n","print(\"Relevant Business Phrases:\", relevant_phrases)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztDtrBXgJFkT","executionInfo":{"status":"ok","timestamp":1728677054282,"user_tz":-60,"elapsed":5892,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"3b28fcaf-6c5f-41f9-d436-f93b9a23e6d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Named Entities: [('Apple', 'ORG'), ('Tesla', 'ORG')]\n","Relevant Business Phrases: ['Apple announced a new partnership with Tesla to develop advanced battery technology.\\n', 'The partnership is expected to revolutionize the electric vehicle market.\\n']\n"]}]},{"cell_type":"markdown","source":["##### 1.3.2 **Resume Parsing**\n","   - **Objective**: Automate the extraction of skills, experience, education, and contact details from resumes to streamline the recruitment process.\n","   - **Approach**:\n","     - Use pattern matching to identify key sections (e.g., \"Experience,\" \"Education\").\n","     - Extract specific details using regular expressions or custom NER models.\n","   - **Code Example**:\n"],"metadata":{"id":"KXLivrfsJGFE"}},{"cell_type":"code","source":["import re\n","\n","# Example resume text containing contact information, work experience, and education details\n","resume_text = \"\"\"\n","John Doe\n","Email: johndoe@example.com\n","Phone: +1-234-567-8901\n","\n","Experience:\n","- Software Engineer at Google (2018 - Present)\n","- Intern at Microsoft (2017 - 2018)\n","\n","Education:\n","- B.S. in Computer Science, MIT, 2017\n","\"\"\"\n","\n","# Regular expression patterns to extract contact details\n","\n","# Pattern to match the email address\n","# \"Email:\\s+\" matches the literal text \"Email:\" followed by one or more whitespace characters\n","# \"([\\w\\.-]+@[\\w\\.-]+)\" captures the email address itself, allowing alphanumeric characters, dots, and hyphens\n","email_pattern = r\"Email:\\s+([\\w\\.-]+@[\\w\\.-]+)\"\n","\n","# Pattern to match the phone number\n","# \"Phone:\\s+\" matches the literal text \"Phone:\" followed by one or more whitespace characters\n","# \"(\\+?\\d[\\d\\s-]{7,}\\d)\" captures the phone number, allowing an optional \"+\" sign, digits, spaces, and hyphens\n","phone_pattern = r\"Phone:\\s+(\\+?\\d[\\d\\s-]{7,}\\d)\"\n","\n","# Extract the email address from the resume text\n","email = re.search(email_pattern, resume_text).group(1)\n","# Extract the phone number from the resume text\n","phone = re.search(phone_pattern, resume_text).group(1)\n","\n","# Output the extracted email and phone number\n","print(\"Email:\", email)\n","print(\"Phone:\", phone)\n","\n","# Regular expression pattern to extract work experience\n","# \"Experience:\\s*\" matches the literal text \"Experience:\" followed by optional whitespace\n","# \"(.+?)\" captures any character (non-greedy match) until it encounters \"Education\" or the end of the string\n","# \"(?=Education|$)\" is a lookahead that stops matching before \"Education\" or at the end of the text\n","experience_pattern = r\"Experience:\\s*(.+?)(?=Education|$)\"\n","\n","# Extract the work experience section from the resume text\n","experience = re.search(experience_pattern, resume_text, re.DOTALL).group(1).strip()\n","# Output the extracted work experience\n","print(\"Work Experience:\", experience)\n","\n","# Regular expression pattern to extract education details\n","# \"Education:\\s*\" matches the literal text \"Education:\" followed by optional whitespace\n","# \"(.+)\" captures the rest of the text, including newline characters\n","education_pattern = r\"Education:\\s*(.+)\"\n","\n","# Extract the education section from the resume text\n","education = re.search(education_pattern, resume_text, re.DOTALL).group(1).strip()\n","# Output the extracted education details\n","print(\"Education:\", education)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jyjO9V0JFB-","executionInfo":{"status":"ok","timestamp":1728677080707,"user_tz":-60,"elapsed":274,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"e5eea9f3-78fd-480c-b80f-c802262cc1c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Email: johndoe@example.com\n","Phone: +1-234-567-8901\n","Work Experience: - Software Engineer at Google (2018 - Present)\n","- Intern at Microsoft (2017 - 2018)\n","Education: - B.S. in Computer Science, MIT, 2017\n"]}]},{"cell_type":"markdown","source":["##### 1.3.3 **Media Monitoring**\n","   - **Objective**: Track mentions of brands, companies, or public figures across news and social media to manage online reputation.\n","   - **Approach**:\n","     - Use NER to detect mentions of target entities (e.g., companies, people).\n","     - Perform sentiment analysis to understand the tone of the content.\n","   - **Code Example**:\n"],"metadata":{"id":"oKTK888VJIPY"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Example social media post about Tesla\n","post = \"\"\"\n","Tesla's new electric car is amazing! The features and performance are unmatched.\n","Elon Musk has really outdone himself this time.\n","\"\"\"\n","\n","# Perform Named Entity Recognition (NER) using spaCy\n","# The 'nlp' object is a spaCy NLP pipeline that processes the text\n","doc = nlp(post)\n","# Extract named entities identified in the text\n","# Each entity has a 'text' attribute (entity string) and a 'label_' attribute (entity type)\n","entities = [(ent.text, ent.label_) for ent in doc.ents]\n","# Output the list of detected entities and their labels\n","print(\"Entities Mentioned:\", entities)\n","\n","# Perform sentiment analysis using TextBlob\n","# TextBlob provides a simple API for common natural language processing tasks, including sentiment analysis\n","blob = TextBlob(post)\n","# Get the sentiment of the text\n","# 'blob.sentiment' returns a namedtuple with two attributes: 'polarity' and 'subjectivity'\n","# Polarity ranges from -1 (negative) to 1 (positive), while subjectivity ranges from 0 (objective) to 1 (subjective)\n","sentiment = blob.sentiment\n","# Output the sentiment analysis results\n","print(\"Sentiment Analysis:\", sentiment)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvOqhTc2JIPZ","executionInfo":{"status":"ok","timestamp":1728677100191,"user_tz":-60,"elapsed":259,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"00112d23-d691-49aa-d18d-0dc0a4a84827"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entities Mentioned: [('Tesla', 'ORG'), ('Elon Musk', 'WORK_OF_ART')]\n","Sentiment Analysis: Sentiment(polarity=0.3621212121212121, subjectivity=0.5181818181818182)\n"]}]},{"cell_type":"markdown","source":["##### 1.3.4 **Scientific Literature Extraction**\n","   - **Objective**: Extract data from scientific research papers in domains like biology or medicine, such as genes, proteins, and diseases.\n","   - **Approach**:\n","     - Use domain-specific NER models to detect entities like gene names or protein names.\n","     - Identify relationships or events (e.g., gene-protein interactions, disease associations).\n","   - **Code Example**:\n"],"metadata":{"id":"k1mSl2CsJIX7"}},{"cell_type":"code","source":["# Example text extracted from a scientific paper discussing cancer research\n","scientific_text = \"\"\"\n","The TP53 gene is a tumor suppressor protein that plays a crucial role in regulating cell division and preventing cancer.\n","Mutations in TP53 are found in many types of cancers, including breast and lung cancer.\n","\"\"\"\n","\n","# Recognize biological entities using spaCy's Named Entity Recognition (NER)\n","# The 'nlp' object processes the scientific text to identify entities\n","bio_entities = [(ent.text, ent.label_) for ent in nlp(scientific_text).ents]\n","# Output the list of biological entities and their labels detected by spaCy\n","print(\"Biological Entities:\", bio_entities)\n","\n","# Extract disease-gene relationships using regular expressions\n","\n","# Pattern to match the specific gene \"TP53\"\n","# \"\\b\" ensures that \"TP53\" is matched as a whole word (boundary on both sides)\n","gene_pattern = r\"\\bTP53\\b\"\n","\n","# Pattern to match references to diseases like \"cancer\" or \"tumor\"\n","# The pattern uses an alternation (|) to match either \"cancer\" or \"tumor\"\n","disease_pattern = r\"(cancer|tumor)\"\n","\n","# Find all occurrences of the gene name in the text using re.findall()\n","gene_mentions = re.findall(gene_pattern, scientific_text)\n","\n","# Find all occurrences of the disease terms in the text using re.findall()\n","disease_mentions = re.findall(disease_pattern, scientific_text)\n","\n","# Output the detected gene and disease mentions\n","print(\"Gene Mentions:\", gene_mentions)\n","print(\"Disease Mentions:\", disease_mentions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVcWGPUjJIX8","executionInfo":{"status":"ok","timestamp":1728677139656,"user_tz":-60,"elapsed":270,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"957be087-8700-4ab8-a966-6a96eb72787a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Biological Entities: []\n","Gene Mentions: ['TP53', 'TP53']\n","Disease Mentions: ['tumor', 'cancer', 'cancer', 'cancer']\n"]}]},{"cell_type":"markdown","source":["##### 1.3.5 **Email Filtering and Classification**\n","   - **Objective**: Identify spam or categorize emails based on content (e.g., urgent, informational).\n","   - **Approach**:\n","     - Use keyword matching or machine learning classifiers to categorize email content.\n","     - Extract specific details such as dates, sender information, or key actions.\n","   - **Code Example**:\n"],"metadata":{"id":"80nJdF09JId_"}},{"cell_type":"code","source":["# Example email content to be analyzed\n","email_content = \"\"\"\n","Subject: Meeting Reminder\n","Dear Team,\n","This is a reminder about the meeting scheduled for tomorrow, 15th October at 10 AM.\n","Please ensure that you bring the necessary documents.\n","Regards,\n","HR\n","\"\"\"\n","\n","# Simple keyword-based classification of the email content\n","# Convert the email content to lowercase to make the keyword search case-insensitive\n","if \"meeting\" in email_content.lower():\n","    # If the word \"meeting\" is found in the email, classify it as a \"Meeting\"\n","    category = \"Meeting\"\n","elif \"urgent\" in email_content.lower():\n","    # If the word \"urgent\" is found (and \"meeting\" is not), classify it as \"Urgent\"\n","    category = \"Urgent\"\n","else:\n","    # If neither \"meeting\" nor \"urgent\" is found, classify the email as \"General\"\n","    category = \"General\"\n","\n","# Output the classification result\n","print(\"Email Category:\", category)\n","\n","# Regular expression pattern to extract the meeting date from the email content\n","# The pattern matches:\n","#   - One or two digits for the day (\\d{1,2})\n","#   - Optional suffixes \"st\", \"nd\", \"rd\", or \"th\" (non-capturing group ?:)\n","#   - A space followed by a month name (capturing full month names, case-insensitively)\n","date_pattern = r\"\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b\"\n","\n","# Search for the date pattern in the email content\n","date_match = re.search(date_pattern, email_content)\n","\n","# If a match is found, extract the date string, otherwise set a default message\n","meeting_date = date_match.group(0) if date_match else \"No date found\"\n","\n","# Output the extracted meeting date or the default message if no date is found\n","print(\"Meeting Date:\", meeting_date)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUA67fVDJIeA","executionInfo":{"status":"ok","timestamp":1728677166960,"user_tz":-60,"elapsed":255,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"fbd491f8-1932-4974-acf4-377b86408f21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Email Category: Meeting\n","Meeting Date: 15th October\n"]}]},{"cell_type":"markdown","source":["##### 1.3.6 **Dialogue Act Classification**\n","   - **Objective**: Classify each line in a conversation based on the type of speech act (e.g., question, statement, command).\n","   - **Approach**:\n","     - Use rule-based methods, keyword matching, or machine learning classifiers to categorize dialogue acts.\n","     - Features like specific keywords, sentence structure, or punctuation can indicate different dialogue acts.\n","   - **Code Example**:\n"],"metadata":{"id":"zo40qagRJIhe"}},{"cell_type":"code","source":["# Example conversation lines to be classified\n","conversation_lines = [\n","    \"Can you send me the report by tomorrow?\",\n","    \"I will get it done.\",\n","    \"Please make sure to double-check the data.\",\n","    \"Why is the server down?\",\n","    \"Restart the server immediately.\"\n","]\n","\n","# Function to classify dialogue acts based on simple keyword and pattern matching\n","def classify_dialogue_act(line):\n","    # Check if the line ends with a question mark to classify it as a \"Question\"\n","    if line.endswith(\"?\"):\n","        return \"Question\"\n","    # Check if the line starts with \"please\" (case-insensitively) to classify it as a \"Command\"\n","    elif line.lower().startswith(\"please\"):\n","        return \"Command\"\n","    # Check for specific keywords (\"can\" or \"why\") to classify the line as a \"Question\"\n","    elif any(word in line.lower() for word in [\"can\", \"why\"]):\n","        return \"Question\"\n","    # Check if the line starts with \"I will\" to classify it as a \"Statement\"\n","    elif line.lower().startswith(\"i will\"):\n","        return \"Statement\"\n","    # Default classification for lines that don't match any of the above criteria\n","    else:\n","        return \"Command\"\n","\n","# Classify each line in the conversation\n","# Apply the classify_dialogue_act function to each line and store the results as a list of tuples\n","classified_lines = [(line, classify_dialogue_act(line)) for line in conversation_lines]\n","\n","# Display the classification results for each line in the conversation\n","for line, act in classified_lines:\n","    print(f\"Line: {line} | Classified as: {act}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TeWHttHjJIhe","executionInfo":{"status":"ok","timestamp":1728677229891,"user_tz":-60,"elapsed":266,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"e4d62619-e844-49f3-eaaa-0215e795e30d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Line: Can you send me the report by tomorrow? | Classified as: Question\n","Line: I will get it done. | Classified as: Statement\n","Line: Please make sure to double-check the data. | Classified as: Command\n","Line: Why is the server down? | Classified as: Question\n","Line: Restart the server immediately. | Classified as: Command\n"]}]},{"cell_type":"markdown","source":["##### 1.3.7 **Named Entity Recognition (NER)**\n","   - **Objective**: Identify and classify named entities in text, such as person names, organization names, locations, and dates.\n","   - **Approach**:\n","     - Use pre-trained language models or rule-based methods to detect entity boundaries and classify entity types.\n","     - Popular libraries like SpaCy and NLTK offer built-in support for NER.\n","   - **Code Example**:\n"],"metadata":{"id":"mZoHcyG2JIkh"}},{"cell_type":"code","source":["# Sample text containing various named entities\n","text = \"Barack Obama, the former president of the United States, was born in Hawaii.\"\n","\n","# Perform Named Entity Recognition (NER) using spaCy\n","# The 'nlp' object processes the text, identifying entities, parts of speech, and syntactic dependencies\n","doc = nlp(text)\n","\n","# Extract the named entities from the processed text\n","# Each entity has 'ent.text' (the entity itself) and 'ent.label_' (the entity type, such as PERSON, GPE, etc.)\n","ner_results = [(ent.text, ent.label_) for ent in doc.ents]\n","\n","# Display the named entities and their corresponding types\n","print(\"Named Entities and Types:\", ner_results)\n","# Expected Output: Named Entities and Types: [('Barack Obama', 'PERSON'), ('United States', 'GPE'), ('Hawaii', 'GPE')]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDeZHkOKJIki","executionInfo":{"status":"ok","timestamp":1728677259909,"user_tz":-60,"elapsed":252,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"9e0760b6-3efb-495c-c974-28cb8ecfa9d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Named Entities and Types: [('Barack Obama', 'PERSON'), ('the United States', 'GPE'), ('Hawaii', 'GPE')]\n"]}]},{"cell_type":"markdown","source":["##### 1.3.8 **Language Identification**\n","   - **Objective**: Detect the language of a given text snippet.\n","   - **Approach**:\n","     - Use language identification libraries like `langdetect` or `langid` to determine the language.\n","     - The models are typically trained on large corpora of multilingual text data.\n","   - **Code Example**:\n"],"metadata":{"id":"YHO_bFf4JInv"}},{"cell_type":"code","source":["!pip install langdetect # Install the langdetect module"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ5lon3bKlAc","executionInfo":{"status":"ok","timestamp":1728676788196,"user_tz":-60,"elapsed":8762,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"5229fd72-ebfe-4b23-e39c-634971f7cc7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/981.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=6f85ccff16ca8db394dff09ea63bb67308cc1c43389d1b499514cc3d1a8413b5\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["from langdetect import detect, detect_langs\n","\n","# Example text snippets in different languages\n","texts = [\n","    \"Bonjour tout le monde\",  # French\n","    \"Hello, how are you?\",    # English\n","    \"Hola, ¿cómo estás?\",     # Spanish\n","    \"Hallo, wie geht's dir?\"  # German\n","]\n","\n","# Detect the language for each text snippet\n","for text in texts:\n","    # Use the 'detect' function to identify the primary language of the text\n","    detected_language = detect(text)\n","\n","    # Use 'detect_langs' to get a list of possible languages with their probability scores\n","    probabilities = detect_langs(text)\n","\n","    # Output the detected language and the associated probabilities\n","    print(f\"Text: '{text}' | Detected Language: {detected_language} | Probabilities: {probabilities}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2We9hN1JInw","executionInfo":{"status":"ok","timestamp":1728677289043,"user_tz":-60,"elapsed":281,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"8d470ecf-3234-4df2-bc71-d02bcf28614d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: 'Bonjour tout le monde' | Detected Language: fr | Probabilities: [fr:0.999995251913927]\n","Text: 'Hello, how are you?' | Detected Language: en | Probabilities: [en:0.8571373215248994, cy:0.14286069879838512]\n","Text: 'Hola, ¿cómo estás?' | Detected Language: es | Probabilities: [es:0.9999948832584671]\n","Text: 'Hallo, wie geht's dir?' | Detected Language: af | Probabilities: [af:0.5714265680470725, de:0.4285727139680703]\n"]}]},{"cell_type":"markdown","source":["##### 1.3.9 **Spam Detection**\n","   - **Objective**: Classify emails or messages as spam or not spam based on their content.\n","   - **Approach**:\n","     - Use machine learning algorithms, such as Naive Bayes, to classify messages based on features such as word frequency, presence of specific keywords, or special characters (e.g., links).\n","     - Train a classifier using a labeled dataset of spam and non-spam emails.\n","   - **Code Example**:"],"metadata":{"id":"KTMshktwJJDg"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# Sample dataset of email messages with corresponding labels\n","# Labels: 1 = spam, 0 = not spam\n","emails = [\n","    \"Win a free iPhone now!\",  # spam\n","    \"Limited time offer, click here to claim your prize.\",  # spam\n","    \"Meeting scheduled for tomorrow\",  # not spam\n","    \"Don't miss out on our special discount\",  # spam\n","    \"Can we reschedule the call?\",  # not spam\n","    \"Congratulations, you've been selected!\"  # spam\n","]\n","labels = [1, 1, 0, 1, 0, 1]  # Corresponding labels indicating spam (1) or not spam (0)\n","\n","# Convert the text data into numerical features using Bag-of-Words representation\n","# CountVectorizer transforms the text into a matrix of token counts\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(emails)  # Fit the vectorizer to the email text and transform it into feature vectors\n","\n","# Split the dataset into training and testing sets\n","# 70% of the data is used for training, and 30% is used for testing\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n","\n","# Train a Naive Bayes classifier\n","# MultinomialNB is suitable for text classification tasks where the features represent word counts\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)  # Fit the classifier to the training data\n","\n","# Predict the labels of the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the accuracy of the classifier\n","# accuracy_score compares the predicted labels with the actual labels in the test set\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Output the predictions and the accuracy of the model\n","print(\"Predictions:\", y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R063epJEJJDh","executionInfo":{"status":"ok","timestamp":1728677310256,"user_tz":-60,"elapsed":269,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"631892ea-25c4-42a8-d2a6-b2d45beb8d4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: [0 0]\n","Accuracy: 0.0\n"]}]},{"cell_type":"markdown","source":["##### 1.3.10 **Textual Entailment Recognition**\n","   - **Objective**: Determine if one sentence logically entails another (i.e., whether the truth of one sentence implies the truth of another).\n","   - **Approach**:\n","     - Use language models or rule-based systems to evaluate whether the second sentence can be inferred from the first.\n","     - Machine learning models, such as BERT, can be fine-tuned on textual entailment datasets like the Stanford Natural Language Inference (SNLI) dataset.\n","   - **Code Example**:\n"],"metadata":{"id":"EtbKPtXnJzyr"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load a pre-trained model for textual entailment (Natural Language Inference - NLI)\n","# The model \"textattack/bert-base-uncased-snli\" is fine-tuned for NLI tasks using the SNLI (Stanford Natural Language Inference) dataset.\n","# It can classify relationships between a premise and a hypothesis as \"entailment,\" \"contradiction,\" or \"neutral.\"\n","nli_model = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-snli\")\n","\n","# Sample premise and hypothesis pairs to evaluate\n","# A premise is a statement that is assumed to be true.\n","premise = \"The cat is sitting on the mat.\"\n","# Hypothesis 1 is logically derived from the premise (i.e., it is expected to have \"entailment\").\n","hypothesis_1 = \"The mat has a cat on it.\"\n","# Hypothesis 2 introduces new information that contradicts or is unrelated to the premise (i.e., it is expected to have \"contradiction\").\n","hypothesis_2 = \"The dog is playing outside.\"\n","\n","# Predict the entailment relationship for each hypothesis using the NLI model\n","# The input format uses \"[SEP]\" to separate the premise and hypothesis for BERT models.\n","result_1 = nli_model(f\"{premise} [SEP] {hypothesis_1}\")\n","result_2 = nli_model(f\"{premise} [SEP] {hypothesis_2}\")\n","\n","# Output the model's predictions for each hypothesis\n","print(f\"Hypothesis 1: {result_1}\")\n","print(f\"Hypothesis 2: {result_2}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsQtV0HZJ0L_","executionInfo":{"status":"ok","timestamp":1728677347674,"user_tz":-60,"elapsed":1668,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"e9ceede5-bd0a-45df-c2ee-5e70249f4c3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Hypothesis 1: [{'label': 'LABEL_1', 'score': 0.5230224132537842}]\n","Hypothesis 2: [{'label': 'LABEL_0', 'score': 0.7244769930839539}]\n"]}]},{"cell_type":"markdown","source":["#### 1.4 **Core Tasks in Information Extraction**\n"],"metadata":{"id":"unnLpTaQB8YJ"}},{"cell_type":"markdown","source":["##### 1.4.1 **Entity Recognition**\n","   - **Objective**: Identify entities such as person names, organizations, locations, dates, and numerical values within text.\n","   - **Approach**: Use pre-trained models (e.g., SpaCy's built-in NER) or rule-based approaches to detect entities.\n","   - **Code Example**:\n"],"metadata":{"id":"nFbf8rPgNgjN"}},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy's pre-trained English model\n","# \"en_core_web_sm\" is a small, general-purpose English model that includes vocabulary, syntax, entities, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Example text for entity recognition\n","# The text contains various entities, such as a person's name, an organization, a date, and a location\n","text = \"Barack Obama, the former president of the United States, was born on August 4, 1961, in Honolulu, Hawaii.\"\n","\n","# Process the text with SpaCy's NLP pipeline\n","# The 'nlp' object will tokenize the text, perform part-of-speech tagging, and identify named entities\n","doc = nlp(text)\n","\n","# Extract entities and their corresponding labels from the processed text\n","# 'ent.text' provides the entity (e.g., \"Barack Obama\"), and 'ent.label_' gives its type (e.g., \"PERSON\")\n","entities = [(ent.text, ent.label_) for ent in doc.ents]\n","\n","# Output the recognized entities and their labels\n","# The expected result will identify \"Barack Obama\" as a PERSON, \"United States\" as a GPE (Geopolitical Entity),\n","# \"August 4, 1961\" as a DATE, and \"Honolulu, Hawaii\" as a GPE.\n","print(\"Entities and their labels:\", entities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wrj-Z87dNg5V","executionInfo":{"status":"ok","timestamp":1728677765266,"user_tz":-60,"elapsed":2799,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"d4be1f81-3901-4f39-ae57-22cef90740bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entities and their labels: [('Barack Obama', 'PERSON'), ('the United States', 'GPE'), ('August 4, 1961', 'DATE'), ('Honolulu', 'GPE'), ('Hawaii', 'GPE')]\n"]}]},{"cell_type":"markdown","source":["##### 1.4.2 **Relation Extraction**\n","   - **Objective**: Identify relationships between recognized entities, such as \"works at,\" \"located in,\" or \"married to.\"\n","   - **Approach**: Use dependency parsing or pattern-based matching to identify relationships between entities.\n","   - **Code Example**:\n"],"metadata":{"id":"sILaWDOHNhfk"}},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy's pre-trained English model\n","# \"en_core_web_sm\" is a small general-purpose English model that includes vocabulary, syntax, entities, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Example text for relation extraction\n","# The text contains a subject (\"Alice\"), an organization (\"Google\"), and a location (\"San Francisco\")\n","text = \"Alice works at Google in San Francisco.\"\n","\n","# Process the text with SpaCy's NLP pipeline\n","# The 'nlp' object will tokenize the text, perform part-of-speech tagging, dependency parsing, and named entity recognition\n","doc = nlp(text)\n","\n","# Extract named entities and their labels from the processed text\n","for ent in doc.ents:\n","    # Print each entity found in the text and its corresponding label\n","    # Example output: \"Entity: Alice, Label: PERSON\" for recognizing \"Alice\" as a PERSON\n","    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n","\n","# Identify relationships using dependency parsing\n","# Dependency parsing helps understand the grammatical structure by showing the relationships between words in a sentence\n","relationships = []\n","for token in doc:\n","    # Check if the token's dependency label indicates it is a nominal subject (\"nsubj\"),\n","    # a direct object (\"dobj\"), or a prepositional object (\"pobj\")\n","    if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\"):\n","        # Extract the subject (token's text), the associated verb (token's head), and the object\n","        subject = token.text\n","        verb = token.head.text\n","        # Get the object by finding the child token of the verb that is a prepositional object (\"pobj\")\n","        object = [child.text for child in token.head.children if child.dep_ == \"pobj\"]\n","        # If an object is found, add the subject, verb, and object as a relationship tuple\n","        if object:\n","            relationships.append((subject, verb, object[0]))\n","\n","# Output the extracted relationships\n","# The expected output might show relationships such as (\"Alice\", \"works\", \"Google\")\n","print(\"Extracted Relationships:\", relationships)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGh615A6Nhfk","executionInfo":{"status":"ok","timestamp":1728677793254,"user_tz":-60,"elapsed":3294,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7fbdf1f2-2b09-4542-88d1-203f11bf8b27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entity: Google, Label: ORG\n","Entity: San Francisco, Label: GPE\n","Extracted Relationships: [('Google', 'at', 'Google'), ('Francisco', 'in', 'Francisco')]\n"]}]},{"cell_type":"markdown","source":["##### 1.4.3 **Event Extraction**\n","   - **Objective**: Detect events described in the text and extract relevant attributes (who did what, where, and when).\n","   - **Approach**: Use rule-based methods, keyword detection, or pre-trained models to identify event-related phrases.\n","   - **Code Example**:\n"],"metadata":{"id":"Vxo4eUD0Nhkw"}},{"cell_type":"code","source":["import re\n","\n","# Example text describing an event\n","# The text includes details about the date, person involved, event type, and location\n","text = \"On January 20, 2021, Joe Biden was inaugurated as the 46th president of the United States in Washington, D.C.\"\n","\n","# Define regular expression patterns for extracting the date, person, event, and location\n","# The date pattern matches a month name followed by a day and year (e.g., \"January 20, 2021\")\n","date_pattern = r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b\"\n","\n","# The person pattern is set to match \"Joe Biden\"\n","# In a real-world scenario, this would be generalized for various person names, potentially using a named entity recognition (NER) approach\n","person_pattern = r\"\\bJoe Biden\\b\"\n","\n","# The event pattern matches the word \"inaugurated\"\n","event_pattern = r\"\\binaugurated\\b\"\n","\n","# The location pattern matches \"Washington, D.C.\"\n","# The backslash before the dot is used to escape it since dot has a special meaning in regex\n","location_pattern = r\"\\bWashington, D\\.C\\.\\b\"\n","\n","# Extract event details using regular expressions\n","# Search the text for a date match\n","date_match = re.search(date_pattern, text)\n","\n","# Search the text for a person match\n","person_match = re.search(person_pattern, text)\n","\n","# Search the text for an event match\n","event_match = re.search(event_pattern, text)\n","\n","# Search the text for a location match\n","location_match = re.search(location_pattern, text)\n","\n","# Create an event dictionary to store the extracted details\n","# Use 'group(0)' to get the matched text if a match is found; otherwise, set the value to \"N/A\"\n","event_details = {\n","    \"Date\": date_match.group(0) if date_match else \"N/A\",\n","    \"Person\": person_match.group(0) if person_match else \"N/A\",\n","    \"Event\": event_match.group(0) if event_match else \"N/A\",\n","    \"Location\": location_match.group(0) if location_match else \"N/A\"\n","}\n","\n","# Output the extracted event details\n","print(\"Extracted Event Details:\", event_details)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWp4B0ZsNhkx","executionInfo":{"status":"ok","timestamp":1728677821618,"user_tz":-60,"elapsed":263,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"24c44adc-120c-4db7-861b-30875f8d18cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted Event Details: {'Date': 'January 20, 2021', 'Person': 'Joe Biden', 'Event': 'inaugurated', 'Location': 'N/A'}\n"]}]},{"cell_type":"markdown","source":["##### 1.4.4 **Template Filling**\n","   - **Objective**: Populate predefined templates with extracted information, such as filling out a table with names, dates, and other details.\n","   - **Approach**: Use the output from previous steps (entity recognition and relation extraction) to map values to specific template slots.\n","   - **Code Example**:\n"],"metadata":{"id":"qD314GcbNhq5"}},{"cell_type":"code","source":["# Example of filling a template with extracted data\n","# The template is a dictionary where keys represent the categories to be filled, and initial values are set to None\n","template = {\n","    \"Person\": None,   # Placeholder for the name of the person involved in the event\n","    \"Date\": None,     # Placeholder for the date of the event\n","    \"Event\": None,    # Placeholder for the type of event\n","    \"Location\": None  # Placeholder for the event location\n","}\n","\n","# Use the extracted event details from the previous example\n","# The 'event_details' dictionary contains the extracted data from the text\n","template[\"Person\"] = event_details[\"Person\"]    # Fill the \"Person\" field with the extracted name\n","template[\"Date\"] = event_details[\"Date\"]        # Fill the \"Date\" field with the extracted date\n","template[\"Event\"] = event_details[\"Event\"]      # Fill the \"Event\" field with the extracted event type\n","template[\"Location\"] = event_details[\"Location\"] # Fill the \"Location\" field with the extracted location\n","\n","# Print the filled template to display the results\n","print(\"Filled Template:\")\n","for key, value in template.items():\n","    # Output each key-value pair in the template, showing the filled-in details\n","    print(f\"{key}: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSsu0FavNhq5","executionInfo":{"status":"ok","timestamp":1728677855587,"user_tz":-60,"elapsed":251,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7dc97b17-c6a0-4061-b746-f9735d99e3fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Filled Template:\n","Person: Joe Biden\n","Date: January 20, 2021\n","Event: inaugurated\n","Location: N/A\n"]}]},{"cell_type":"markdown","source":["#### 1.5 **Typical Information Extraction Pipeline**\n"],"metadata":{"id":"DOniaPScB8bd"}},{"cell_type":"markdown","source":["##### 1.5.1 **Preprocessing - Tokenization**\n","\n","###### Objective\n","Tokenization is the process of breaking down text into individual words or sub-words (tokens). This step is crucial for text analysis and processing.\n","\n","###### Code Example\n"],"metadata":{"id":"PT650BBJS3_e"}},{"cell_type":"code","source":["import nltk\n","\n","# Download the tokenizer resources if not already done\n","# 'punkt' is a pre-trained model in NLTK that helps in tokenizing text into words and sentences\n","nltk.download('punkt')\n","\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text to demonstrate tokenization\n","# The text contains technical terms and abbreviations to show how tokenization works on various word types\n","text = \"Natural Language Processing (NLP) is a subfield of artificial intelligence.\"\n","\n","# Tokenize the text into words\n","# word_tokenize splits the text into individual tokens (words and punctuation)\n","tokens = word_tokenize(text)\n","\n","# Display the original text and the resulting list of tokens\n","print(\"Original Text:\", text)\n","print(\"Tokens:\", tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lhxkx1IDS4Zc","executionInfo":{"status":"ok","timestamp":1728679258898,"user_tz":-60,"elapsed":1093,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"cacbd0da-8d8d-41d1-8693-b3f2ac25c618"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Original Text: Natural Language Processing (NLP) is a subfield of artificial intelligence.\n","Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.']\n"]}]},{"cell_type":"markdown","source":["##### 1.5.2 **Preprocessing - Sentence Segmentation**\n","\n","###### Objective\n","Sentence segmentation divides a text into individual sentences. This step is useful for sentence-level analysis, such as summarization or document classification.\n","\n","###### Code Example\n"],"metadata":{"id":"tfdUIAVWS5De"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","\n","# Sample text containing multiple sentences\n","# The text provides an example of how sentence segmentation works in breaking down a paragraph into individual sentences\n","text = \"Information extraction involves several steps. Tokenization is the first step. Then comes sentence segmentation.\"\n","\n","# Split the text into individual sentences using NLTK's sentence tokenizer\n","# sent_tokenize identifies sentence boundaries and segments the text into separate sentences\n","sentences = sent_tokenize(text)\n","\n","# Display the original text and the segmented sentences\n","print(\"Original Text:\", text)\n","print(\"Segmented Sentences:\", sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFdEJOQ1S5Df","executionInfo":{"status":"ok","timestamp":1728679275542,"user_tz":-60,"elapsed":260,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"188c2258-af0e-4fca-d8b5-b14c0a3c2e33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: Information extraction involves several steps. Tokenization is the first step. Then comes sentence segmentation.\n","Segmented Sentences: ['Information extraction involves several steps.', 'Tokenization is the first step.', 'Then comes sentence segmentation.']\n"]}]},{"cell_type":"markdown","source":["##### 1.5.3 **Part-of-Speech Tagging**\n","\n","###### Objective\n","Part-of-Speech (POS) tagging involves assigning grammatical roles (e.g., noun, verb, adjective) to each token in a sentence. POS tagging helps in understanding the syntactic structure of a sentence.\n","\n","###### Code Example\n"],"metadata":{"id":"rABv3JUnS5H3"}},{"cell_type":"code","source":["# Download the POS tagging resources if not already done\n","# 'averaged_perceptron_tagger' is a pre-trained model in NLTK used for part-of-speech tagging\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Example text for POS (Part-of-Speech) tagging\n","# The sentence contains a variety of word types to illustrate different POS tags\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the text into individual words\n","# word_tokenize splits the text into words and punctuation\n","tokens = word_tokenize(text)\n","\n","# Perform POS tagging on the tokenized words\n","# nltk.pos_tag assigns a part-of-speech tag to each token\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Display the tokens along with their corresponding POS tags\n","print(\"Tokens and POS Tags:\")\n","for token, pos in pos_tags:\n","    # Print each word and its associated POS tag\n","    print(f\"{token}: {pos}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfSvwpv9S5H3","executionInfo":{"status":"ok","timestamp":1728679295340,"user_tz":-60,"elapsed":810,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"6a2b5f0b-7e51-47f0-e665-9da9022e20fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Tokens and POS Tags:\n","The: DT\n","quick: JJ\n","brown: NN\n","fox: NN\n","jumps: VBZ\n","over: IN\n","the: DT\n","lazy: JJ\n","dog: NN\n",".: .\n"]}]},{"cell_type":"markdown","source":["##### 1.5.4 **Named Entity Recognition (NER)**\n","\n","###### Objective\n","Named Entity Recognition (NER) is used to identify entities (e.g., people, organizations, locations, dates) within a text. This step transforms unstructured text into structured information by categorizing words into predefined categories.\n","\n","###### Code Example\n"],"metadata":{"id":"n2QIfe6bS5Ot"}},{"cell_type":"code","source":["# Download the required NER resources if not already done\n","# 'maxent_ne_chunker' is a named entity chunker for NLTK, and 'words' is a list of English words used for NER\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","from nltk import ne_chunk\n","\n","# Example text containing named entities\n","# The sentence includes a person name (\"Barack Obama\"), a date (\"August 4, 1961\"), and a location (\"Honolulu, Hawaii\")\n","text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n","\n","# Tokenize the text into individual words\n","# word_tokenize splits the text into words and punctuation marks\n","tokens = word_tokenize(text)\n","\n","# Perform POS (Part-of-Speech) tagging on the tokenized words\n","# nltk.pos_tag assigns a POS tag to each word, which is necessary for NER\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Perform Named Entity Recognition (NER) using NLTK's ne_chunk\n","# ne_chunk takes POS-tagged tokens and identifies named entities, returning a tree structure with the results\n","ner_tree = ne_chunk(pos_tags)\n","\n","# Display the named entity recognition tree\n","# The output will show the hierarchical structure with labeled named entities\n","print(\"Named Entity Recognition Tree:\")\n","print(ner_tree)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZzkzWcjS5Ou","executionInfo":{"status":"ok","timestamp":1728679313160,"user_tz":-60,"elapsed":1084,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"86a25b24-f554-4e20-bb54-f9d8068f3d5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Named Entity Recognition Tree:\n","(S\n","  (PERSON Barack/NNP)\n","  (PERSON Obama/NNP)\n","  was/VBD\n","  born/VBN\n","  on/IN\n","  August/NNP\n","  4/CD\n","  ,/,\n","  1961/CD\n","  ,/,\n","  in/IN\n","  (GPE Honolulu/NNP)\n","  ,/,\n","  (GPE Hawaii/NNP)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["##### 1.5.5 **Relation Detection**\n","\n","###### Objective\n","Relation detection aims to identify relationships between named entities. For example, detecting that a person \"works at\" a specific organization. This step often involves pattern matching or dependency parsing to find relationships between entities.\n","\n","###### Code Example\n"],"metadata":{"id":"jlz8r2ofS5Xo"}},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy's pre-trained English model\n","# \"en_core_web_sm\" is a small, general-purpose English model that includes vocabulary, syntax, entities, and word vectors\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Example text containing named entities and a potential relationship\n","# The sentence includes a subject (\"Alice\"), an organization (\"Google\"), and a location (\"San Francisco\")\n","text = \"Alice works at Google in San Francisco.\"\n","\n","# Process the text with SpaCy's NLP pipeline\n","# The 'nlp' object will tokenize the text, perform part-of-speech tagging, dependency parsing, and named entity recognition\n","doc = nlp(text)\n","\n","# Extract named entities from the processed text\n","# 'ent.text' is the entity itself, and 'ent.label_' is the entity type (e.g., PERSON, ORG, GPE)\n","entities = [(ent.text, ent.label_) for ent in doc.ents]\n","# Display the recognized named entities and their corresponding labels\n","print(\"Named Entities:\", entities)\n","\n","# Detect relationships in the text using dependency parsing\n","# Dependency parsing helps identify grammatical relationships between words in a sentence\n","relationships = []\n","for token in doc:\n","    # Check if the token's dependency label indicates it is a subject (\"nsubj\"), direct object (\"dobj\"), or prepositional object (\"pobj\")\n","    if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\"):\n","        # Extract the subject (token's text), the associated verb (token's head), and the object\n","        subject = token.text\n","        verb = token.head.text\n","        # Find the prepositional object associated with the verb (if any)\n","        obj = [child.text for child in token.head.children if child.dep_ == \"pobj\"]\n","        # If a prepositional object is found, append the relationship as a tuple (subject, verb, object)\n","        if obj:\n","            relationships.append((subject, verb, obj[0]))\n","\n","# Display the detected relationships, which may include subject-verb-object or subject-verb-location structures\n","print(\"Detected Relationships:\", relationships)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDB6bX0cS5Xo","executionInfo":{"status":"ok","timestamp":1728679342240,"user_tz":-60,"elapsed":1966,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"073623dc-180b-41e0-a67f-5ea454a06577"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Named Entities: [('Google', 'ORG'), ('San Francisco', 'GPE')]\n","Detected Relationships: [('Google', 'at', 'Google'), ('Francisco', 'in', 'Francisco')]\n"]}]},{"cell_type":"markdown","source":["#### 1.6 **Creative Observations in Information Extraction**\n","- **Multi-Level Analysis is Crucial**:\n","  - IE benefits significantly from combining different levels of analysis (word, sentence, document).\n","  - Multi-level approaches can resolve ambiguities by leveraging context at various granularities.\n","\n","- **Integration with Knowledge Graph\n","\n","s**:\n","  - Linking extracted entities to external knowledge bases (e.g., Wikidata) can enhance the quality of IE.\n","  - Knowledge graphs can be used to validate relationships or infer new connections.\n","\n","- **Role of Pre-trained Language Models**:\n","  - Models like BERT and GPT can dramatically improve the accuracy of tasks such as NER and relation extraction.\n","  - These models capture rich contextual information, making them suitable for transfer learning in domain-specific applications.\n","\n","- **Human-in-the-Loop Approaches**:\n","  - Incorporating human feedback during IE (e.g., correcting entity recognition errors) can refine model performance.\n","  - Active learning techniques can be used to select the most informative examples for human annotation.\n","\n","- **Scalability Considerations**:\n","  - When dealing with large-scale text data, efficient algorithms and distributed computing frameworks (e.g., Apache Spark) are necessary.\n","  - Streaming data processing techniques enable real-time information extraction from continuous text flows (e.g., social media).\n","\n"],"metadata":{"id":"jbvyRy7gB8kB"}},{"cell_type":"markdown","source":["#### 1.7 **Demonstration of Creative Observations**\n"],"metadata":{"id":"sUFIWz20B8my"}},{"cell_type":"markdown","source":["#### 1.7.1 **Multi-Level Analysis is Crucial**\n","   - **Observation**: Combining different levels of analysis (word, sentence, document) can resolve ambiguities by providing context.\n","   - **Demonstration**:\n"],"metadata":{"id":"zvvoptYQB8p1"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# Download necessary resources for tokenization\n","# 'punkt' is a pre-trained model in NLTK that helps with sentence and word tokenization\n","nltk.download('punkt')\n","\n","# Sample text containing both \"Apple\" as a company and \"apple\" as a fruit\n","# This text helps demonstrate sentence segmentation and word tokenization\n","text = \"Apple announced a new iPhone. The apple tree in the backyard is blooming.\"\n","\n","# Sentence-level segmentation\n","# sent_tokenize splits the text into individual sentences\n","sentences = sent_tokenize(text)\n","print(\"Sentences:\", sentences)\n","\n","# Word-level tokenization for each sentence\n","# Loop through each sentence and tokenize it into words\n","for sentence in sentences:\n","    words = word_tokenize(sentence)  # word_tokenize splits the sentence into individual words and punctuation marks\n","    # Output the tokenized words for each sentence\n","    print(f\"Words in '{sentence}': {words}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rh8vOo7n6Vyy","executionInfo":{"status":"ok","timestamp":1728824021688,"user_tz":-60,"elapsed":400,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"1dd43478-b660-4a97-adda-7f66d59db5b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences: ['Apple announced a new iPhone.', 'The apple tree in the backyard is blooming.']\n","Words in 'Apple announced a new iPhone.': ['Apple', 'announced', 'a', 'new', 'iPhone', '.']\n","Words in 'The apple tree in the backyard is blooming.': ['The', 'apple', 'tree', 'in', 'the', 'backyard', 'is', 'blooming', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["  - **Explanation**:\n","    - The example demonstrates multi-level analysis by first performing sentence segmentation and then tokenizing each sentence into words.\n","    - This approach provides context at both sentence and word levels, which can help disambiguate terms like \"Apple\" (company vs. fruit).\n","\n"],"metadata":{"id":"gtSlPRw26nV3"}},{"cell_type":"markdown","source":["#### 1.7.2 **Integration with Knowledge Graphs**\n","   - **Observation**: Linking extracted entities to external knowledge bases (e.g., Wikidata) can enhance the quality of IE.\n","   - **Demonstration**:\n"],"metadata":{"id":"FZnwNmKvB8s8"}},{"cell_type":"code","source":["# Install the rdflib library for working with RDF data\n","!pip install rdflib\n","\n","from rdflib import Graph, Literal, RDF, URIRef\n","\n","# Create an RDF graph\n","# The RDF graph is a data structure that represents relationships between entities in a knowledge graph format\n","g = Graph()\n","\n","# Define some URIs for entities and relationships\n","# URIs represent the unique identifiers for entities or concepts. Here, we're defining URIs for \"Apple\", \"iPhone\", and \"produces\"\n","apple_uri = URIRef(\"http://example.org/Apple\")   # Represents the \"Apple\" entity (a company)\n","iphone_uri = URIRef(\"http://example.org/iPhone\") # Represents the \"iPhone\" entity (a product)\n","produces = URIRef(\"http://example.org/produces\") # Represents the \"produces\" relationship between Apple and iPhone\n","\n","# Add the relationship to the graph\n","# We're adding a triple (subject, predicate, object) that represents \"Apple produces iPhone\"\n","g.add((apple_uri, produces, iphone_uri))\n","\n","# Print the contents of the graph\n","print(\"Knowledge Graph:\")\n","# Loop through each triple (subject, predicate, object) in the RDF graph and print them\n","for subj, pred, obj in g:\n","    print(f\"{subj} {pred} {obj}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6ZS3qG26Rv-","executionInfo":{"status":"ok","timestamp":1728824051272,"user_tz":-60,"elapsed":5840,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"67b12ac9-9b42-469c-af23-574fbf1041bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.0.0)\n","Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.6.1)\n","Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n","Knowledge Graph:\n","http://example.org/Apple http://example.org/produces http://example.org/iPhone\n"]}]},{"cell_type":"markdown","source":["   - **Explanation**:\n","     - This example demonstrates how to integrate extracted entities into a knowledge graph using RDF.\n","     - Adding relationships like \"Apple produces iPhone\" to the graph allows linking structured data to external knowledge sources, which can then be used for reasoning or querying.\n","\n"],"metadata":{"id":"0OfVZVk26vmD"}},{"cell_type":"markdown","source":["#### 1.7.3 **Role of Pre-trained Language Models**\n","   - **Observation**: Models like BERT and GPT capture rich contextual information, improving tasks such as Named Entity Recognition (NER).\n","   - **Demonstration**:\n"],"metadata":{"id":"eSXCfwLCB8vX"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load a pre-trained Named Entity Recognition (NER) model\n","# We're using a BERT-based model (\"dbmdz/bert-large-cased-finetuned-conll03-english\") that has been fine-tuned on the CoNLL-03 dataset for English NER\n","# This model can recognize entities such as people, locations, organizations, and more\n","nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","\n","# Sample text for NER\n","# The text includes a person's name (\"Barack Obama\") and a location (\"Hawaii\"), which the model should recognize\n","text = \"Barack Obama was born in Hawaii.\"\n","\n","# Perform Named Entity Recognition (NER) on the sample text\n","# The model will identify named entities and classify them as PERSON, LOCATION, etc.\n","ner_results = nlp(text)\n","\n","# Output the NER results, which will include the recognized entities, their types, and their positions in the text\n","print(\"NER Results:\", ner_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6W8VKK7h6OsW","executionInfo":{"status":"ok","timestamp":1728824088583,"user_tz":-60,"elapsed":1883,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"fd11556f-c116-4767-8c81-61d47ba821b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["NER Results: [{'entity': 'I-PER', 'score': 0.9990103, 'index': 1, 'word': 'Barack', 'start': 0, 'end': 6}, {'entity': 'I-PER', 'score': 0.999342, 'index': 2, 'word': 'Obama', 'start': 7, 'end': 12}, {'entity': 'I-LOC', 'score': 0.99945, 'index': 6, 'word': 'Hawaii', 'start': 25, 'end': 31}]\n"]}]},{"cell_type":"markdown","source":["   - **Explanation**:\n","     - This example uses a pre-trained BERT model fine-tuned on an NER task to recognize entities in text.\n","     - The model identifies \"Barack Obama\" as a PERSON and \"Hawaii\" as a LOCATION, showing how pre-trained language models can improve IE accuracy.\n","\n"],"metadata":{"id":"GKo-tK8-B8y8"}},{"cell_type":"markdown","source":["#### 1.7.4 **Human-in-the-Loop Approaches**\n","   - **Observation**: Incorporating human feedback can refine model performance, especially for challenging cases.\n","   - **Demonstration**:\n"],"metadata":{"id":"AzpPiC5xB82h"}},{"cell_type":"code","source":["# Simulated NER results before human feedback\n","# The NER model incorrectly labels \"apple\" as an organization (ORG) when it should be a fruit (FRUIT)\n","ner_results = [(\"Apple\", \"ORG\"), (\"iPhone\", \"PRODUCT\"), (\"apple\", \"ORG\")]\n","\n","# Apply human feedback to correct the mistake\n","# The human feedback suggests that \"apple\" in lowercase should be labeled as \"FRUIT\", not \"ORG\"\n","corrected_results = [\n","    (entity, \"FRUIT\" if entity.lower() == \"apple\" and label == \"ORG\" else label)  # Check if the entity is \"apple\" (case-insensitive)\n","    for entity, label in ner_results  # Iterate over the original NER results\n","]\n","\n","# Output the corrected NER results\n","print(\"Corrected NER Results:\", corrected_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOp7gjwt65Wb","executionInfo":{"status":"ok","timestamp":1728824109732,"user_tz":-60,"elapsed":515,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"34875a9e-4e2f-4997-ec9e-cd0a4372c67e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Corrected NER Results: [('Apple', 'FRUIT'), ('iPhone', 'PRODUCT'), ('apple', 'FRUIT')]\n"]}]},{"cell_type":"markdown","source":["   - **Explanation**:\n","     - The example simulates a human-in-the-loop approach by manually correcting a mislabeling in NER output.\n","     - Human feedback helps to improve the accuracy of models by refining predictions, especially in ambiguous cases.\n"],"metadata":{"id":"6pBvMekF7B1F"}},{"cell_type":"markdown","source":["#### 1.7.5 **Scalability Considerations**\n","   - **Observation**: Efficient algorithms and distributed computing frameworks (e.g., Apache Spark) are necessary for large-scale text processing.\n","   - **Demonstration** (basic parallel processing with Python's `concurrent.futures`):\n"],"metadata":{"id":"2Y3GX5H-67K5"}},{"cell_type":"code","source":["from concurrent.futures import ProcessPoolExecutor\n","\n","# Function to simulate text processing (e.g., tokenization)\n","# This function simply splits the input text into individual words (tokens)\n","def process_text(text):\n","    return text.split()\n","\n","# Sample large dataset (a list of texts)\n","# We multiply the list by 1000 to simulate a larger dataset of texts for parallel processing\n","texts = [\"Apple releases a new iPhone.\", \"Google announces AI advancements.\", \"Tesla launches a new model.\"] * 1000\n","\n","# Use parallel processing to speed up the text processing\n","# ProcessPoolExecutor is used to distribute the text processing workload across multiple CPU cores\n","# This can significantly reduce the time needed for large datasets\n","with ProcessPoolExecutor() as executor:\n","    # executor.map applies the 'process_text' function to each text in 'texts' in parallel\n","    # The results are returned as a list, where each element corresponds to the tokenized version of the input text\n","    results = list(executor.map(process_text, texts))\n","\n","# Output the number of processed texts\n","# This shows that all 3000 text entries (3 original texts * 1000) were processed\n","print(\"Processed texts count:\", len(results))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8IxG4EQ69QC","executionInfo":{"status":"ok","timestamp":1728824139801,"user_tz":-60,"elapsed":1758,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7699e68c-4049-43ea-a933-b49ebaf1db3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed texts count: 3000\n"]}]},{"cell_type":"markdown","source":["   - **Explanation**:\n","     - The example uses parallel processing to handle a large dataset, demonstrating how scalability can be achieved for large-scale text processing.\n","     - While this is a simple example, more advanced frameworks like Apache Spark could be used for processing massive datasets.\n"],"metadata":{"id":"b1R0KFX567Hf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-nQlFaEH9EMP"}}]}