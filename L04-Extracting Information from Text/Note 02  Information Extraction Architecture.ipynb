{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPeApTHI0xzCcTis8qkPDsb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ca3b131ccb654972abb564eee95e2d23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8547c28a4d3e487cbbe7b76cd95ae877","IPY_MODEL_0d4fa4c511554b339c432b6c53146992","IPY_MODEL_9d85ff0041b74a648fe22cab1ae984bb"],"layout":"IPY_MODEL_75ccb3f349bc4d8f8f86277ed203a83e"}},"8547c28a4d3e487cbbe7b76cd95ae877":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7790ef142c4c453e8353c96a34dda675","placeholder":"​","style":"IPY_MODEL_5cefffb00ce6411cbb35bfec37281673","value":"tokenizer_config.json: 100%"}},"0d4fa4c511554b339c432b6c53146992":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49732c8b5ab5471cb7a66de1f198a264","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18ec3f0d694549da9ccc98e6f90ec53a","value":48}},"9d85ff0041b74a648fe22cab1ae984bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f872ff846814bd5bc74cf3f80946171","placeholder":"​","style":"IPY_MODEL_9b7a971f170e4917b24e86cd8dd03702","value":" 48.0/48.0 [00:00&lt;00:00, 890B/s]"}},"75ccb3f349bc4d8f8f86277ed203a83e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7790ef142c4c453e8353c96a34dda675":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cefffb00ce6411cbb35bfec37281673":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49732c8b5ab5471cb7a66de1f198a264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18ec3f0d694549da9ccc98e6f90ec53a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f872ff846814bd5bc74cf3f80946171":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b7a971f170e4917b24e86cd8dd03702":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12eca434498743c58750897a080d86fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1323f6ec5d6f44598518479255aa558f","IPY_MODEL_a78ef6b3c3f0492aa54bf7bb09dc364e","IPY_MODEL_3fbd3063f62e490cb5ea38b066b59d50"],"layout":"IPY_MODEL_2adab27d4cf84c3e95e8e1d613471aa2"}},"1323f6ec5d6f44598518479255aa558f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc5cc534c68449b980d441cc13972877","placeholder":"​","style":"IPY_MODEL_5f7170638d694fe3a30d1c7a5a0c61cf","value":"vocab.txt: 100%"}},"a78ef6b3c3f0492aa54bf7bb09dc364e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c347d9f9e5f4d08bd8ac35bd462e92b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22a3c25755654caea2541f4d646374be","value":231508}},"3fbd3063f62e490cb5ea38b066b59d50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c11af8396f54ff98d095906a27f0ca4","placeholder":"​","style":"IPY_MODEL_5f4ca99b4b3f46628d71cb9ee31d2459","value":" 232k/232k [00:00&lt;00:00, 2.12MB/s]"}},"2adab27d4cf84c3e95e8e1d613471aa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5cc534c68449b980d441cc13972877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7170638d694fe3a30d1c7a5a0c61cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c347d9f9e5f4d08bd8ac35bd462e92b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a3c25755654caea2541f4d646374be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c11af8396f54ff98d095906a27f0ca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f4ca99b4b3f46628d71cb9ee31d2459":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8eca9f23672b494c914b8ba5185e7077":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6535a6b0ee34e0886e828e96550aa79","IPY_MODEL_0152f01726d54d79bad6f10b4cb2f74d","IPY_MODEL_c826b1793d744a2792721b4b7395e07e"],"layout":"IPY_MODEL_a1707d2a0e514627b1553f78768ff2d9"}},"b6535a6b0ee34e0886e828e96550aa79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b19a002a8f84852b7e19bd07cdb7888","placeholder":"​","style":"IPY_MODEL_8dc2fc15eed84cf1835d3e42f46b3fe4","value":"tokenizer.json: 100%"}},"0152f01726d54d79bad6f10b4cb2f74d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33037e9dbbc14dc1b766fd829768aa43","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fc20f7c68734852ab3163ad9cbe0293","value":466062}},"c826b1793d744a2792721b4b7395e07e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e448119f47704243bcf283c5666fd650","placeholder":"​","style":"IPY_MODEL_49da645f3f2943e38c44e606d458e144","value":" 466k/466k [00:00&lt;00:00, 5.15MB/s]"}},"a1707d2a0e514627b1553f78768ff2d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b19a002a8f84852b7e19bd07cdb7888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc2fc15eed84cf1835d3e42f46b3fe4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33037e9dbbc14dc1b766fd829768aa43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fc20f7c68734852ab3163ad9cbe0293":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e448119f47704243bcf283c5666fd650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49da645f3f2943e38c44e606d458e144":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3af9acfc7854c44a210ec902a68087d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51985386d02c431284a49d179da0a67a","IPY_MODEL_d1749272d5c242ebbdcb9e2d3def7b2b","IPY_MODEL_3504ee1f77104a328f78a3463108a376"],"layout":"IPY_MODEL_46e3bb82ae064083ac7d327c35e07a4f"}},"51985386d02c431284a49d179da0a67a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_646771c6b0b4446199a56f33e1c4467f","placeholder":"​","style":"IPY_MODEL_019d9627cbd8442087c9e1f79259343a","value":"config.json: 100%"}},"d1749272d5c242ebbdcb9e2d3def7b2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe56a4f959864440b46453ba106031a6","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a8dc0699662418897f28c34ecece653","value":570}},"3504ee1f77104a328f78a3463108a376":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b6a7b6305624f46b8b5ab9bdc2a1a35","placeholder":"​","style":"IPY_MODEL_1839b48dd583437da126d50abc0ee0c2","value":" 570/570 [00:00&lt;00:00, 10.6kB/s]"}},"46e3bb82ae064083ac7d327c35e07a4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646771c6b0b4446199a56f33e1c4467f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"019d9627cbd8442087c9e1f79259343a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe56a4f959864440b46453ba106031a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8dc0699662418897f28c34ecece653":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b6a7b6305624f46b8b5ab9bdc2a1a35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1839b48dd583437da126d50abc0ee0c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9b192d5a50141f9beb802b6fd0e3693":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22ae5da1257d40ee8ffe343186547ece","IPY_MODEL_96370f063ae34c8fb7b01fcf6fd2d005","IPY_MODEL_992bc0c3ad8845deb334e6a27014d450"],"layout":"IPY_MODEL_ebfac05c3a564ae2931f253e7da516da"}},"22ae5da1257d40ee8ffe343186547ece":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cb9bc78b1a342968704332bb95b3cb5","placeholder":"​","style":"IPY_MODEL_40ded1bb35f14dbbb91566bb7e88283e","value":"tokenizer_config.json: 100%"}},"96370f063ae34c8fb7b01fcf6fd2d005":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb1763d9c70949c88a2957a23b58f1f5","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81cc023e09054f8c976266c3d33809b8","value":60}},"992bc0c3ad8845deb334e6a27014d450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa6d864e555044b78766d1ab2e0845b0","placeholder":"​","style":"IPY_MODEL_1aa4305391d146e483085aa5b1d08b0a","value":" 60.0/60.0 [00:00&lt;00:00, 1.03kB/s]"}},"ebfac05c3a564ae2931f253e7da516da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cb9bc78b1a342968704332bb95b3cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40ded1bb35f14dbbb91566bb7e88283e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb1763d9c70949c88a2957a23b58f1f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81cc023e09054f8c976266c3d33809b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa6d864e555044b78766d1ab2e0845b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aa4305391d146e483085aa5b1d08b0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"219fe10003634bdbbc3be79832850a82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88aed65a4e854a09b8ac32706746a70d","IPY_MODEL_161567eb39c94d0a9d2274d95b4bfd60","IPY_MODEL_92059f5a96d1472fbcde4809ac9df401"],"layout":"IPY_MODEL_f93eb3fc02694f7abb8310517994509f"}},"88aed65a4e854a09b8ac32706746a70d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01410e0644894052ba1b22e3a85653b3","placeholder":"​","style":"IPY_MODEL_d4318c33f839405cae9c76b35736fbfd","value":"vocab.txt: 100%"}},"161567eb39c94d0a9d2274d95b4bfd60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85bb4f9d713d4190989b2d52f62967d2","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_309afc32520e4e6da74ed54a13d0f491","value":213450}},"92059f5a96d1472fbcde4809ac9df401":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4e46b93cc204f01a4cfff9093a64a6a","placeholder":"​","style":"IPY_MODEL_b4665ff500c34f6f80655f1242eedd00","value":" 213k/213k [00:00&lt;00:00, 4.27MB/s]"}},"f93eb3fc02694f7abb8310517994509f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01410e0644894052ba1b22e3a85653b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4318c33f839405cae9c76b35736fbfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85bb4f9d713d4190989b2d52f62967d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309afc32520e4e6da74ed54a13d0f491":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4e46b93cc204f01a4cfff9093a64a6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4665ff500c34f6f80655f1242eedd00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24536663f76f47f0b48e54c93e8d0c24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_237d392e34cc4010a2376ac9c23300b4","IPY_MODEL_7f95bb949bf94eeab6294805c6964508","IPY_MODEL_5bed4e37a3614faab8f73ca5c5010472"],"layout":"IPY_MODEL_52dc42363c394d09bf9054fb546b7d65"}},"237d392e34cc4010a2376ac9c23300b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5d695284a844267bad097e266f2c02f","placeholder":"​","style":"IPY_MODEL_2a3d863e35584fb389d07ee37f4b2834","value":"config.json: 100%"}},"7f95bb949bf94eeab6294805c6964508":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62ae99a23a05460e9040a513170953f1","max":998,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c103972e019413395a2590e5a9e1ce2","value":998}},"5bed4e37a3614faab8f73ca5c5010472":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_461bb9c1f2194dbb9c60948258f76d8d","placeholder":"​","style":"IPY_MODEL_71c1a24699f94e798e68199fd1772b61","value":" 998/998 [00:00&lt;00:00, 22.4kB/s]"}},"52dc42363c394d09bf9054fb546b7d65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5d695284a844267bad097e266f2c02f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a3d863e35584fb389d07ee37f4b2834":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62ae99a23a05460e9040a513170953f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c103972e019413395a2590e5a9e1ce2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"461bb9c1f2194dbb9c60948258f76d8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71c1a24699f94e798e68199fd1772b61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03358f79c531439d8ace9cebbb59721b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8de2680e952846da81b8d3d55bee0761","IPY_MODEL_6fac98cc3bf6450a8f2ca6dd90afb032","IPY_MODEL_a98c8d5aec5146308def46fd80749597"],"layout":"IPY_MODEL_d5ac6d90a96a4529bf5acd97a3223252"}},"8de2680e952846da81b8d3d55bee0761":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a9560de2c4e4bebafd209e36af88960","placeholder":"​","style":"IPY_MODEL_07653687fdac40f19d95c452d379c738","value":"model.safetensors: 100%"}},"6fac98cc3bf6450a8f2ca6dd90afb032":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3394df11a24248dda215c4fe12244e82","max":1334400964,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5311fd86628247cab73d95c092450cad","value":1334400964}},"a98c8d5aec5146308def46fd80749597":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e301d63de18245ac9601510a50ec0e6a","placeholder":"​","style":"IPY_MODEL_b8b2521841e0404ba4f1e43e7a2bde0d","value":" 1.33G/1.33G [00:20&lt;00:00, 43.8MB/s]"}},"d5ac6d90a96a4529bf5acd97a3223252":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a9560de2c4e4bebafd209e36af88960":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07653687fdac40f19d95c452d379c738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3394df11a24248dda215c4fe12244e82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5311fd86628247cab73d95c092450cad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e301d63de18245ac9601510a50ec0e6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8b2521841e0404ba4f1e43e7a2bde0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["- Information Extraction (IE) architecture is the structured framework used to process unstructured text and extract meaningful entities, relationships, and events.\n","- This architecture typically follows a pipeline approach, where different stages in the pipeline are responsible for specific tasks such as\n","  tokenization,\n","  part-of-speech tagging,\n","  named entity recognition, and\n","  relation extraction.\n","- Each stage plays a critical role in ensuring that the text is processed accurately and that relevant information is extracted effectively.\n","\n"],"metadata":{"id":"OdZMzmgR9GXx"}},{"cell_type":"markdown","source":["# Discussion"],"metadata":{"id":"ocBeAslEDRgz"}},{"cell_type":"markdown","source":["## 2.1 **Overview of the Pipeline**\n","\n","- **Pipeline Definition**:\n","  - A sequence of stages or modules that sequentially process the text to extract structured information.\n","  - Each module is responsible for a specific NLP task (e.g., tokenization, tagging, entity recognition) and passes its output to the next stage.\n","  - This modularity allows for flexibility and scalability in handling different types of text and extraction tasks.\n","\n","- **Main Stages in the Pipeline**:\n","  - **Sentence Segmentation**: Dividing text into individual sentences.\n","  - **Tokenization**: Splitting sentences into tokens (words or phrases).\n","  - **Part-of-Speech (POS) Tagging**: Labeling tokens with grammatical categories (e.g., noun, verb).\n","  - **Named Entity Recognition (NER)**: Identifying and classifying named entities (e.g., person names, locations).\n","  - **Relation Detection**: Extracting relationships between identified entities (e.g., \"works at,\" \"located in\").\n","\n","- **Modularity**:\n","  - Each stage in the pipeline is designed as a separate module, making the pipeline adaptable to various IE tasks and domains.\n","  - Pipelines can be customized to include additional stages such as co-reference resolution or event detection based on the use case.\n","\n"],"metadata":{"id":"rrmdmvzI9GOV"}},{"cell_type":"markdown","source":["- **Example Code (Simple Pipeline Implementation)**:\n","\n"],"metadata":{"id":"XfPcfGyt9GK1"}},{"cell_type":"code","source":["import nltk\n","import spacy\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk import pos_tag, ne_chunk\n","from nltk.tree import Tree\n","\n","# Download necessary NLTK data\n","# 'punkt' is a tokenizer for breaking text into words and sentences.\n","nltk.download('punkt')\n","# 'averaged_perceptron_tagger' is used for part-of-speech (POS) tagging.\n","nltk.download('averaged_perceptron_tagger')\n","# 'maxent_ne_chunker' is used for Named Entity Recognition (NER) in NLTK.\n","nltk.download('maxent_ne_chunker')\n","# 'words' is a list of known English words needed for NER in NLTK.\n","nltk.download('words')\n","\n","# Load the spaCy model for advanced NER and relation detection.\n","# 'en_core_web_sm' is a small English model that includes pre-trained NER capabilities.\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample text for demonstration\n","# The text contains a person's name, organization, location, and a date, useful for NER tasks.\n","text = \"John Doe works at Google in Mountain View, California. He attended Stanford University in 2010.\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVOHwNj7_gDG","executionInfo":{"status":"ok","timestamp":1728828956371,"user_tz":-60,"elapsed":3055,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"f61deecb-ab77-4bf5-88ea-619b6b32e9f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## 2.2 **Sentence Segmentation**\n","\n","- **Definition**:\n","  - The process of breaking down a text document into individual sentences.\n","  - Sentence segmentation is crucial for downstream tasks like tokenization, part-of-speech tagging, and relation extraction, as many NLP tasks operate at the sentence level.\n","\n","- **Challenges**:\n","  - Handling punctuation marks that do not indicate sentence boundaries (e.g., periods in abbreviations like \"Dr.\" or \"U.S.\").\n","  - Dealing with quotations, parentheses, and other special symbols that complicate sentence boundaries.\n","\n","- **Techniques**:\n","  - **Rule-Based Approaches**: Using predefined rules to split text based on punctuation and capitalization.\n","  - **Statistical Methods**: Utilizing machine learning models trained on labeled sentence boundaries to predict the start and end of sentences.\n","\n","- **Creative Observations**:\n","  - Multi-lingual sentence segmentation: Sentence boundaries vary significantly across languages, requiring language-specific models or rules for effective segmentation.\n","  - In the context of informal or social media text, sentence segmentation becomes more difficult due to inconsistent punctuation.\n","\n","- **Example Code (Sentence Segmentation)**:\n"],"metadata":{"id":"cCPi-Zy09GIG"}},{"cell_type":"code","source":["def sentence_segmentation(text):\n","    # Use NLTK's sent_tokenize to split the input text into sentences\n","    sentences = sent_tokenize(text)\n","    return sentences\n","\n","# Example usage\n","# Call the sentence_segmentation function and pass the input text to it\n","sentences = sentence_segmentation(text)\n","\n","# Print the segmented sentences\n","print(\"Sentence Segmentation:\", sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugKYSVcd_cI9","executionInfo":{"status":"ok","timestamp":1728828973779,"user_tz":-60,"elapsed":292,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"d3eb2e79-a08d-439d-e804-a3f4b4fa4972"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Segmentation: ['John Doe works at Google in Mountain View, California.', 'He attended Stanford University in 2010.']\n"]}]},{"cell_type":"markdown","source":["## 2.3 **Tokenization**\n","\n","- **Definition**:\n","  - Tokenization is the process of splitting a sentence into individual words, subwords, or symbols, known as tokens.\n","  - It is the first step in most NLP tasks as it provides the basic units of text (words) that can be further processed.\n","\n","- **Challenges**:\n","  - Handling compound words, contractions, and multi-word expressions (e.g., \"New York\" vs. \"New\" and \"York\").\n","  - Tokenizing languages with different writing systems (e.g., Chinese characters vs. English words).\n","\n","- **Techniques**:\n","  - **Word Tokenization**: Splitting sentences based on spaces and punctuation.\n","  - **Subword Tokenization**: Breaking words into smaller meaningful units (e.g., using Byte-Pair Encoding for subword tokenization in BERT).\n","  - **Custom Tokenization**: Developing domain-specific tokenizers for technical or specialized texts.\n","\n","- **Creative Observations**:\n","  - Subword tokenization, as used in models like BERT, allows for better handling of rare or out-of-vocabulary words by decomposing them into more frequent subunits.\n","  - Tokenization for social media data often requires special handling of hashtags, mentions, and emoticons.\n","\n","- **Example Code (Word Tokenization)**:\n"],"metadata":{"id":"vvTvb0-B9GC7"}},{"cell_type":"code","source":["def tokenize(sentences):\n","    # Tokenize each sentence using NLTK's word_tokenize\n","    # word_tokenize splits sentences into individual words (tokens)\n","    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n","    return tokenized_sentences\n","\n","# Example usage\n","# Call the tokenize function to tokenize each sentence\n","tokenized_sentences = tokenize(sentences)\n","\n","# Print the list of tokenized sentences (each sentence is a list of words)\n","print(\"Tokenization:\", tokenized_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feFN1gcB_ZwF","executionInfo":{"status":"ok","timestamp":1728829001809,"user_tz":-60,"elapsed":286,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"d5449214-326e-4445-c17c-7da9fc04c80c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization: [['John', 'Doe', 'works', 'at', 'Google', 'in', 'Mountain', 'View', ',', 'California', '.'], ['He', 'attended', 'Stanford', 'University', 'in', '2010', '.']]\n"]}]},{"cell_type":"markdown","source":["## 2.4 **Part-of-Speech (POS) Tagging**\n","\n","- **Definition**:\n","  - POS tagging is the process of assigning a part-of-speech label (e.g., noun, verb, adjective) to each token in a sentence.\n","  - POS tags provide syntactic information that is valuable for subsequent NLP tasks such as entity recognition and parsing.\n","\n","- **Challenges**:\n","  - Words that can have multiple POS tags depending on context (e.g., \"run\" can be a noun or a verb).\n","  - Dealing with ambiguous or complex sentence structures.\n","\n","- **Techniques**:\n","  - **Rule-Based Tagging**: Using predefined grammar rules to assign POS tags.\n","  - **Statistical Tagging**: Utilizing machine learning models (e.g., Hidden Markov Models, Conditional Random Fields) trained on labeled POS data.\n","\n","- **Creative Observations**:\n","  - POS tagging is language-dependent; for morphologically rich languages (e.g., Finnish), POS tagging requires more sophisticated models to capture inflection and case.\n","  - POS tagging in informal text (e.g., tweets) requires handling of slang, abbreviations, and incomplete sentences.\n","\n","- **Example Code (POS Tagging)**:\n"],"metadata":{"id":"C20QKwqw9F9z"}},{"cell_type":"code","source":["def pos_tagging(tokenized_sentences):\n","    # Perform Part-of-Speech (POS) tagging on each tokenized sentence using NLTK's pos_tag\n","    # pos_tag assigns POS tags to each word (e.g., noun, verb, adjective)\n","    pos_tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n","    return pos_tagged_sentences\n","\n","# Example usage\n","# Call the pos_tagging function to apply POS tagging to each tokenized sentence\n","pos_tagged_sentences = pos_tagging(tokenized_sentences)\n","\n","# Print the POS-tagged sentences (each sentence is a list of tuples where each tuple contains a word and its POS tag)\n","print(\"POS Tagging:\", pos_tagged_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bK6ZbpHY_WVt","executionInfo":{"status":"ok","timestamp":1728829027783,"user_tz":-60,"elapsed":424,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"24ee075c-75b5-4691-a1d9-c4d51720c0c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["POS Tagging: [[('John', 'NNP'), ('Doe', 'NNP'), ('works', 'VBZ'), ('at', 'IN'), ('Google', 'NNP'), ('in', 'IN'), ('Mountain', 'NNP'), ('View', 'NNP'), (',', ','), ('California', 'NNP'), ('.', '.')], [('He', 'PRP'), ('attended', 'VBD'), ('Stanford', 'NNP'), ('University', 'NNP'), ('in', 'IN'), ('2010', 'CD'), ('.', '.')]]\n"]}]},{"cell_type":"markdown","source":["## 2.5 **Named Entity Recognition (NER)**\n","\n","- **Definition**:\n","  - NER is the task of identifying and classifying named entities in text into predefined categories such as persons, organizations, locations, dates, etc.\n","  - It is essential for extracting structured information about the key entities mentioned in the text.\n","\n","- **Challenges**:\n","  - Disambiguation of entity types (e.g., \"Washington\" as a person, location, or organization).\n","  - Handling multi-word named entities (e.g., \"New York City\" vs. \"York\").\n","  - Domain adaptation, where entity types may vary based on the field (e.g., biological entities vs. company names).\n","\n","- **Techniques**:\n","  - **Dictionary-Based Methods**: Matching tokens to a predefined dictionary of known entities.\n","  - **Statistical Models**: Using supervised machine learning models (e.g., CRFs, BiLSTM-CRF) trained on annotated NER corpora.\n","  - **Neural Networks**: Utilizing deep learning techniques like transformers (e.g., BERT) for state-of-the-art NER performance.\n","\n","- **Creative Observations**:\n","  - Pre-trained models like BERT offer improved performance by leveraging contextual embeddings, enabling the model to better capture ambiguous and rare entities.\n","  - NER in multi-lingual or domain-specific contexts (e.g., medical or legal domains) requires specialized training data to achieve high accuracy.\n","\n","- **Example Code (NER using NLTK)**:\n"],"metadata":{"id":"4I2gyRgr_B6I"}},{"cell_type":"code","source":["def ner_nltk(pos_tagged_sentences):\n","    # Perform Named Entity Recognition (NER) using NLTK's ne_chunk\n","    # ne_chunk takes POS-tagged sentences and returns a tree structure where named entities are identified\n","    named_entities = [ne_chunk(sentence) for sentence in pos_tagged_sentences]\n","    return named_entities\n","\n","# Example usage\n","# Call the ner_nltk function to extract named entities from POS-tagged sentences\n","ner_entities_nltk = ner_nltk(pos_tagged_sentences)\n","\n","# Print the named entities recognized by NLTK (in the form of a tree structure for each sentence)\n","print(\"NER (NLTK):\", ner_entities_nltk)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKQxf-Of_S0L","executionInfo":{"status":"ok","timestamp":1728829051824,"user_tz":-60,"elapsed":284,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"fe4c3c00-a959-446b-84d5-282683034e96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NER (NLTK): [Tree('S', [Tree('PERSON', [('John', 'NNP')]), Tree('ORGANIZATION', [('Doe', 'NNP')]), ('works', 'VBZ'), ('at', 'IN'), Tree('ORGANIZATION', [('Google', 'NNP')]), ('in', 'IN'), Tree('GPE', [('Mountain', 'NNP'), ('View', 'NNP')]), (',', ','), Tree('GPE', [('California', 'NNP')]), ('.', '.')]), Tree('S', [('He', 'PRP'), ('attended', 'VBD'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), ('2010', 'CD'), ('.', '.')])]\n"]}]},{"cell_type":"markdown","source":["- **Example Code (NER with SpaCy)**:\n"],"metadata":{"id":"0o0EuhQNAWfk"}},{"cell_type":"code","source":["def ner_spacy(text):\n","    # Process the input text with spaCy's NLP pipeline\n","    # The pipeline will perform tokenization, POS tagging, and Named Entity Recognition (NER)\n","    doc = nlp(text)\n","\n","    # Extract named entities from the processed text\n","    # For each entity, return its text and label (e.g., PERSON, ORG, GPE)\n","    return [(ent.text, ent.label_) for ent in doc.ents]\n","\n","# Example usage\n","# Call the ner_spacy function to extract named entities from the input text using spaCy\n","ner_entities_spacy = ner_spacy(text)\n","\n","# Print the named entities recognized by spaCy (as tuples of entity text and entity label)\n","print(\"NER (spaCy):\", ner_entities_spacy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlzTTOwyAXLz","executionInfo":{"status":"ok","timestamp":1728829077141,"user_tz":-60,"elapsed":384,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"e82e7f58-fe08-488c-fba8-2310e1cb7823"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NER (spaCy): [('John', 'PERSON'), ('Google', 'ORG'), ('Mountain View', 'GPE'), ('California', 'GPE'), ('Stanford University', 'ORG'), ('2010', 'DATE')]\n"]}]},{"cell_type":"markdown","source":["## 2.6 **Relation Detection**\n","\n","- **Definition**:\n","  - Relation detection involves identifying relationships\n","\n"," between the extracted entities (e.g., \"works for,\" \"located in\").\n","  - The goal is to link named entities through their interactions or roles in a sentence.\n","\n","- **Challenges**:\n","  - Ambiguity in identifying the correct relation (e.g., \"John works at Google\" vs. \"John visited Google\").\n","  - Capturing implicit or complex relationships that are not explicitly stated in the text.\n","\n","- **Techniques**:\n","  - **Rule-Based Approaches**: Using regular expressions and syntactic patterns to identify relations between entities.\n","  - **Statistical Approaches**: Using machine learning models trained on relation-annotated corpora to predict relationships.\n","  - **Neural Approaches**: Leveraging deep learning models (e.g., dependency parsing combined with transformers) to extract complex relations.\n","\n","- **Creative Observations**:\n","  - Relation extraction in domain-specific texts, such as legal or biomedical documents, requires specialized knowledge and relation schemas to ensure high accuracy.\n","  - Pre-trained language models can significantly improve relation extraction by capturing the complex, context-dependent interactions between entities.\n","\n","- **Example Code (Relation Detection)**:\n"],"metadata":{"id":"8Dz8n1Aw_Bz9"}},{"cell_type":"markdown","source":["Way -1"],"metadata":{"id":"boe3j15sAi9H"}},{"cell_type":"code","source":["# Simplified rule-based relation extraction\n","import re\n","\n","# Sample text for relation extraction\n","text = \"Alice works at Google in Mountain View.\"\n","\n","# Define a regex pattern to extract person, organization, and location\n","# The pattern uses named capturing groups: (?P<name>pattern)\n","# - (?P<person>[A-Z][a-z]+): Captures a person with a capitalized first name\n","# - (?P<organization>[A-Z][a-z]+): Captures an organization with a capitalized name\n","# - (?P<location>[A-Z][a-z]+ [A-Z][a-z]+): Captures a two-word location (both words capitalized)\n","pattern = r\"(?P<person>[A-Z][a-z]+) works at (?P<organization>[A-Z][a-z]+) in (?P<location>[A-Z][a-z]+ [A-Z][a-z]+)\"\n","\n","# Search for the pattern in the text\n","match = re.search(pattern, text)\n","\n","# If a match is found, print the named groups as a dictionary\n","if match:\n","    print(match.groupdict())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzSDoAL5_OVn","executionInfo":{"status":"ok","timestamp":1728829102276,"user_tz":-60,"elapsed":283,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"5f204695-bcba-4f80-8048-fb23e4842adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'person': 'Alice', 'organization': 'Google', 'location': 'Mountain View'}\n"]}]},{"cell_type":"markdown","source":["Way -2"],"metadata":{"id":"xFE_sBecAkcg"}},{"cell_type":"code","source":["def relation_detection_spacy(text):\n","    # Process the input text with spaCy's NLP pipeline\n","    doc = nlp(text)\n","\n","    relations = []  # Initialize an empty list to store detected relations\n","\n","    # Iterate over the named entities in the document\n","    for ent in doc.ents:\n","        # Check if the entity is a person (labeled \"PERSON\" by spaCy's NER)\n","        if ent.label_ == \"PERSON\":\n","            # Check the syntactic dependencies of the person's root word\n","            # Look for a preposition (prep) that is related to the verb \"work\"\n","            for token in ent.root.head.children:\n","                if token.dep_ == \"prep\" and token.head.lemma_ == \"work\":\n","                    # Find the organization (ORG) entity that appears after the person entity\n","                    org = [ent2 for ent2 in doc.ents if ent2.start > ent.start and ent2.label_ == \"ORG\"]\n","                    # If an organization is found, add the relation (person, verb, organization) to the relations list\n","                    if org:\n","                        relations.append((ent.text, token.head.lemma_, org[0].text))\n","\n","    return relations  # Return the list of detected relations\n","\n","# Example usage\n","relations = relation_detection_spacy(text)\n","print(\"Relations Detected:\", relations)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C35KlBsAAgjm","executionInfo":{"status":"ok","timestamp":1728829126792,"user_tz":-60,"elapsed":346,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"e4f74899-2162-43c7-f4b8-746579191c55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Relations Detected: [('Alice', 'work', 'Google')]\n"]}]},{"cell_type":"markdown","source":["## 2.7 **Modularity and Scalability of the Pipeline**\n","\n","- **Modular Design**:\n","  - Each stage in the pipeline is modular, allowing for easy replacement, enhancement, or addition of stages based on the specific requirements of the task or domain.\n","  - For example, a pipeline for legal text extraction may include additional steps like case identification or contract clause extraction.\n","\n","- **Scalability**:\n","  - The architecture should be scalable to handle large datasets or real-time data streams.\n","  - Parallelization and distributed computing frameworks (e.g., Apache Spark, Hadoop) can be used to scale the extraction pipeline for massive text corpora.\n","\n","- **Creative Observations**:\n","  - A modular pipeline can incorporate domain adaptation mechanisms, allowing the architecture to switch between general-purpose and domain-specific models based on the input text.\n","  - Integration with knowledge graphs (e.g., Wikidata, DBpedia) enhances the scalability of relation extraction by linking extracted entities to known relationships in structured datasets.\n","\n","- **Example Code (Adding Modularity)**:\n"],"metadata":{"id":"JtRf72eW_H3B"}},{"cell_type":"code","source":["def process_text(text, pipeline):\n","    # Apply each stage in the pipeline to the text sequentially\n","    for stage in pipeline:\n","        text = stage(text)  # Update the text with the output of each stage\n","    return text\n","\n","# Define modular pipeline stages\n","def tokenize(text):\n","    # Tokenize the input text into individual words (tokens) using NLTK's word_tokenize\n","    return word_tokenize(text)\n","\n","def pos_tagging(tokens):\n","    # Perform part-of-speech (POS) tagging on the tokenized words using NLTK's pos_tag\n","    return pos_tag(tokens)\n","\n","def named_entity_recognition(pos_tags):\n","    # Perform Named Entity Recognition (NER) using NLTK's ne_chunk on POS-tagged words\n","    return ne_chunk(pos_tags)\n","\n","# Create a modular pipeline\n","# Each function (tokenize, pos_tagging, named_entity_recognition) is treated as a stage in the pipeline\n","pipeline = [tokenize, pos_tagging, named_entity_recognition]\n","\n","# Process text through the pipeline\n","# The input text goes through each stage in the pipeline sequentially\n","text = \"Alice works at Google in Mountain View.\"\n","result = process_text(text, pipeline)\n","\n","# Print the result after passing through the entire pipeline\n","print(result)\n"],"metadata":{"id":"74OqL-8x_Klx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829168151,"user_tz":-60,"elapsed":311,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"98db5680-d074-4e54-e161-f0d4660ca70e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (GPE Alice/NNP)\n","  works/VBZ\n","  at/IN\n","  (ORGANIZATION Google/NNP)\n","  in/IN\n","  (GPE Mountain/NNP View/NNP)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["# Demonstration (Continuation)"],"metadata":{"id":"63lstD__Bfaq"}},{"cell_type":"markdown","source":["#### 3.1 **Sentence Segmentation (Advanced)**\n","\n","- **Advanced Sentence Segmentation**:\n","  - Sentence segmentation can get tricky when dealing with abbreviations or multiple punctuation marks. Below is an example that handles edge cases using a custom rule-based approach.\n","\n"],"metadata":{"id":"R5cqA2nZBjw9"}},{"source":["import re\n","\n","def advanced_sentence_segmentation(text):\n","    \"\"\"\n","    Segments text into sentences using a refined regex pattern.\n","\n","    Args:\n","        text (str): The input text to be segmented.\n","\n","    Returns:\n","        list: A list of segmented sentences.\n","    \"\"\"\n","    # Regex to split sentences, addressing abbreviations and titles.\n","    # This pattern uses negative lookbehind with word boundaries for better accuracy.\n","    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s')\n","\n","    # Split the text based on the pattern.\n","    sentences = sentence_endings.split(text)\n","\n","    return sentences\n","\n","# Sample text with abbreviations and punctuation.\n","text = \"Dr. Smith went to the U.S. He met Mrs. Jones. Isn't it great?\"\n","\n","# Apply the sentence segmentation.\n","sentences = advanced_sentence_segmentation(text)\n","\n","# Display the segmented sentences.\n","print(sentences)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyXqlwx-QPdl","executionInfo":{"status":"ok","timestamp":1728829294248,"user_tz":-60,"elapsed":382,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"69bf0f09-1527-4cbd-93dc-a3bd2e2e0bba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Dr. Smith went to the U.S. He met Mrs.', 'Jones.', \"Isn't it great?\"]\n"]}]},{"cell_type":"markdown","source":["#### 3.2 **Tokenization (Advanced)**\n","\n","- **Custom Tokenization for Complex Cases**:\n","  - Below is a custom tokenizer that handles email addresses, URLs, and contractions more effectively than a simple word tokenizer.\n","\n"],"metadata":{"id":"ROrSq7G7Bjtj"}},{"cell_type":"code","source":["import re\n","\n","def custom_tokenizer(text):\n","    # Handling URLs, email addresses, and contractions in the tokenization process\n","    # - Email addresses are matched with: [\\w\\.-]+@[\\w\\.-]+\n","    # - URLs are matched with: \\w+://[^\\s]+\n","    # - Contractions (e.g., I'll, John's) are matched with: [A-Za-z]+['’]?\\w*\n","    # - Words (basic token matching) are matched with: \\w+\n","    pattern = r\"[\\w\\.-]+@[\\w\\.-]+|\\w+://[^\\s]+|[A-Za-z]+['’]?\\w*|\\w+\"\n","\n","    # Use re.findall to return all matching tokens based on the pattern\n","    return re.findall(pattern, text)\n","\n","# Sample text with email, URL, and contraction for tokenization\n","text = \"You can reach me at john.doe@gmail.com or visit http://example.com. I'll be there!\"\n","\n","# Tokenize the text using the custom tokenizer\n","tokens = custom_tokenizer(text)\n","\n","# Print the resulting tokens\n","print(tokens)\n"],"metadata":{"id":"LLVCOJP2Bu0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829326204,"user_tz":-60,"elapsed":298,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"4f757bf5-6e71-42bf-9f59-2b9f18ac268f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['You', 'can', 'reach', 'me', 'at', 'john.doe@gmail.com', 'or', 'visit', 'http://example.com.', \"I'll\", 'be', 'there']\n"]}]},{"cell_type":"markdown","source":["#### 3.3 **Part-of-Speech (POS) Tagging (Advanced)**\n","\n","- **Using a Custom POS Tagger with Contextual Rules**:\n","  - This example creates a simple rule-based POS tagger for certain words, adding more logic to handle context and token-specific rules.\n","\n"],"metadata":{"id":"P2N3VT1IBjqz"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Download the necessary POS tagger model\n","nltk.download('averaged_perceptron_tagger')\n","\n","def custom_pos_tagger(tokens):\n","    # Perform default POS tagging using NLTK's pos_tag function\n","    pos_tags = nltk.pos_tag(tokens)\n","\n","    # Create a list to hold the customized POS tags\n","    custom_pos = []\n","\n","    # Iterate over the word-POS tag pairs to apply custom rules\n","    for word, tag in pos_tags:\n","        # Custom rule: Always tag 'Google' as a proper noun (NNP), even if the default tag is different\n","        if word.lower() == 'google':\n","            custom_pos.append((word, 'NNP'))  # NNP stands for proper noun (singular)\n","        # Custom rule: Change 'run' tagged as a noun (NN) to a verb (VB) based on our logic\n","        elif word.lower() == 'run' and tag == 'NN':\n","            custom_pos.append((word, 'VB'))  # VB is the tag for base form verbs\n","        else:\n","            # For all other cases, keep the default POS tag\n","            custom_pos.append((word, tag))\n","\n","    return custom_pos\n","\n","# Sample sentence for POS tagging\n","sentence = \"He will Google the problem and run away.\"\n","\n","# Tokenize the sentence into words\n","tokens = word_tokenize(sentence)\n","\n","# Perform custom POS tagging on the tokenized sentence\n","pos_tags = custom_pos_tagger(tokens)\n","\n","# Print the customized POS tags\n","print(pos_tags)\n"],"metadata":{"id":"IIjbPI9lBzrc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829356218,"user_tz":-60,"elapsed":258,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"647272fc-3a23-46f8-9f23-1ba6456916cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('He', 'PRP'), ('will', 'MD'), ('Google', 'NNP'), ('the', 'DT'), ('problem', 'NN'), ('and', 'CC'), ('run', 'VB'), ('away', 'RB'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","source":["#### 3.4 **Named Entity Recognition (NER) (Advanced)**\n","\n","- **NER with SpaCy**:\n","  - Using SpaCy’s pre-trained models for more advanced NER, which can handle entity recognition at scale.\n","\n"],"metadata":{"id":"8ETUmg1nBjoP"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the SpaCy model for English NER\n","# \"en_core_web_sm\" is a small, pre-trained English model that includes vocabulary, part-of-speech tagging, and named entity recognition\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def spacy_ner(text):\n","    # Process the text through spaCy's NLP pipeline\n","    doc = nlp(text)\n","\n","    # Iterate over the named entities detected in the text\n","    for ent in doc.ents:\n","        # Print the entity text (the part of the text that is recognized as an entity) and its label (the entity type)\n","        print(ent.text, ent.label_)\n","\n","# Sample text for entity recognition\n","text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n","\n","# Call the function to perform NER and print the results\n","spacy_ner(text)\n"],"metadata":{"id":"7GXot7kyB0Qy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829380507,"user_tz":-60,"elapsed":2241,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"289ad80b-b6ac-49c3-fbca-3af7df1d2b64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Apple ORG\n","U.K. GPE\n","$1 billion MONEY\n"]}]},{"cell_type":"markdown","source":["#### 3.5 **Relation Detection (Advanced)**\n","\n","- **Relation Detection Using Dependency Parsing**:\n","  - Relation detection often involves identifying the syntactic structure of the sentence. Dependency parsing can be used to detect relationships between entities.\n","\n"],"metadata":{"id":"RjfqoIutBjlU"}},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy model for dependency parsing and relation extraction\n","# \"en_core_web_sm\" is a pre-trained model that includes vocabulary, part-of-speech tagging, and dependency parsing\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def relation_extraction(text):\n","    # Process the text using spaCy's NLP pipeline\n","    doc = nlp(text)\n","\n","    # Iterate through each token in the document\n","    for token in doc:\n","        # Check for entities that are either nominal subjects (nsubj) or direct objects (dobj)\n","        if token.dep_ in ('nsubj', 'dobj'):\n","            # Print the entity (token text), the relation (the head word of the token), and the type (subject or object)\n","            print(f\"Entity: {token.text}, Relation: {token.head.text}, Type: {token.dep_}\")\n","\n","# Sample text for relation extraction\n","text = \"Google acquired YouTube in 2006 for $1.65 billion.\"\n","\n","# Call the function to perform relation extraction and print the results\n","relation_extraction(text)\n"],"metadata":{"id":"32a9RmyjB0sH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829409623,"user_tz":-60,"elapsed":3109,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"94a74f0e-d52c-4809-a22a-b6d96e0d9e82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entity: Google, Relation: acquired, Type: nsubj\n","Entity: YouTube, Relation: acquired, Type: dobj\n"]}]},{"cell_type":"markdown","source":["#### 3.6 **Modularity and Scalability (Advanced)**\n","\n","- **Dynamic Pipeline for Entity and Relation Extraction**:\n","  - This demonstrates a dynamically configurable pipeline, where users can add or remove stages such as tokenization, POS tagging, and NER based on their needs.\n","\n"],"metadata":{"id":"dsKUXPYHBjia"}},{"cell_type":"code","source":["import spacy\n","\n","# Define stages\n","def tokenize(text):\n","    # Tokenize the text using spaCy and return a list of token texts\n","    return [token.text for token in nlp(text)]\n","\n","def pos_tagging(tokens):\n","    # Convert the tokens back into a string and run through spaCy's NLP pipeline for POS tagging\n","    doc = nlp(\" \".join(tokens))\n","    # Return a list of tuples with each token and its corresponding POS tag\n","    return [(token.text, token.pos_) for token in doc]\n","\n","def named_entity_recognition(tokens):\n","    # Check if the input 'tokens' is a list of tuples (from pos_tagging)\n","    if isinstance(tokens[0], tuple):\n","        # If yes, extract only the token texts for NER\n","        tokens = [token[0] for token in tokens]\n","    # Convert the tokens back into a string and run through spaCy's NLP pipeline for NER\n","    doc = nlp(\" \".join(tokens))\n","    # Return a list of tuples with each named entity and its corresponding label\n","    return [(ent.text, ent.label_) for ent in doc.ents]\n","\n","# Dynamic pipeline function\n","def dynamic_pipeline(text, stages):\n","    # Sequentially apply each stage in the pipeline to the text\n","    for stage in stages:\n","        text = stage(text)  # Update the text at each stage with the processed result\n","    return text  # Return the final result after all stages\n","\n","# Example of a dynamic pipeline usage\n","nlp = spacy.load(\"en_core_web_sm\")\n","text = \"John works at Google in California.\"\n","\n","# Define the sequence of stages in the pipeline\n","pipeline = [tokenize, pos_tagging, named_entity_recognition]\n","\n","# Run the dynamic pipeline on the input text\n","result = dynamic_pipeline(text, pipeline)\n","\n","# Print the result after going through the pipeline\n","print(result)"],"metadata":{"id":"ivN_ix_WB1GH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829498472,"user_tz":-60,"elapsed":2111,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"b9c64ba1-8c33-4677-b015-029da038481d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('John', 'PERSON'), ('Google', 'ORG'), ('California', 'GPE')]\n"]}]},{"cell_type":"markdown","source":["#### 3.7 **Error Handling in Pipeline**\n","\n","- **Robustness in Pipeline by Handling Failures**:\n","  - It's important to build robust pipelines that can handle errors and exceptions gracefully without breaking the entire process.\n","\n"],"metadata":{"id":"DZulReAXBjf0"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy model for English NLP tasks\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Define stages with error handling for robust processing\n","\n","def safe_tokenize(text):\n","    # Try to tokenize the text and handle any errors gracefully\n","    try:\n","        return [token.text for token in nlp(text)]  # Tokenize the text\n","    except Exception as e:\n","        print(f\"Error in tokenization: {e}\")  # Print an error message if an exception occurs\n","        return []  # Return an empty list in case of an error\n","\n","def safe_pos_tagging(tokens):\n","    # Try to perform POS tagging on the tokens and handle any errors gracefully\n","    try:\n","        doc = nlp(\" \".join(tokens))  # Join tokens into a single string and pass through spaCy\n","        return [(token.text, token.pos_) for token in doc]  # Return token and its POS tag\n","    except Exception as e:\n","        print(f\"Error in POS tagging: {e}\")  # Print an error message if an exception occurs\n","        return []  # Return an empty list in case of an error\n","\n","def safe_ner(tokens):\n","    # Try to perform Named Entity Recognition (NER) and handle any errors gracefully\n","    try:\n","        doc = nlp(\" \".join(tokens))  # Join tokens into a string and pass through spaCy for NER\n","        return [(ent.text, ent.label_) for ent in doc.ents]  # Return entities and their labels\n","    except Exception as e:\n","        print(f\"Error in NER: {e}\")  # Print an error message if an exception occurs\n","        return []  # Return an empty list in case of an error\n","\n","# Robust dynamic pipeline function with error handling\n","def robust_pipeline(text, stages):\n","    # Apply each stage in the pipeline, passing the text through all stages sequentially\n","    for stage in stages:\n","        text = stage(text)  # Update text with the result of each stage\n","    return text  # Return the final result after all stages\n","\n","# Example usage with robustness in mind\n","text = \"John works at Google in California.\"\n","\n","# Define the pipeline with the robust, error-handling stages\n","pipeline = [safe_tokenize, safe_pos_tagging, safe_ner]\n","\n","# Run the robust pipeline on the input text\n","result = robust_pipeline(text, pipeline)\n","\n","# Print the final result after processing through the pipeline\n","print(result)\n"],"metadata":{"id":"jfYfrJTCCI3-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728829526036,"user_tz":-60,"elapsed":1902,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"c55c843f-9235-4b57-cc8d-a1eb97b0722e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error in NER: sequence item 0: expected str instance, tuple found\n","[]\n"]}]},{"cell_type":"markdown","source":["# Demonstration using NTLK library"],"metadata":{"id":"II4rRFPcC6KI"}},{"cell_type":"markdown","source":["## 4.1 **Sentence Segmentation (Advanced with NLTK)**\n","\n","- **Advanced Sentence Segmentation Using NLTK**:\n","  - This demonstrates sentence segmentation using custom regex rules combined with NLTK’s sentence tokenizer, specifically handling abbreviations and complex sentence boundaries.\n","\n"],"metadata":{"id":"Wz7q2BS6Bjc_"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","\n","def advanced_sentence_segmentation(text):\n","    # Using NLTK sentence tokenizer for default segmentation\n","    sentences = nltk.sent_tokenize(text)\n","\n","    # Custom post-processing for handling abbreviations and other edge cases\n","    processed_sentences = []\n","\n","    for sentence in sentences:\n","        # If the sentence ends with 'Dr.' or 'U.S.', we append it as is\n","        if sentence.endswith('Dr.') or sentence.endswith('U.S.'):\n","            processed_sentences.append(sentence)\n","        else:\n","            # Strip any trailing/leading whitespace and append\n","            processed_sentences.append(sentence.strip())\n","\n","    return processed_sentences\n","\n","# Sample text with abbreviations and punctuation for testing\n","text = \"Dr. Smith went to the U.S. He said, 'It was great!'\"\n","\n","# Call the function to perform sentence segmentation\n","sentences = advanced_sentence_segmentation(text)\n","\n","# Print the processed sentences\n","print(sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvT-pd6dDv2y","executionInfo":{"status":"ok","timestamp":1728829604585,"user_tz":-60,"elapsed":291,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"5b451ef1-89e9-48f1-9277-45d056378e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Dr. Smith went to the U.S.', \"He said, 'It was great!'\"]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## 4.2 **Tokenization (Advanced Custom Tokenization with NLTK)**\n","\n","- **Custom Tokenization Handling Special Cases with NLTK**:\n","  - This example demonstrates handling of contractions, numbers, and special cases using a combination of NLTK’s `word_tokenize` and custom logic.\n","\n"],"metadata":{"id":"QJZQ4rCiBjaS"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","def custom_nltk_tokenizer(text):\n","    # Basic word tokenization using NLTK\n","    tokens = word_tokenize(text)\n","    processed_tokens = []\n","\n","    # Post-processing to handle contractions and numbers\n","    for token in tokens:\n","        if token == \"n't\":\n","            # Append contraction (\"n't\") to the previous word\n","            processed_tokens[-1] = processed_tokens[-1] + \"n't\"\n","        elif token.replace('.', '', 1).isdigit():  # Handle decimal numbers\n","            processed_tokens.append(token)\n","        else:\n","            # Append the token as is for all other cases\n","            processed_tokens.append(token)\n","    return processed_tokens\n","\n","# Sample text containing contractions and numbers\n","text = \"She won't go, and it costs 100.50 dollars.\"\n","\n","# Tokenize the text using the custom tokenizer\n","tokens = custom_nltk_tokenizer(text)\n","\n","# Print the processed tokens\n","print(tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrIzCdwQDysN","executionInfo":{"status":"ok","timestamp":1728829629685,"user_tz":-60,"elapsed":288,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"ba3b5871-7fe0-4fbb-860f-a88431cbd1ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['She', \"won't\", 'go', ',', 'and', 'it', 'costs', '100.50', 'dollars', '.']\n"]}]},{"cell_type":"markdown","source":["## 4.3 **Part-of-Speech (POS) Tagging (Advanced with NLTK)**\n","\n","- **Handling Ambiguity in POS Tagging with NLTK**:\n","  - This example deals with ambiguous words like \"run\" or \"lead\" and assigns different tags based on specific contexts using custom rule logic along with NLTK’s default POS tagger.\n","\n"],"metadata":{"id":"pZjeE-8KBjXg"}},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","\n","# Download necessary resources for POS tagging\n","nltk.download('averaged_perceptron_tagger')\n","\n","def custom_pos_tagging(sentence):\n","    # Tokenize the sentence into words\n","    tokens = word_tokenize(sentence)\n","\n","    # Perform default POS tagging using NLTK\n","    pos_tags = pos_tag(tokens)\n","\n","    # Apply custom rules to handle ambiguous words\n","    custom_tags = []\n","    for word, tag in pos_tags:\n","        if word.lower() == 'lead':\n","            if 'VB' in tag:  # If 'lead' is used as a verb\n","                custom_tags.append((word, 'VB'))\n","            else:  # Otherwise, assume it is a noun\n","                custom_tags.append((word, 'NN'))\n","        elif word.lower() == 'run':\n","            # Handle 'run' based on its context: verb (VB) or noun (NN)\n","            custom_tags.append((word, 'VB' if 'VB' in tag else 'NN'))\n","        else:\n","            # For other words, keep the default POS tag\n","            custom_tags.append((word, tag))\n","\n","    return custom_tags\n","\n","# Example sentence containing ambiguous words like \"lead\"\n","sentence = \"The CEO will lead the meeting, and the lead pipe was rusty.\"\n","\n","# Perform custom POS tagging\n","pos_tags = custom_pos_tagging(sentence)\n","\n","# Print the resulting POS tags after applying custom rules\n","print(pos_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXt0fYOfD0wL","executionInfo":{"status":"ok","timestamp":1728829663945,"user_tz":-60,"elapsed":295,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"d7bd77ab-3480-43df-fdf5-6fc8cfc579e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT'), ('CEO', 'NNP'), ('will', 'MD'), ('lead', 'VB'), ('the', 'DT'), ('meeting', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('lead', 'NN'), ('pipe', 'NN'), ('was', 'VBD'), ('rusty', 'JJ'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"markdown","source":["## 4.4 **Named Entity Recognition (NER) with NLTK (Advanced)**\n","\n","- **Using a Custom NER System Based on NLTK**:\n","  - This example demonstrates building a simple NER system using NLTK’s chunking features combined with POS tags to detect named entities.\n","\n"],"metadata":{"id":"rb9YXTxgBjUu"}},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag, ne_chunk\n","from nltk.tokenize import word_tokenize\n","\n","# Download necessary resources for NER\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","def custom_nltk_ner(sentence):\n","    # Tokenizing the sentence into words\n","    tokens = word_tokenize(sentence)\n","\n","    # Performing POS tagging on the tokens\n","    pos_tags = pos_tag(tokens)\n","\n","    # Performing Named Entity Recognition (NER) using NLTK's ne_chunk on POS-tagged tokens\n","    ner_tree = ne_chunk(pos_tags)\n","\n","    return ner_tree\n","\n","# Example sentence for NER\n","sentence = \"Barack Obama was born in Honolulu and became the president of the USA.\"\n","\n","# Perform custom NER using NLTK\n","ner_result = custom_nltk_ner(sentence)\n","\n","# Print the resulting NER tree\n","print(ner_result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMx1BTGaD1DA","executionInfo":{"status":"ok","timestamp":1728829694862,"user_tz":-60,"elapsed":345,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"457ef432-4465-4db6-f84f-3a2f1db83b99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Barack/NNP)\n","  (PERSON Obama/NNP)\n","  was/VBD\n","  born/VBN\n","  in/IN\n","  (GPE Honolulu/NNP)\n","  and/CC\n","  became/VBD\n","  the/DT\n","  president/NN\n","  of/IN\n","  the/DT\n","  (ORGANIZATION USA/NNP)\n","  ./.)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## 4.5 **Relation Detection Using Dependency Parsing (NLTK)**\n","\n","- **Relation Extraction Using POS Patterns and Chunking in NLTK**:\n","  - This code demonstrates extracting relations between entities by combining POS tagging with chunking patterns to detect simple entity relations in a sentence.\n","\n"],"metadata":{"id":"u5b_19L0BjSK"}},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag\n","from nltk.chunk import RegexpParser\n","from nltk.tokenize import word_tokenize\n","\n","def custom_relation_extraction(text):\n","    # Tokenization and POS tagging\n","    tokens = word_tokenize(text)\n","    pos_tags = pos_tag(tokens)\n","\n","    # Define a simple grammar for extracting NP (Noun Phrase) and VP (Verb Phrase) relations\n","    grammar = r\"\"\"\n","        NP: {<DT>?<JJ>*<NN.*>}    # Noun phrase: optional determiner (DT), adjectives (JJ), and noun(s) (NN.*)\n","        VP: {<VB.*>}              # Verb phrase: any form of verb (VB.*)\n","    \"\"\"\n","\n","    # Create a RegexpParser with the defined grammar\n","    chunker = RegexpParser(grammar)\n","\n","    # Apply the chunking to the POS-tagged tokens\n","    chunked_tree = chunker.parse(pos_tags)\n","\n","    # Print the chunked sentence tree for visualization\n","    print(chunked_tree)\n","\n","    # Extract NP-VP-NP relations\n","    relations = []\n","    current_np = None\n","\n","    # Traverse the chunked tree to find noun phrases (NP)\n","    for subtree in chunked_tree:\n","        if type(subtree) == nltk.Tree:\n","            # If the subtree is a noun phrase (NP), extract the text\n","            if subtree.label() == 'NP':\n","                if current_np is None:\n","                    current_np = \" \".join([word for word, tag in subtree.leaves()])\n","                else:\n","                    relations.append((current_np, \" \".join([word for word, tag in subtree.leaves()])))\n","                    current_np = None\n","\n","    return relations\n","\n","# Example sentence for relation extraction\n","text = \"John bought a car from the dealership.\"\n","\n","# Perform relation extraction\n","relations = custom_relation_extraction(text)\n","\n","# Print the extracted NP-VP-NP relations\n","print(relations)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JR2bvcyBD2rV","executionInfo":{"status":"ok","timestamp":1728829739139,"user_tz":-60,"elapsed":286,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"93f1426a-8ce5-441b-d006-271e8d23dc6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP John/NNP)\n","  (VP bought/VBD)\n","  (NP a/DT car/NN)\n","  from/IN\n","  (NP the/DT dealership/NN)\n","  ./.)\n","[('John', 'a car')]\n"]}]},{"cell_type":"markdown","source":["## 4.6 **Error Handling and Robustness in NLTK Pipelines**\n","\n","- **Handling Errors in NER and POS Tagging Using NLTK**:\n","  - A robust pipeline that handles errors during tokenization, POS tagging, or NER without breaking the entire process.\n","\n"],"metadata":{"id":"ECwKfboTBjPX"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag, ne_chunk\n","\n","# Safe tokenization with error handling\n","def safe_tokenize(text):\n","    try:\n","        return word_tokenize(text)  # Tokenize the input text\n","    except Exception as e:\n","        print(f\"Error in tokenization: {e}\")\n","        return []  # Return an empty list in case of an error\n","\n","# Safe POS tagging with error handling\n","def safe_pos_tag(tokens):\n","    try:\n","        return pos_tag(tokens)  # Perform POS tagging on the tokenized input\n","    except Exception as e:\n","        print(f\"Error in POS tagging: {e}\")\n","        return []  # Return an empty list in case of an error\n","\n","# Safe NER (Named Entity Recognition) with error handling\n","def safe_ner(pos_tags):\n","    try:\n","        return ne_chunk(pos_tags)  # Perform NER using the POS-tagged tokens\n","    except Exception as e:\n","        print(f\"Error in NER: {e}\")\n","        return []  # Return an empty list in case of an error\n","\n","# Robust NLTK pipeline with error handling at each stage\n","def robust_nltk_pipeline(text):\n","    tokens = safe_tokenize(text)  # Tokenize the input text\n","    pos_tags = safe_pos_tag(tokens)  # Perform POS tagging on the tokens\n","    named_entities = safe_ner(pos_tags)  # Perform Named Entity Recognition (NER) on the POS tags\n","\n","    return named_entities\n","\n","# Example input text\n","text = \"John Smith lives in California and works at Google.\"\n","\n","# Run the robust NLTK pipeline\n","entities = robust_nltk_pipeline(text)\n","\n","# Print the resulting named entities\n","print(entities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSSIdME3D3UU","executionInfo":{"status":"ok","timestamp":1728829780233,"user_tz":-60,"elapsed":385,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7e121b67-e80c-4a18-f65f-0fe99d945b28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON John/NNP)\n","  (PERSON Smith/NNP)\n","  lives/VBZ\n","  in/IN\n","  (GPE California/NNP)\n","  and/CC\n","  works/NNS\n","  at/IN\n","  (ORGANIZATION Google/NNP)\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["## 4.7 **Recursive Chunking and Cascaded Chunkers with NLTK**\n","\n","- **Recursive Chunking for Handling Nested Entities in NLTK**:\n","  - Demonstrating cascaded chunking with NLTK for handling nested structures, where noun phrases can contain other phrases.\n","\n"],"metadata":{"id":"AjXi2pZWBjM4"}},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","from nltk.chunk import RegexpParser\n","\n","def recursive_chunking(text):\n","    # Tokenization and POS tagging\n","    tokens = word_tokenize(text)\n","    pos_tags = pos_tag(tokens)\n","\n","    # Define a recursive grammar for chunking\n","    grammar = r\"\"\"\n","        NP: {<DT>?<JJ>*<NN.*>}        # Noun phrase: optional determiner (DT), adjectives (JJ), and noun(s)\n","        VP: {<VB.*><NP|PP>}           # Verb phrase: verb followed by NP (Noun Phrase) or PP (Prepositional Phrase)\n","        PP: {<IN><NP>}                # Prepositional phrase: preposition followed by NP\n","    \"\"\"\n","\n","    # Chunk the sentence according to the recursive grammar\n","    chunker = RegexpParser(grammar)\n","    chunked_tree = chunker.parse(pos_tags)\n","\n","    return chunked_tree\n","\n","# Example text for chunking\n","text = \"The quick brown fox jumps over the lazy dog near the river.\"\n","\n","# Perform recursive chunking\n","chunked_tree = recursive_chunking(text)\n","\n","# Print the resulting chunked tree\n","print(chunked_tree)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Byn1LmC5D33o","executionInfo":{"status":"ok","timestamp":1728829807795,"user_tz":-60,"elapsed":307,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"c4354c8f-43ca-4951-f8b0-93342431bc61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP The/DT quick/JJ brown/NN)\n","  (NP fox/NN)\n","  jumps/VBZ\n","  (PP over/IN (NP the/DT lazy/JJ dog/NN))\n","  (PP near/IN (NP the/DT river/NN))\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["# Demonstration using pyTorch"],"metadata":{"id":"rADCp3MDNHYi"}},{"cell_type":"markdown","source":["## 5.1 **Tokenization using PyTorch**\n","\n","- **Custom Subword Tokenization Using PyTorch and Hugging Face's Tokenizers**:\n","  - This example demonstrates how to use subword tokenization (like Byte-Pair Encoding) with PyTorch using Hugging Face’s `transformers` library for tokenization, often used in large models like BERT.\n","\n"],"metadata":{"id":"ss2E6uyLBi_Q"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Load the tokenizer from Hugging Face for BERT\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","def subword_tokenization(text):\n","    # Perform subword tokenization using BERT's tokenizer\n","    tokens = tokenizer.tokenize(text)  # Tokenize the input text into subwords\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)  # Convert the tokens into their corresponding token IDs\n","    return tokens, token_ids\n","\n","# Example text for subword tokenization\n","text = \"The quick brown fox jumps over the lazy dog near the river.\"\n","\n","# Perform subword tokenization\n","tokens, token_ids = subword_tokenization(text)\n","\n","# Print the resulting subword tokens and their token IDs\n","print(\"Tokens:\", tokens)\n","print(\"Token IDs:\", token_ids)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339,"referenced_widgets":["ca3b131ccb654972abb564eee95e2d23","8547c28a4d3e487cbbe7b76cd95ae877","0d4fa4c511554b339c432b6c53146992","9d85ff0041b74a648fe22cab1ae984bb","75ccb3f349bc4d8f8f86277ed203a83e","7790ef142c4c453e8353c96a34dda675","5cefffb00ce6411cbb35bfec37281673","49732c8b5ab5471cb7a66de1f198a264","18ec3f0d694549da9ccc98e6f90ec53a","0f872ff846814bd5bc74cf3f80946171","9b7a971f170e4917b24e86cd8dd03702","12eca434498743c58750897a080d86fd","1323f6ec5d6f44598518479255aa558f","a78ef6b3c3f0492aa54bf7bb09dc364e","3fbd3063f62e490cb5ea38b066b59d50","2adab27d4cf84c3e95e8e1d613471aa2","cc5cc534c68449b980d441cc13972877","5f7170638d694fe3a30d1c7a5a0c61cf","9c347d9f9e5f4d08bd8ac35bd462e92b","22a3c25755654caea2541f4d646374be","5c11af8396f54ff98d095906a27f0ca4","5f4ca99b4b3f46628d71cb9ee31d2459","8eca9f23672b494c914b8ba5185e7077","b6535a6b0ee34e0886e828e96550aa79","0152f01726d54d79bad6f10b4cb2f74d","c826b1793d744a2792721b4b7395e07e","a1707d2a0e514627b1553f78768ff2d9","9b19a002a8f84852b7e19bd07cdb7888","8dc2fc15eed84cf1835d3e42f46b3fe4","33037e9dbbc14dc1b766fd829768aa43","2fc20f7c68734852ab3163ad9cbe0293","e448119f47704243bcf283c5666fd650","49da645f3f2943e38c44e606d458e144","b3af9acfc7854c44a210ec902a68087d","51985386d02c431284a49d179da0a67a","d1749272d5c242ebbdcb9e2d3def7b2b","3504ee1f77104a328f78a3463108a376","46e3bb82ae064083ac7d327c35e07a4f","646771c6b0b4446199a56f33e1c4467f","019d9627cbd8442087c9e1f79259343a","fe56a4f959864440b46453ba106031a6","6a8dc0699662418897f28c34ecece653","9b6a7b6305624f46b8b5ab9bdc2a1a35","1839b48dd583437da126d50abc0ee0c2"]},"id":"aI26x8u-NMcI","executionInfo":{"status":"ok","timestamp":1728829830738,"user_tz":-60,"elapsed":3337,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"c6a1270c-65ed-4f13-a2cb-015c5fb2d6e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3b131ccb654972abb564eee95e2d23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12eca434498743c58750897a080d86fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eca9f23672b494c914b8ba5185e7077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3af9acfc7854c44a210ec902a68087d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'near', 'the', 'river', '.']\n","Token IDs: [1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 2379, 1996, 2314, 1012]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## 5.2 **POS Tagging Using a Custom RNN in PyTorch**\n","\n","- **POS Tagging with a Simple RNN in PyTorch**:\n","  - This example builds a simple RNN model to perform part-of-speech tagging. The model takes in tokenized sentences, passes them through an embedding layer, and processes them using an RNN to predict the POS tags.\n","\n"],"metadata":{"id":"rb90co1uBi7q"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Sample dataset\n","data = [(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n","        (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n","\n","# Vocabulary and tag set\n","word_to_ix = {}\n","tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Define tag mappings for parts of speech\n","for sent, tags in data:\n","    for word in sent:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)  # Create a word-to-index mapping\n","\n","# Define the RNN model\n","class RNNTagger(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n","        super(RNNTagger, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)  # Simple RNN layer\n","        self.fc = nn.Linear(hidden_dim, tagset_size)  # Fully connected layer to map to tag set\n","\n","    def forward(self, sentence):\n","        embeds = self.embedding(sentence)  # Convert input words to embeddings\n","        rnn_out, _ = self.rnn(embeds)  # Pass embeddings through the RNN\n","        tag_space = self.fc(rnn_out)  # Linear transformation to tag space\n","        tag_scores = nn.functional.log_softmax(tag_space, dim=2)  # Log-softmax for classification\n","        return tag_scores\n","\n","# Hyperparameters\n","EMBEDDING_DIM = 6  # Size of word embeddings\n","HIDDEN_DIM = 6  # Size of the RNN hidden state\n","vocab_size = len(word_to_ix)\n","tagset_size = len(tag_to_ix)\n","\n","# Initialize the model, loss function, and optimizer\n","model = RNNTagger(vocab_size, tagset_size, EMBEDDING_DIM, HIDDEN_DIM)\n","loss_function = nn.NLLLoss()  # Negative log likelihood loss\n","optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic gradient descent optimizer\n","\n","# Prepare input data as tensors\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]  # Convert words to their index values\n","    return torch.tensor(idxs, dtype=torch.long)  # Return tensor of word indices\n","\n","# Training loop\n","for epoch in range(300):\n","    for sentence, tags in data:\n","        model.zero_grad()  # Clear the gradients\n","\n","        sentence_in = prepare_sequence(sentence, word_to_ix)  # Convert input sentence to indices\n","        targets = prepare_sequence(tags, tag_to_ix)  # Convert target tags to indices\n","\n","        tag_scores = model(sentence_in)  # Forward pass through the model\n","        loss = loss_function(tag_scores.view(-1, tagset_size), targets)  # Compute the loss\n","        loss.backward()  # Backpropagate the loss\n","        optimizer.step()  # Update model parameters\n","\n","# Testing the model\n","with torch.no_grad():\n","    sentence = prepare_sequence(\"Everybody read that book\".split(), word_to_ix)  # Prepare test sentence\n","    tag_scores = model(sentence)  # Get tag scores for the sentence\n","    print(tag_scores)  # Print the tag scores for each word in the sentence\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"QFSuSsUtNQr9","executionInfo":{"status":"error","timestamp":1728829883526,"user_tz":-60,"elapsed":3525,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"fa362144-79b9-42e1-ea6f-f07994a01379"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-2, 1], but got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-8ba4a8629fe3>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_ix\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert target tags to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-8ba4a8629fe3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass embeddings through the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Linear transformation to tag space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Log-softmax for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1977\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1978\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"]}]},{"cell_type":"markdown","source":["## 5.3 **Named Entity Recognition (NER) Using PyTorch and BERT**\n","\n","- **NER Using Pre-trained BERT Model in PyTorch**:\n","  - This example demonstrates how to use a pre-trained BERT model for named entity recognition (NER) using Hugging Face's `transformers` library integrated with PyTorch.\n","\n"],"metadata":{"id":"FKpnRxLKBi4_"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForTokenClassification\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer for token classification (NER)\n","tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","\n","# Tokenize the input text\n","text = \"Hawking was a theoretical physicist at the University of Cambridge.\"\n","input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Convert text to token ids in PyTorch tensor format\n","\n","# Get predictions from the model\n","outputs = model(input_ids)  # Forward pass through the model\n","logits = outputs.logits  # Extract logits (unnormalized predictions)\n","\n","# Get predicted entity labels (the highest logit value per token corresponds to the predicted label)\n","predictions = torch.argmax(logits, dim=2)  # Get the index of the max logit per token\n","\n","# Convert token ids to actual tokens\n","tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","# Convert predicted label ids to actual label names\n","labels = [model.config.id2label[p.item()] for p in predictions[0]]\n","\n","# Print tokens and their corresponding entity labels\n","for token, label in zip(tokens, labels):\n","    print(f\"{token}: {label}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495,"referenced_widgets":["f9b192d5a50141f9beb802b6fd0e3693","22ae5da1257d40ee8ffe343186547ece","96370f063ae34c8fb7b01fcf6fd2d005","992bc0c3ad8845deb334e6a27014d450","ebfac05c3a564ae2931f253e7da516da","8cb9bc78b1a342968704332bb95b3cb5","40ded1bb35f14dbbb91566bb7e88283e","bb1763d9c70949c88a2957a23b58f1f5","81cc023e09054f8c976266c3d33809b8","aa6d864e555044b78766d1ab2e0845b0","1aa4305391d146e483085aa5b1d08b0a","219fe10003634bdbbc3be79832850a82","88aed65a4e854a09b8ac32706746a70d","161567eb39c94d0a9d2274d95b4bfd60","92059f5a96d1472fbcde4809ac9df401","f93eb3fc02694f7abb8310517994509f","01410e0644894052ba1b22e3a85653b3","d4318c33f839405cae9c76b35736fbfd","85bb4f9d713d4190989b2d52f62967d2","309afc32520e4e6da74ed54a13d0f491","c4e46b93cc204f01a4cfff9093a64a6a","b4665ff500c34f6f80655f1242eedd00","24536663f76f47f0b48e54c93e8d0c24","237d392e34cc4010a2376ac9c23300b4","7f95bb949bf94eeab6294805c6964508","5bed4e37a3614faab8f73ca5c5010472","52dc42363c394d09bf9054fb546b7d65","a5d695284a844267bad097e266f2c02f","2a3d863e35584fb389d07ee37f4b2834","62ae99a23a05460e9040a513170953f1","7c103972e019413395a2590e5a9e1ce2","461bb9c1f2194dbb9c60948258f76d8d","71c1a24699f94e798e68199fd1772b61","03358f79c531439d8ace9cebbb59721b","8de2680e952846da81b8d3d55bee0761","6fac98cc3bf6450a8f2ca6dd90afb032","a98c8d5aec5146308def46fd80749597","d5ac6d90a96a4529bf5acd97a3223252","7a9560de2c4e4bebafd209e36af88960","07653687fdac40f19d95c452d379c738","3394df11a24248dda215c4fe12244e82","5311fd86628247cab73d95c092450cad","e301d63de18245ac9601510a50ec0e6a","b8b2521841e0404ba4f1e43e7a2bde0d"]},"id":"6WSV09DSNQdd","executionInfo":{"status":"ok","timestamp":1728829944000,"user_tz":-60,"elapsed":28840,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"6b18793a-b63e-4777-8205-056a9011b37c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b192d5a50141f9beb802b6fd0e3693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219fe10003634bdbbc3be79832850a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24536663f76f47f0b48e54c93e8d0c24"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03358f79c531439d8ace9cebbb59721b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["[CLS]: O\n","Hawk: I-PER\n","##ing: I-PER\n","was: O\n","a: O\n","theoretical: O\n","physicist: O\n","at: O\n","the: O\n","University: I-ORG\n","of: I-ORG\n","Cambridge: I-ORG\n",".: O\n","[SEP]: O\n"]}]},{"cell_type":"markdown","source":["## 5.4 **Relation Extraction Using a Custom CNN in PyTorch**\n","\n","- **Relation Extraction Using CNN in PyTorch**:\n","  - In this example, a simple convolutional neural network (CNN) is built to predict relationships between entities based on the context of the sentence.\n","\n"],"metadata":{"id":"r-n3YSsNM_NL"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","# Create a simple CNN for relation extraction\n","class CNNRelationExtractor(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_filters, num_classes):\n","        super(CNNRelationExtractor, self).__init__()\n","        # Embedding layer to convert input tokens into embeddings\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        # Convolutional layer with 'num_filters' filters of size (3, embedding_dim)\n","        self.conv1 = nn.Conv2d(1, num_filters, (3, embedding_dim))\n","        # Max pooling layer to reduce dimensionality\n","        self.pool = nn.MaxPool2d((2, 1))\n","        # Fully connected layer for classification\n","        self.fc1 = nn.Linear(num_filters, num_classes)\n","\n","    def forward(self, x):\n","        # Convert input tokens to embeddings\n","        x = self.embedding(x)\n","        # Add a channel dimension for the convolutional layer (batch_size, 1, sentence_len, embedding_dim)\n","        x = x.unsqueeze(1)\n","        # Apply convolutional layer\n","        x = self.conv1(x)\n","        # Apply ReLU activation function\n","        x = torch.relu(x)\n","        # Apply max pooling\n","        x = self.pool(x)\n","        # Flatten the output for the fully connected layer\n","        x = x.view(x.size(0), -1)\n","        # Apply fully connected layer to get final class predictions\n","        x = self.fc1(x)\n","        return torch.softmax(x, dim=1)\n","\n","# Hyperparameters\n","VOCAB_SIZE = 100  # Vocabulary size for embedding layer\n","EMBEDDING_DIM = 50  # Size of the word embeddings\n","NUM_FILTERS = 10  # Number of filters in the convolutional layer\n","NUM_CLASSES = 3  # Number of output classes (relation types)\n","\n","# Create model, loss function, and optimizer\n","model = CNNRelationExtractor(VOCAB_SIZE, EMBEDDING_DIM, NUM_FILTERS, NUM_CLASSES)\n","loss_function = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n","\n","# Dummy data for training (list of tokenized sentences with corresponding relation labels)\n","data = [([10, 20, 30, 40, 50], 0), ([50, 60, 70, 80, 90], 1)]\n","batch_size = 2\n","\n","# Training loop\n","for epoch in range(100):\n","    for sentence, label in data:\n","        # Convert sentences and labels to PyTorch tensors\n","        sentence = Variable(torch.LongTensor([sentence]))\n","        label = Variable(torch.LongTensor([label]))\n","\n","        # Zero the gradients before the forward pass\n","        model.zero_grad()\n","        # Forward pass through the model\n","        output = model(sentence)\n","        # Compute the loss\n","        loss = loss_function(output, label)\n","        # Backward pass (compute gradients)\n","        loss.backward()\n","        # Update model parameters\n","        optimizer.step()\n","\n","# Example test case\n","test_sentence = Variable(torch.LongTensor([[10, 20, 30, 40, 50]]))  # Prepare test input\n","with torch.no_grad():  # Disable gradient calculation for testing\n","    prediction = model(test_sentence)  # Get model predictions for the test input\n","    print(prediction)  # Print the predicted class probabilities\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lpe1mZLtNY4u","executionInfo":{"status":"ok","timestamp":1728829957693,"user_tz":-60,"elapsed":899,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"b5929260-e6a3-4225-cf6e-cdacf2cdf1bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[9.9632e-01, 3.3740e-03, 3.0674e-04]])\n"]}]},{"cell_type":"markdown","source":["## 5.5 **Building a Dynamic PyTorch-Based Pipeline**\n","\n","- **Building a Modular Information Extraction Pipeline in PyTorch**:\n","  - This example demonstrates how to build a dynamic pipeline where various NLP tasks such as tokenization, embedding, and entity extraction are modularized and can be assembled based on need.\n","\n"],"metadata":{"id":"nCZJHmx2NDAs"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertForTokenClassification\n","\n","# Load tokenizer and BERT model for token classification (NER)\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","# Define the pipeline stages\n","\n","# Tokenization stage\n","def tokenize(text):\n","    tokens = tokenizer.encode(text, return_tensors=\"pt\")  # Convert text to BERT token IDs as a tensor\n","    return tokens\n","\n","# Run the BERT model on the tokenized input\n","def run_model(tokens):\n","    output = model(tokens)  # Pass tokens through the model\n","    return output.logits  # Return the raw logits (unnormalized predictions)\n","\n","# Dynamic pipeline executor\n","def dynamic_pipeline(text, stages):\n","    for stage in stages:  # Iterate through each stage in the pipeline\n","        text = stage(text)  # Apply the stage and update the result\n","    return text  # Return the final result after all stages\n","\n","# Assemble and execute the pipeline\n","pipeline = [tokenize, run_model]  # Define the stages of the pipeline\n","\n","# Execute the pipeline on the input text\n","result = dynamic_pipeline(\"Hello, my name is John.\", pipeline)\n","\n","# Print the resulting logits (unnormalized scores) from the BERT model\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3tQO0WENbTY","executionInfo":{"status":"ok","timestamp":1728830034571,"user_tz":-60,"elapsed":659,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"b0db6e1a-c787-4173-d750-21df7f25c040"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.0879,  0.1843],\n","         [ 0.0200,  0.2126],\n","         [ 0.2310,  0.0548],\n","         [ 0.2503,  0.3087],\n","         [ 0.0089,  0.2088],\n","         [ 0.0530,  0.2011],\n","         [-0.0215,  0.4724],\n","         [ 0.2831, -0.0015],\n","         [ 0.2905, -0.1730]]], grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","source":["## 5.6 **Error Handling in PyTorch Models**\n","\n","- **Handling Errors in a PyTorch Model Pipeline**:\n","  - A robust pipeline that ensures error handling during different stages of the PyTorch-based information extraction pipeline.\n","\n"],"metadata":{"id":"fESZMF0uNCiJ"}},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForTokenClassification\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","# Define error-safe functions for the pipeline stages\n","\n","# Tokenization with error handling\n","def safe_tokenize(text):\n","    try:\n","        return tokenizer.encode(text, return_tensors=\"pt\")  # Tokenize input text and return tensor\n","    except Exception as e:\n","        print(f\"Error in tokenization: {e}\")\n","        return None  # Return None if an error occurs\n","\n","# Model execution with error handling\n","def safe_model_run(tokens):\n","    try:\n","        return model(tokens).logits  # Run the model and return logits\n","    except Exception as e:\n","        print(f\"Error in model run: {e}\")\n","        return None  # Return None if an error occurs\n","\n","# Define a robust pipeline with error handling\n","def robust_pipeline(text, stages):\n","    for stage in stages:\n","        text = stage(text)  # Apply each stage\n","        if text is None:\n","            break  # Stop if any stage fails\n","    return text  # Return the final result or None if a stage failed\n","\n","# Use the pipeline with error handling\n","pipeline = [safe_tokenize, safe_model_run]\n","result = robust_pipeline(\"Hello, my name is John.\", pipeline)\n","\n","# Print the result (either the logits or None if any stage failed)\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tm3kUoVeNeBP","executionInfo":{"status":"ok","timestamp":1728830072550,"user_tz":-60,"elapsed":869,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"27b88c3e-2ed8-4cd6-901e-ba41fe7fdce2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.0990, -0.5672],\n","         [ 0.3384, -0.5945],\n","         [-0.5949, -0.2023],\n","         [-0.3101, -0.4697],\n","         [-0.9338, -0.0466],\n","         [-0.8299, -0.1092],\n","         [ 0.0443, -0.7669],\n","         [-0.0305, -0.2591],\n","         [-0.0138, -0.2138]]], grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"YCh6-AnfTfc_"}}]}