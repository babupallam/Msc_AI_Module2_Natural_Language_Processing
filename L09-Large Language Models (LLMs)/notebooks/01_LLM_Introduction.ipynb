{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **01_LLM_Introduction**\n",
    "\n"
   ],
   "id": "7ce1cc3276fe7a6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **1. Introduction to Language Models**\n",
    "   - **What is a Language Model (LM)?**\n",
    "     - A model that predicts words or phrases in sentences.\n",
    "     - Helps computers \"understand\" and \"generate\" human-like text.\n",
    "     - Example: Predicting the next word in \"I like to eat ___\" — the model might suggest \"pizza,\" \"ice cream,\" etc.\n",
    "\n",
    "   - **Applications of Language Models:**\n",
    "     - **Text Generation**: Creating new sentences, stories, poems.\n",
    "     - **Machine Translation**: Converting text from one language to another.\n",
    "     - **Sentiment Analysis**: Determining if a statement is positive, negative, or neutral.\n",
    "     - **Chatbots**: Engaging in conversations with users.\n",
    "     - **Autocomplete**: Suggesting words while typing, like in Google search.\n",
    "\n",
    "   - **Types of Language Models:**\n",
    "     - **Statistical Models**: Early models based on probabilities (e.g., N-grams).\n",
    "     - **Neural Network Models**: Modern models using deep learning (e.g., transformers).\n",
    "\n",
    "---\n"
   ],
   "id": "35d50bd3a06434b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **2. What are Large Language Models (LLMs)?**\n",
    "   - **Definition of LLMs:**\n",
    "     - Language models with billions of parameters.\n",
    "     - Can understand and generate complex language.\n",
    "     - Example: Models like GPT-3, GPT-4, which can answer questions, write essays, and even code.\n",
    "\n",
    "   - **Why “Large”?**\n",
    "     - Large refers to the massive size of the model, often billions of parameters.\n",
    "     - **Parameter**: Think of it as a setting or a weight that helps the model decide its answer.\n",
    "\n",
    "   - **Key Features of LLMs:**\n",
    "     - **Context Understanding**: Can process large chunks of text and remember context.\n",
    "     - **Adaptability**: Can be used across a wide range of tasks without task-specific training.\n",
    "     - **Language Generation**: Can generate text that sounds human-like.\n",
    "     - **Multi-Language Support**: Many LLMs understand multiple languages.\n",
    "\n",
    "   - **Popular Examples of LLMs:**\n",
    "     - **GPT (Generative Pre-trained Transformer)**: Known for text generation and used in applications like ChatGPT.\n",
    "     - **BERT (Bidirectional Encoder Representations from Transformers)**: Great for understanding text, answering questions.\n",
    "     - **LLaMA (Large Language Model Meta AI)**: An open-source model with various language capabilities.\n",
    "\n",
    "---\n"
   ],
   "id": "35524d7ab374149b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3. Evolution of Language Models**\n",
    "\n",
    "#### **1. Early Models (1990s - 2000s)**\n",
    "\n",
    "   - **N-grams**:\n",
    "     - Utilized to predict the likelihood of a word appearing based on the previous \\( N-1 \\) words.\n",
    "     - Effective for short sequences and limited contexts but struggled with capturing long-term dependencies and complex sentence structures.\n",
    "\n",
    "   - **Markov Chains**:\n",
    "     - Models text as a sequence of states with probabilistic transitions, predicting the next word based solely on the current state.\n",
    "     - Useful for simple sequences but lacked semantic depth and could not understand nuanced meanings or context shifts, limiting their applicability for complex language understanding.\n",
    "\n",
    "#### **2. Neural Networks and Word Embeddings (2010s)**\n",
    "\n",
    "   - **Word2Vec**:\n",
    "     - Introduced a revolutionary way to represent words as dense vectors (embeddings) in a continuous space.\n",
    "     - Captured semantic similarity by positioning similar words (e.g., \"king\" and \"queen\") closer in vector space, leading to practical applications like analogy-solving (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
    "\n",
    "   - **LSTM (Long Short-Term Memory)**:\n",
    "     - A specialized form of RNN that mitigates the problem of vanishing gradients, allowing it to retain information over longer sequences.\n",
    "     - Enabled models to handle contextual dependencies better, significantly improving applications like speech recognition and machine translation.\n",
    "\n",
    "#### **3. The Transformer Era (Late 2010s)**\n",
    "\n",
    "   - **Transformers**:\n",
    "     - Introduced a new architecture that abandoned the sequential processing of RNNs in favor of parallelism, which vastly accelerated training and allowed for the processing of larger datasets.\n",
    "     - Pioneered by the \"Attention Is All You Need\" paper (Vaswani et al., 2017), this model architecture laid the groundwork for significant advancements in NLP.\n",
    "\n",
    "   - **Self-Attention Mechanism**:\n",
    "     - Enabled models to weigh different parts of a sentence according to their relevance, allowing for nuanced understanding of relationships between words (e.g., understanding that \"it\" in \"The dog barked. It ran away.\" refers to \"dog\").\n",
    "     - Became the backbone of transformer-based models, boosting their ability to understand complex language tasks and long-range dependencies within text.\n",
    "\n",
    "#### **4. Modern Large Language Models (2020s)**\n",
    "\n",
    "   - **BERT, GPT-2, GPT-3, ChatGPT**:\n",
    "     - These models represent a leap in NLP capabilities, showcasing the power of transformers and massive data training.\n",
    "     - Capable of handling a diverse range of tasks (e.g., translation, summarization, question-answering) without extensive task-specific training, thanks to transfer learning from large corpora.\n",
    "\n",
    "   - **Scaling Up**:\n",
    "     - The modern approach has focused on increasing model parameters (reaching billions or even trillions of parameters) to improve performance and handle complex tasks with nuanced understanding.\n",
    "     - This scalability has pushed the boundaries of what language models can achieve, enabling sophisticated applications like real-time chatbots, automated code generation, and advanced data analysis.\n",
    "\n"
   ],
   "id": "a56226b20c1f1b57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **4. Key Concepts in Large Language Models (LLMs)**\n",
    "\n",
    "#### **1. Self-Attention Mechanism**\n",
    "\n",
    "   - **Functionality**:\n",
    "     - The self-attention mechanism enables the model to focus on specific words within a sentence, helping it understand relationships and dependencies across different parts of the text.\n",
    "     - This mechanism weighs the importance of each word in relation to others, allowing the model to capture context effectively, even over long distances within the text.\n",
    "\n",
    "   - **Example**:\n",
    "     - In the sentence “She loves her dog because it is loyal,” self-attention allows the model to associate “it” with “dog,” recognizing the intended reference despite the intervening words. \n",
    "     - This focus on relevant parts of the text is crucial for tasks requiring deep understanding, like summarization or context-based generation.\n",
    "\n",
    "#### **2. Parameters and Model Size**\n",
    "\n",
    "   - **Definition**:\n",
    "     - Parameters are adjustable values within the model, akin to settings that shape how the model interprets and generates language. These parameters include weights and biases, which the model fine-tunes during training to learn patterns in data.\n",
    "   \n",
    "   - **Significance**:\n",
    "     - Generally, a higher parameter count allows the model to capture more complex patterns and subtleties in the language. This capability translates into better performance across diverse tasks, especially in larger, more nuanced datasets.\n",
    "     - However, an increase in parameters also demands more computational resources, memory, and energy, making it a balancing act between performance gains and practicality.\n",
    "\n",
    "   - **Example**:\n",
    "     - GPT-3, with its 175 billion parameters, can manage a vast range of tasks, outperforming smaller models with only millions of parameters in complex language tasks like nuanced conversation, creative writing, and advanced summarization.\n",
    "     - This scaling effect has enabled newer models to achieve performance levels previously unattainable with smaller architectures.\n",
    "\n",
    "#### **3. Fine-tuning**\n",
    "\n",
    "   - **Purpose**:\n",
    "     - Fine-tuning is the process of training an LLM on specific datasets to optimize its performance for particular applications. By continuing training on task-specific or domain-specific data, the model can adapt and improve accuracy for specialized contexts.\n",
    "   \n",
    "   - **Process**:\n",
    "     - Fine-tuning involves using a smaller, curated dataset (e.g., legal documents, medical text) to adjust the model's weights slightly. This adjustment helps the model learn the nuances and terminology of the target domain, improving its ability to handle related queries accurately.\n",
    "\n",
    "   - **Example**:\n",
    "     - A general-purpose LLM might be fine-tuned on a dataset of medical text, transforming it into a model specialized for healthcare. This fine-tuned model can then handle healthcare-related queries more accurately, identifying terminology and context with greater precision than a general model.\n",
    "\n",
    "--- \n"
   ],
   "id": "87688516c026a00f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **5. How Do Large Language Models (LLMs) Work?**\n",
    "\n",
    "#### **1. Pre-training**\n",
    "\n",
    "   - **Objective**:\n",
    "     - In the pre-training phase, the model is exposed to vast amounts of diverse text data, such as books, websites, and articles, allowing it to learn general language structures, syntax, grammar, and semantic patterns.\n",
    "     - This stage helps the model acquire a foundational understanding of language, including basic grammar, word meanings, and common phrases. This generalized knowledge forms the basis of the model's understanding.\n",
    "\n",
    "   - **Process**:\n",
    "     - During pre-training, the model is usually trained in an unsupervised or self-supervised manner. For example, it might be tasked with predicting the next word in a sentence or filling in missing words. These tasks encourage the model to recognize language patterns and associations.\n",
    "\n",
    "   - **Example**:\n",
    "     - By training on sentences like “The cat is on the ___,” the model learns to predict words such as “mat” based on context. This ability to predict words helps it understand the flow and structure of language.\n",
    "\n",
    "#### **2. Fine-tuning**\n",
    "\n",
    "   - **Purpose**:\n",
    "     - Fine-tuning is the process of adapting the pre-trained model to specific tasks or domains by training it further on a curated dataset. This allows the model to refine its understanding and adjust its responses for particular applications, such as question answering, sentiment analysis, or conversational tasks.\n",
    "\n",
    "   - **Process**:\n",
    "     - In this stage, the model’s parameters are slightly adjusted to specialize in the target domain. For example, to create a medical chatbot, the model might be fine-tuned on medical texts and conversations, enabling it to understand medical terminology and provide accurate responses to healthcare-related queries.\n",
    "\n",
    "   - **Example**:\n",
    "     - Fine-tuning a general model on customer service chat logs could transform it into a specialized chatbot that can handle customer inquiries efficiently, with responses that are more relevant and contextually accurate.\n",
    "\n",
    "#### **3. Inference**\n",
    "\n",
    "   - **Purpose**:\n",
    "     - Inference is the stage where the trained and fine-tuned model is deployed to make predictions, generate responses, or provide outputs based on user input. This is the final, practical application stage where the model’s capabilities are utilized.\n",
    "\n",
    "   - **Process**:\n",
    "     - When a user inputs text (e.g., a question or a prompt), the model analyzes the input, leveraging its pre-trained and fine-tuned knowledge to generate a response. This process involves selecting the most likely sequence of words based on the input and the knowledge it has acquired through training.\n",
    "   \n",
    "   - **Example**:\n",
    "     - When a user asks ChatGPT, “What is the capital of France?” the model processes the question and generates the response “Paris.” This response is based on patterns and knowledge it has learned, as well as the statistical relationships between words and facts within its training data.\n",
    "\n",
    "---\n",
    "\n",
    "Together, these stages — pre-training, fine-tuning, and inference — form the lifecycle of an LLM. Each stage contributes to the model’s ability to understand, adapt, and respond accurately, enabling it to perform complex language tasks across various domains."
   ],
   "id": "f7fea9531606c73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **6. Applications of Large Language Models (LLMs)**\n",
    "\n",
    "- **Text Generation**\n",
    "   - **Purpose**: Automates content creation, enabling efficient production of various written materials.\n",
    "   - **Uses**:\n",
    "     - Drafting articles, stories, and creative writing.\n",
    "     - Generating posts for social media, blogs, and marketing campaigns.\n",
    "     - Assisting with brainstorming by generating ideas or continuing storylines.\n",
    "   - **Example**:\n",
    "     - A news organization uses an LLM to draft articles on current events based on specific keywords or headlines provided by users.\n",
    "\n",
    "- **Translation**\n",
    "   - **Purpose**: Enhances cross-lingual communication by converting text accurately between languages.\n",
    "   - **Uses**:\n",
    "     - Translating documents, emails, and online content into multiple languages.\n",
    "     - Providing real-time translation in apps, enhancing accessibility.\n",
    "     - Supporting bilingual or multilingual customer support.\n",
    "   - **Example**:\n",
    "     - Translating phrases like “Hello, how are you?” into Spanish as “Hola, ¿cómo estás?” while retaining context, tone, and accuracy.\n",
    "\n",
    "- **Summarization**\n",
    "   - **Purpose**: Makes lengthy documents more accessible by condensing key information into shorter, focused summaries.\n",
    "   - **Uses**:\n",
    "     - Summarizing research papers, articles, or meeting notes for quick review.\n",
    "     - Creating executive summaries for business reports.\n",
    "     - Reducing content to bullet points or abstracts for easier consumption.\n",
    "   - **Example**:\n",
    "     - Summarizing a 10-page scientific paper into a concise outline that captures main findings, methods, and conclusions.\n",
    "\n",
    "- **Chatbots and Virtual Assistants**\n",
    "   - **Purpose**: Provides interactive, human-like responses for customer support and virtual assistance.\n",
    "   - **Uses**:\n",
    "     - Powering customer service chatbots that handle FAQs, product support, and user queries.\n",
    "     - Acting as virtual assistants for scheduling, reminders, and task management.\n",
    "     - Engaging in personalized conversations for apps, websites, and smart home devices.\n",
    "   - **Example**:\n",
    "     - A customer support chatbot that answers questions, processes orders, and troubleshoots basic issues, enhancing user experience with instant responses.\n",
    "\n",
    "- **Sentiment Analysis**\n",
    "   - **Purpose**: Identifies opinions or emotions in text, useful for understanding customer attitudes and public sentiment.\n",
    "   - **Uses**:\n",
    "     - Analyzing customer reviews to determine satisfaction or detect complaints.\n",
    "     - Monitoring social media sentiment toward products, brands, or events.\n",
    "     - Conducting market research by understanding public opinion and trends.\n",
    "   - **Example**:\n",
    "     - Analyzing tweets about a new product launch to determine if responses are mostly positive, negative, or neutral, guiding marketing strategies.\n",
    "\n",
    "- **Information Retrieval and Question Answering (Q&A)**\n",
    "   - **Purpose**: Extracts precise answers from large datasets or documents, making it easy to find specific information.\n",
    "   - **Uses**:\n",
    "     - Answering specific questions based on product manuals, legal documents, or databases.\n",
    "     - Assisting with research by retrieving relevant facts from extensive knowledge sources.\n",
    "     - Powering Q&A bots that respond accurately based on provided context.\n",
    "   - **Example**:\n",
    "     - A model trained on Wikipedia or encyclopedic data that answers “Who is the President of the United States?” with up-to-date information.\n",
    "\n",
    "--- \n",
    "\n",
    "This structured approach highlights how LLMs support diverse applications, from automating content generation to enhancing customer service, making them versatile tools in both business and everyday contexts."
   ],
   "id": "7ac673358b0d204d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **7. Advantages of Large Language Models**\n",
    "   - **Broad Knowledge**:\n",
    "     - Trained on diverse text, so LLMs know a lot about different topics.\n",
    "     - Example: Can answer questions on history, science, and current events.\n",
    "\n",
    "   - **Versatility**:\n",
    "     - Can perform many tasks without needing separate models.\n",
    "     - Example: A single model can translate, summarize, and answer questions.\n",
    "\n",
    "   - **Context Awareness**:\n",
    "     - Maintains context over long passages, understanding complex relationships between words.\n",
    "     - Example: In a multi-sentence story, it knows who \"he\" or \"she\" refers to.\n",
    "\n",
    "---\n"
   ],
   "id": "2a93b3500bc6902d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **8. Challenges of Large Language Models (LLMs)**\n",
    "\n",
    "- **Bias in Training Data**\n",
    "   - **Description**: LLMs learn from vast datasets, which may contain biases related to gender, race, age, or other factors. As a result, models can unintentionally propagate or even amplify these biases in generated content.\n",
    "   - **Implications**:\n",
    "     - Can reinforce stereotypes or produce responses that are biased, impacting fairness in applications.\n",
    "     - May lead to unintended discrimination if used in sensitive fields like hiring or law.\n",
    "   - **Example**:\n",
    "     - If training data shows gender biases in professional roles (e.g., “nurses” as female, “engineers” as male), the model might generate responses that reflect these biases.\n",
    "\n",
    "- **Huge Resource Requirements**\n",
    "   - **Description**: Training and deploying LLMs require significant computational resources, including powerful hardware like GPUs and TPUs, as well as substantial energy consumption.\n",
    "   - **Implications**:\n",
    "     - High costs associated with hardware and energy consumption can limit accessibility to larger organizations.\n",
    "     - Raises environmental concerns due to the carbon footprint of prolonged training sessions.\n",
    "   - **Example**:\n",
    "     - Training GPT-3 reportedly required thousands of GPUs over several weeks, making it resource-intensive and costly.\n",
    "\n",
    "- **Privacy Risks**\n",
    "   - **Description**: If LLMs are trained on data containing sensitive or personal information, they risk inadvertently disclosing private details, even if unintended.\n",
    "   - **Implications**:\n",
    "     - Potential for privacy breaches, especially if the model is accessible to the public and used in customer-facing applications.\n",
    "     - Raises ethical concerns around data collection, storage, and anonymization.\n",
    "   - **Example**:\n",
    "     - A chatbot trained on customer interactions could accidentally produce a previous user’s private details, such as an address or phone number, if not properly filtered.\n",
    "\n",
    "- **Not Always Accurate (Hallucination)**\n",
    "   - **Description**: LLMs sometimes produce responses that sound convincing but are factually incorrect or misleading, a phenomenon often referred to as “hallucination.”\n",
    "   - **Implications**:\n",
    "     - Can lead to misinformation, especially if users rely on the model for accurate information in critical fields like healthcare or finance.\n",
    "     - Undermines trust if users recognize frequent inaccuracies in responses.\n",
    "   - **Example**:\n",
    "     - The model may confidently answer a question about a fictional event or made-up statistic as if it were real, potentially misleading users.\n",
    "\n",
    "---\n",
    "\n",
    "These challenges underscore the importance of careful design, ethical considerations, and transparency in the deployment of LLMs, especially as they become more widely used across various industries."
   ],
   "id": "f04e468333bfc7b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **9. Examples of Popular LLMs**\n",
    "   - **GPT-3 (OpenAI)**:\n",
    "     - Known for its broad language abilities, writing text, answering questions, summarizing.\n",
    "   \n",
    "   - **BERT (Google)**:\n",
    "     - Good at understanding sentence context, answering questions, and text classification.\n",
    "   \n",
    "   - **LLaMA (Meta AI)**:\n",
    "     - Open-source model focused on tasks like text generation, summarization, and conversation.\n",
    "\n",
    "---\n"
   ],
   "id": "3f436dd5c088dd19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a7cbdfe1abbaa425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **10. Future of Large Language Models (LLMs)**\n",
    "\n",
    "- **More Efficient Models**\n",
    "   - **Objective**: Reduce computational and energy costs, making LLMs more accessible and sustainable.\n",
    "   - **Techniques**:\n",
    "     - Developing methods like LoRA (Low-Rank Adaptation) to enable faster and more efficient training.\n",
    "     - Exploring sparsity, quantization, and distillation to shrink model size while maintaining performance.\n",
    "   - **Example**:\n",
    "     - Implementing LoRA to train large models with lower hardware requirements, which can help in deploying LLMs on more compact devices.\n",
    "\n",
    "- **Improved Accuracy and Safety**\n",
    "   - **Objective**: Minimize hallucination, bias, and misinformation in model outputs for higher trustworthiness.\n",
    "   - **Focus Areas**:\n",
    "     - Enhancing training datasets to reduce misinformation and applying filtering techniques to manage biases.\n",
    "     - Developing more rigorous testing and fine-tuning methods to produce accurate, consistent responses.\n",
    "   - **Implication**:\n",
    "     - A model with higher accuracy can be more reliably used in sensitive fields like healthcare, finance, and law.\n",
    "\n",
    "- **Ethics and Privacy**\n",
    "   - **Objective**: Ensure LLMs are used ethically and protect user data to maintain privacy.\n",
    "   - **Strategies**:\n",
    "     - Introducing techniques to remove or anonymize sensitive data from training sets.\n",
    "     - Creating guidelines and frameworks for responsible AI use, focusing on transparency and user consent.\n",
    "   - **Example**:\n",
    "     - Developing tools to identify and exclude personal or private information from datasets to prevent unintentional disclosures.\n",
    "\n",
    "- **Expansion to Multimodal Models**\n",
    "   - **Objective**: Extend LLMs to understand and process multiple types of data, like images, audio, and video.\n",
    "   - **Capabilities**:\n",
    "     - Allowing models to generate and interpret text, images, and audio together for richer interactions.\n",
    "     - Enabling applications like image description, video summarization, and audio-to-text with contextual awareness.\n",
    "   - **Example**:\n",
    "     - A model that generates descriptive captions for images, allowing for applications like assisting visually impaired users.\n",
    "\n",
    "---\n",
    "\n",
    "These advancements highlight a promising future for LLMs, with a focus on efficiency, ethical use, multimodal integration, and reliability improvements, driving innovation while addressing current limitations."
   ],
   "id": "65197e1898df946e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "48d167401917ccbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9de9580413195d4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bf7bcd9dd2994cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "734816e062692300"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "26113f231a420db5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b52b89ed1f21f03a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fb96031722154575"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bec187f00fa2385f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1cd1197c027d3448"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e34012a3b7481833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "24bb4e6f2347f5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "38675a01068758b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fe66f9910bcabac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6c4cc88a46f1bb66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6b92908068b0783d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "218705114f085c59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4e607f6e3d142de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1b87212a4453b398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "798434a5c9e1c76a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "326fdd90046d3fdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f649e2126e18969d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "aedad40a4ab174f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ed2affb3a58e4dc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d29dd8e94ae8b8af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9eb2392bad441997"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e4b0e75f14c0c052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1a8079d77f7e0485"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e4a986c0428d77a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e56e287f3ec87bda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8c0eb51691dee9f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6f649ae73f332048"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a923a6c1b6c13ec4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "78973f267e808ce6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "388dc593a935f780"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "11b64b3a8b54bdb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d2912de80c52c8c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "502262adecd3315e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "69c27115549331c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d1147425bc7746c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b2437e49d72a70b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4df7560c166a31dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5c6c67f51eac674e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
