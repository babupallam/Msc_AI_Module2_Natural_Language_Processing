{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Can we do chatbot with normal RNN or CNN? Why we do with Seq2Seq Models?\n",
    "\n"
   ],
   "metadata": {
    "id": "kcI0EdyDqYtf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Yes, chatbots can technically be built using standard **RNNs (Recurrent Neural Networks)** or **CNNs (Convolutional Neural Networks)**, but there are reasons why **Seq2Seq models** are preferred, especially for tasks involving dialogue generation or machine translation. Here's why:\n"
   ],
   "metadata": {
    "id": "PQ-kPj9FqXT6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 1. **RNNs for Chatbots**\n",
    "   - **What They Are**: RNNs are designed to handle sequences of data, where each input depends on previous inputs, making them useful for tasks like language modeling.\n",
    "   - **Why It’s Not Ideal**:\n",
    "     - **Fixed Input/Output Length**: Standard RNNs are generally designed for fixed-length input and output sequences, which is not ideal for chatbots where input and output can be of varying lengths.\n",
    "     - **Vanishing Gradient Problem**: RNNs struggle with long-term dependencies due to the vanishing gradient issue, making them less effective for long conversations or sentences.\n",
    "     - **One-to-One Mapping**: In its basic form, RNNs are designed for simple tasks where there is a one-to-one mapping of input to output (e.g., sentiment analysis). In chatbots, the relationship between input and output is more complex and varies in length.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "w6eVAeTgqXQW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 2. **CNNs for Chatbots**\n",
    "   - **What They Are**: CNNs are typically used for image processing but can be applied to NLP tasks by treating text as a 1D sequence of data, using convolutional filters to capture local patterns (like phrases or n-grams).\n",
    "   - **Why It’s Not Ideal**:\n",
    "     - **Local Dependencies**: CNNs are good at capturing local dependencies (e.g., nearby words or phrases), but they lack the ability to capture long-range dependencies between words in a sequence. This is crucial for understanding context in conversations.\n",
    "     - **No Memory**: CNNs do not have a mechanism to \"remember\" previous words or sentences, which is essential for chatbots to maintain context over long conversations.\n",
    "     - **Fixed Context Window**: The convolutional filters operate within a fixed window of text, meaning the model may miss important information that falls outside of that window.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "PiyBhG1HqXNy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3. **Seq2Seq Models for Chatbots**\n",
    "   - **Why Seq2Seq Is Used**:\n",
    "     - **Variable-Length Input and Output**: Seq2Seq models are specifically designed to handle input and output sequences of varying lengths, which is a fundamental requirement for conversational AI (e.g., the user can ask a short question, and the chatbot can give a long response).\n",
    "     - **Encoder-Decoder Architecture**: Seq2Seq models have an **encoder** that processes the entire input sequence and a **decoder** that generates the output sequence. This architecture allows the model to translate input sentences (queries) into output sentences (responses).\n",
    "     - **Handles Long-Term Dependencies**: Seq2Seq models can better capture long-term dependencies between words in a sentence because of the use of LSTMs, GRUs, or transformers in the encoder-decoder setup. This helps in generating more coherent and contextually appropriate responses.\n",
    "     - **Attention Mechanisms**: Modern Seq2Seq models incorporate attention, allowing the model to \"attend\" to different parts of the input sequence while generating each word in the output. This solves the problem of information bottleneck in long sequences.\n",
    "   \n",
    "**Example of Input/Output Length Variability**:\n",
    "- Input: \"How are you?\"\n",
    "- Output: \"I’m doing well, thank you! How about you?\"\n",
    "\n",
    "Here, the input and output have different lengths, which is handled well by Seq2Seq models but would be more challenging for standard RNNs or CNNs.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "hW1oJFESqXLS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fundamental requirements for conversational AI"
   ],
   "metadata": {
    "id": "TMfDwXYDqvlo"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. **Natural Language Understanding (NLU)**\n",
    "   - **Intent Recognition**: The ability to understand the user’s intention from the input text (e.g., is the user asking a question, giving a command, or making a statement?).\n",
    "   - **Entity Recognition**: Extracting important entities (e.g., names, dates, locations) from the user input to provide relevant responses.\n",
    "\n",
    "### 2. **Natural Language Generation (NLG)**\n",
    "   - **Response Generation**: The capability to generate coherent, contextually appropriate responses to user inputs.\n",
    "   - **Context Retention**: The ability to maintain context over multiple turns in a conversation, especially in long dialogues.\n",
    "\n",
    "### 3. **Handling Variable-Length Sequences**\n",
    "   - The ability to manage inputs and outputs of varying lengths, as conversations often involve exchanges of different sentence lengths.\n",
    "   - Models like Seq2Seq or transformers are used to map variable-length input sequences (user queries) to variable-length output sequences (chatbot responses).\n",
    "\n",
    "### 4. **Conversational Flow**\n",
    "   - **Multi-turn Dialogue Management**: Ensuring the chatbot can keep track of conversation history and respond appropriately in multi-turn interactions.\n",
    "   - **Context Awareness**: The chatbot must retain context from earlier parts of the conversation to ensure that responses are meaningful and relevant.\n",
    "\n",
    "### 5. **Training Data**\n",
    "   - **Dialogue Dataset**: A large, diverse set of conversational data is required for training the chatbot to understand and generate human-like dialogue (e.g., Cornell Movie Dialogues Corpus).\n",
    "   - **Preprocessing**: Data should be cleaned, tokenized, and normalized to make it suitable for training (removing noise, handling special characters, etc.).\n",
    "\n",
    "### 6. **Sequence Modeling**\n",
    "   - **Seq2Seq Models**: Required for converting sequences of user input into sequences of output responses. These models are key to handling conversational tasks, especially when paired with attention mechanisms.\n",
    "   - **Transformer Models**: Often used for more advanced, scalable solutions due to their ability to capture long-term dependencies efficiently.\n",
    "\n",
    "### 7. **Decoding Strategies**\n",
    "   - **Greedy Search / Beam Search**: Efficient methods to decode responses by choosing the most probable word at each time step or exploring multiple response paths for better accuracy.\n",
    "   - **Sampling Techniques**: Advanced methods like top-k sampling or nucleus (top-p) sampling to generate more diverse and context-aware responses.\n",
    "\n",
    "### 8. **Response Quality**\n",
    "   - **Coherence**: The chatbot’s responses must be logically consistent with the previous conversation context.\n",
    "   - **Fluency**: The responses should be grammatically correct and natural-sounding, like human-generated language.\n",
    "\n",
    "### 9. **Error Handling**\n",
    "   - **Fallback Responses**: Ability to handle unrecognized queries or out-of-scope topics gracefully with fallback responses (e.g., \"I’m sorry, I didn’t understand that.\").\n",
    "   - **Clarification**: Asking clarifying questions if the user's intent or query is ambiguous.\n",
    "\n",
    "### 10. **Real-Time Processing**\n",
    "   - **Low Latency**: The chatbot must generate responses quickly and efficiently to maintain a smooth user experience.\n",
    "   - **Scalability**: The system must be able to handle multiple users and large volumes of conversations simultaneously.\n",
    "\n",
    "### 11. **Model Evaluation**\n",
    "   - **Quantitative Metrics**: Metrics like BLEU, ROUGE, or perplexity to evaluate the chatbot’s language generation capabilities.\n",
    "   - **User Feedback**: Qualitative evaluation through real-time interaction, collecting user feedback to improve the chatbot’s relevance and engagement.\n",
    "\n",
    "### 12. **Personalization**\n",
    "   - **User Profiles**: Ability to store and retrieve user-specific data (e.g., preferences, previous conversations) to make the chatbot more personalized and context-aware.\n",
    "   - **Adaptive Responses**: Dynamically adapting responses based on user feedback or past interactions to improve the conversational flow.\n",
    "\n",
    "### 13. **Deployment Environment**\n",
    "   - **Web Interface / API**: A framework like Flask, Dialogflow, or Microsoft Bot Framework to deploy the chatbot for real-time interaction with users.\n",
    "   - **Cloud Infrastructure**: For scalability and accessibility, cloud platforms (e.g., AWS, Google Cloud) are essential for hosting and serving the chatbot to a large user base.\n"
   ],
   "metadata": {
    "id": "4sE8Yt5dqviD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observations based on Encoder-Decoder Model for Chatbots"
   ],
   "metadata": {
    "id": "4ixhXz_VtK8c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why Do We Need an Encoder-Decoder Model for Chatbots?\n",
    "\n",
    "- **Variable-Length Input and Output**:\n",
    "  - Chatbot conversations often involve varying lengths of input (user queries) and output (responses).\n",
    "  - The encoder-decoder model handles this by processing input and output sequences of different lengths, unlike traditional models that expect fixed-size input and output.\n",
    "\n",
    "- **Capturing Complex Dependencies**:\n",
    "  - Conversations are complex, and the relationship between input and output is not always straightforward.\n",
    "  - The encoder-decoder architecture can capture long-term dependencies in sentences, ensuring more coherent responses by preserving context.\n",
    "\n",
    "- **Translating Input to Output**:\n",
    "  - The encoder-decoder model is designed for tasks where one sequence (input) needs to be \"translated\" into another (output), making it ideal for tasks like machine translation, summarization, and chatbots.\n",
    "\n",
    "### How Does an Encoder-Decoder Model Work?\n",
    "\n",
    "- **Encoder**:\n",
    "  - Processes the entire input sequence (user query) word by word.\n",
    "  - Encodes the sequence into a fixed-length vector (context vector) that summarizes the entire input.\n",
    "\n",
    "- **Decoder**:\n",
    "  - Takes the context vector from the encoder and generates the output sequence (chatbot response) one word at a time.\n",
    "  - Uses the previous word generated or the target word (during training) as input for the next word prediction.\n",
    "\n",
    "- **Context Vector**:\n",
    "  - The context vector contains the compressed information of the entire input sequence, which the decoder uses to produce relevant outputs.\n",
    "\n",
    "- **Attention Mechanism**:\n",
    "  - Attention is added to the decoder to focus on different parts of the input sequence during decoding, improving performance by giving more weight to important words in the input.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are the Benefits of the Encoder-Decoder Model?\n",
    "\n",
    "- **Handles Long Sequences**:\n",
    "  - The model can handle long input and output sequences by compressing input into a context vector and generating responses step by step.\n",
    "\n",
    "- **Maintains Flexibility**:\n",
    "  - Allows for flexibility in input-output length, crucial in conversations where sentences and responses vary greatly in size.\n",
    "\n",
    "- **Better Context Retention**:\n",
    "  - The encoder captures the entire input context, ensuring that the chatbot’s response is relevant and context-aware.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Use Simple RNNs Instead of Encoder-Decoder?\n",
    "\n",
    "- **Fixed-Length Output**:\n",
    "  - Simple RNNs struggle with variable-length input/output, which is common in conversations.\n",
    "  \n",
    "- **Loss of Long-Term Dependencies**:\n",
    "  - RNNs suffer from the vanishing gradient problem, which limits their ability to remember long-term dependencies in input sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### What Problems Does the Encoder-Decoder Solve in Chatbots?\n",
    "\n",
    "- **Variable-Length Conversations**:\n",
    "  - Allows the chatbot to generate responses that match the length of user inputs dynamically.\n",
    "  \n",
    "- **Context Preservation**:\n",
    "  - The context vector preserves the overall meaning of the input sequence, improving the relevance of generated responses.\n",
    "\n",
    "- **Handling Complex Queries**:\n",
    "  - The encoder-decoder can handle complex user queries that require generating responses based on both the meaning and structure of the input.\n",
    "\n",
    "---\n",
    "\n",
    "### When Should You Use an Encoder-Decoder Model?\n",
    "\n",
    "- **For Conversations**:\n",
    "  - Ideal for chatbots where input and output sequences are conversational and vary in length.\n",
    "\n",
    "- **For Translation or Summarization**:\n",
    "  - Suitable for tasks like machine translation or text summarization, where one sequence needs to be converted into another.\n",
    "\n",
    "- **For Long Sequences**:\n",
    "  - Necessary when handling long input sequences where simple models would lose important context or fail to generate relevant responses.\n",
    "\n",
    "---\n",
    "\n",
    "### How Is Attention Used in Encoder-Decoder Models?\n",
    "\n",
    "- **Focus on Important Words**:\n",
    "  - The attention mechanism helps the decoder focus on important words from the input sequence while generating each word of the response.\n",
    "\n",
    "- **Improves Accuracy**:\n",
    "  - By focusing on different parts of the input at different stages of decoding, attention improves the accuracy and relevance of the generated response.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are Common Issues with Encoder-Decoder Models?\n",
    "\n",
    "- **Information Bottleneck**:\n",
    "  - In basic encoder-decoder models, the entire input sequence is compressed into a single context vector, which can lead to information loss, especially with long inputs.\n",
    "\n",
    "- **Solution – Attention**:\n",
    "  - Attention mechanisms help solve this by allowing the decoder to access all encoder hidden states, reducing the reliance on a single context vector.\n",
    "\n",
    "---\n",
    "\n",
    "### Other Questions to Explore:\n",
    "\n",
    "- **How Does the Encoder-Decoder Model Handle Multi-Turn Conversations?**\n",
    "  - Multi-turn conversations require additional mechanisms like memory modules or context windows to maintain context across multiple exchanges.\n",
    "\n",
    "- **Why Do We Need Teacher Forcing During Training?**\n",
    "  - Teacher forcing helps the model learn by feeding the correct output as the next input during training, speeding up convergence and improving the accuracy of the response.\n",
    "\n",
    "- **How Can the Encoder-Decoder Model Be Improved with Transformers?**\n",
    "  - Transformer models eliminate the need for sequential data processing (like in RNNs) by using self-attention mechanisms, improving both performance and scalability.\n",
    "\n",
    "- **What Happens if We Don’t Use Attention in the Decoder?**\n",
    "  - Without attention, the decoder relies solely on the fixed context vector, which can lead to poor performance, especially in long conversations or sentences.\n"
   ],
   "metadata": {
    "id": "Ud8wV_E6qvEI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Observations from Current Research"
   ],
   "metadata": {
    "id": "P8jekgGcqvBY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Why Do We Need Attention Mechanisms in Chatbots?\n",
    "\n",
    "- **Issue with Standard Seq2Seq Models**:\n",
    "  - Standard Seq2Seq models compress the entire input into a single fixed-size context vector, which can lead to information loss, especially for longer sequences.\n",
    "  \n",
    "- **Solution with Attention**:\n",
    "  - Attention allows the model to focus on different parts of the input sequence during each step of decoding, helping the chatbot generate more accurate and contextually relevant responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How Does the Transformer Model Improve Over Seq2Seq?\n",
    "\n",
    "- **Self-Attention**:\n",
    "  - Instead of processing sequences one step at a time (like RNNs), transformers use self-attention to compute dependencies between all words in the input sequence simultaneously.\n",
    "  \n",
    "- **Parallelization**:\n",
    "  - Transformers allow for better parallelization during training, making them faster and more efficient than RNN-based Seq2Seq models.\n",
    "\n",
    "- **Improved Long-Term Dependencies**:\n",
    "  - Transformers handle long-term dependencies better because each word can directly attend to all others, regardless of distance in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why Is Teacher Forcing Used in Seq2Seq Model Training?\n",
    "\n",
    "- **Speeding Up Convergence**:\n",
    "  - Teacher forcing involves using the actual target word as the next input during training, rather than the model’s own prediction. This helps the model converge faster by keeping it on track.\n",
    "  \n",
    "- **Mitigating Error Accumulation**:\n",
    "  - Without teacher forcing, errors in early predictions can compound and lead to worse performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How Does Beam Search Improve Over Greedy Search in Chatbot Decoding?\n",
    "\n",
    "- **Greedy Search Limitation**:\n",
    "  - Greedy search only chooses the highest probability word at each step, which may lead to locally optimal but globally suboptimal responses.\n",
    "\n",
    "- **Beam Search Advantage**:\n",
    "  - Beam search keeps track of multiple potential sequences at each step, allowing the chatbot to explore several possible responses and choose the best one overall.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What Is the Role of Pre-trained Language Models in Chatbots?\n",
    "\n",
    "- **Pre-training on Large Datasets**:\n",
    "  - Models like GPT-3, BERT, and T5 are pre-trained on massive corpora and then fine-tuned for specific chatbot tasks, allowing them to leverage vast amounts of general knowledge.\n",
    "  \n",
    "- **Transfer Learning**:\n",
    "  - These models are fine-tuned for specific domains, making them highly adaptable for various conversational tasks with relatively little data.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. How Does Nucleus (Top-p) Sampling Add Diversity to Chatbot Responses?\n",
    "\n",
    "- **Limitation of Greedy/Top-k Sampling**:\n",
    "  - Greedy and top-k sampling often lead to repetitive or deterministic responses by always picking the most probable words.\n",
    "  \n",
    "- **Nucleus Sampling**:\n",
    "  - Nucleus sampling chooses from the smallest set of words whose cumulative probability exceeds a threshold `p`. This adds randomness and diversity, making responses more human-like and less predictable.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What Is the Vanishing Gradient Problem in RNNs and How Does It Affect Chatbots?\n",
    "\n",
    "- **What It Is**:\n",
    "  - The vanishing gradient problem occurs when gradients become too small during backpropagation through time (BPTT), making it difficult for the model to learn long-term dependencies.\n",
    "  \n",
    "- **Impact on Chatbots**:\n",
    "  - RNNs struggle to remember earlier parts of long conversations due to vanishing gradients, leading to poor context retention and irrelevant responses in chatbots.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Why Is Context Retention Crucial for Chatbots?\n",
    "\n",
    "- **Maintaining Conversation Flow**:\n",
    "  - In multi-turn conversations, the chatbot must remember previous exchanges to provide contextually relevant responses.\n",
    "  \n",
    "- **Avoiding Repetition**:\n",
    "  - Good context retention prevents the chatbot from repeating itself or giving answers that ignore previous user inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How Do Large Language Models Handle Multi-Turn Conversations?\n",
    "\n",
    "- **Memory and History**:\n",
    "  - Large models like GPT-3 handle multi-turn dialogues by keeping track of the conversation history as part of the input, allowing the chatbot to reference earlier parts of the dialogue when generating responses.\n",
    "  \n",
    "- **Context Window**:\n",
    "  - The model can only retain context within a certain window (e.g., 1024 tokens for GPT-3), which may limit its ability to handle very long conversations.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. What Is the Role of Transfer Learning in Chatbots?\n",
    "\n",
    "- **Pre-trained Models**:\n",
    "  - Transfer learning allows chatbots to leverage large pre-trained models that have already learned general language patterns, reducing the need for extensive training on domain-specific data.\n",
    "  \n",
    "- **Fine-Tuning**:\n",
    "  - Fine-tuning on smaller, domain-specific datasets helps tailor the chatbot’s responses to the target application (e.g., customer support, healthcare).\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Why Are Seq2Seq Models Preferred Over Simple RNNs in Chatbots?\n",
    "\n",
    "- **Handling Variable-Length Input/Output**:\n",
    "  - Seq2Seq models can handle input and output sequences of different lengths, which is critical for conversations where queries and responses can vary in size.\n",
    "  \n",
    "- **Long-Term Dependencies**:\n",
    "  - Seq2Seq models, especially with attention mechanisms, are better suited to capturing long-term dependencies and generating coherent responses across entire conversations.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. How Does Self-Attention in Transformers Improve Response Generation?\n",
    "\n",
    "- **Attention to All Words**:\n",
    "  - Self-attention allows the model to consider all words in the input sequence simultaneously, rather than just processing one word at a time as in RNNs.\n",
    "  \n",
    "- **Better Contextual Understanding**:\n",
    "  - By focusing on different words at each step of response generation, transformers generate more contextually relevant and coherent responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. What Challenges Do Chatbots Face with Out-of-Vocabulary (OOV) Words?\n",
    "\n",
    "- **Limited Vocabulary**:\n",
    "  - Chatbots trained on a specific dataset may encounter words they haven’t seen before (OOV words), leading to poor responses or errors.\n",
    "  \n",
    "- **Solutions**:\n",
    "  - Pre-trained models with vast vocabularies (e.g., GPT-3) or subword tokenization (e.g., Byte Pair Encoding) can mitigate the OOV problem by breaking words into smaller, known units.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. What Is the Importance of User Feedback in Chatbot Improvement?\n",
    "\n",
    "- **Interactive Learning**:\n",
    "  - Chatbots can improve over time by incorporating user feedback, identifying common errors, and learning from user corrections.\n",
    "  \n",
    "- **Personalization**:\n",
    "  - User feedback allows chatbots to personalize responses, making them more relevant to individual users’ preferences and conversational styles.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. How Do Multimodal Chatbots Enhance Conversational Interactions?\n",
    "\n",
    "- **Integration of Text, Image, and Voice**:\n",
    "  - Multimodal chatbots handle input from multiple sources (e.g., text, images, voice), creating richer and more interactive conversations.\n",
    "  \n",
    "- **Applications**:\n",
    "  - These chatbots are especially useful in scenarios like customer service (e.g., uploading images of products) or healthcare (e.g., voice interactions for patient support).\n"
   ],
   "metadata": {
    "id": "iNamu-tmqu-t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple RNN Implementation and some obervations"
   ],
   "metadata": {
    "id": "xNaU8IKaqu8A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIl_xmGBYWwe",
    "outputId": "6f0f83ec-ede2-491a-99ab-dcc29cfcc888"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: tensor([0.8439, 0.5961, 0.6774])\n",
      "Target: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer: transforms input of size `input_size` to `hidden_size`\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # ReLU activation function: introduces non-linearity to the model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Second fully connected layer: transforms hidden layer of size `hidden_size` to `output_size`\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Define the forward pass for the neural network\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Apply ReLU activation to the output of the first layer\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass the activated output through the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Return the output of the network\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "# - `input_size`: the number of input features (3 in this case)\n",
    "# - `hidden_size`: the number of neurons in the hidden layer (5 in this case)\n",
    "# - `output_size`: the number of output features (2 in this case)\n",
    "model = SimpleNN(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# Define the loss function\n",
    "# - We're using Mean Squared Error (MSE) loss, which is appropriate for regression tasks.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "# - We're using the Adam optimizer, which adjusts learning rates adaptively.\n",
    "# - `model.parameters()` refers to the model's learnable parameters (weights and biases).\n",
    "# - `lr=0.01` is the learning rate, determining the step size for updating weights.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create dummy input and target data for this training example\n",
    "# - `inputs`: a random 1D tensor of size 3 (representing input features)\n",
    "# - `targets`: a 1D tensor representing the expected output (2 values here)\n",
    "inputs = torch.randn(3)  # Random input tensor (e.g., from a data sample)\n",
    "targets = torch.tensor([0.0, 1.0])  # Target values (ground truth)\n",
    "\n",
    "# Print the initial input and target for reference\n",
    "print(f\"Input: {inputs}\")\n",
    "print(f\"Target: {targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-lOUndbCzpqd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "997fb167-395a-454e-f2be-fd482ccf1084"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss before backpropagation: 0.07941519469022751\n",
      "Loss after backpropagation: 0.07941519469022751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop (perform one optimization step)\n",
    "\n",
    "# Step 1: Zero the parameter gradients\n",
    "# - Gradients accumulate by default in PyTorch, so we need to reset them before each training step.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Step 2: Forward pass\n",
    "# - Compute the model's predictions for the current inputs by passing them through the network.\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Step 3: Compute the loss\n",
    "# - Compare the model's predictions (`outputs`) to the actual target values (`targets`).\n",
    "# - The `criterion` (MSE loss) measures how far off the predictions are from the targets.\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "# Print the loss value before backpropagation to observe the initial error\n",
    "print(f\"Loss before backpropagation: {loss.item()}\")\n",
    "\n",
    "# Step 4: Backward pass (backpropagation)\n",
    "# - Compute the gradients of the loss with respect to the model's parameters (weights and biases).\n",
    "# - These gradients are used to update the parameters during the optimization step.\n",
    "loss.backward()\n",
    "\n",
    "# Step 5: Optimization step\n",
    "# - Apply the optimizer to adjust the model's parameters based on the computed gradients.\n",
    "# - This step updates the weights and biases to reduce the loss on future forward passes.\n",
    "optimizer.step()\n",
    "\n",
    "# Print the loss value again to show it after backpropagation and parameter updates\n",
    "# The loss won't change immediately after `.step()`, but it would be reduced in future iterations.\n",
    "print(f\"Loss after backpropagation: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ9lSy1aXlXg"
   },
   "source": [
    "**Explanations**\n",
    "\n",
    "1. **Model Definition (`SimpleNN` Class)**:\n",
    "   - The `SimpleNN` class inherits from `nn.Module`, the base class in PyTorch for creating neural networks, allowing PyTorch to manage layers and parameters.\n",
    "   - Within the `__init__` method, layers are defined:\n",
    "     - `self.fc1`: A fully connected layer that maps the input features to the hidden layer.\n",
    "     - `self.relu`: A ReLU activation layer that introduces non-linearity.\n",
    "     - `self.fc2`: Another fully connected layer that connects the hidden layer to the output layer.\n",
    "   - In the `forward` method, data flows through these layers sequentially to produce predictions.\n",
    "\n",
    "2. **Model Initialization**:\n",
    "   - An instance of `SimpleNN` is created, specifying the sizes for input, hidden, and output layers.\n",
    "   - The model’s structure is set up, enabling it to process input data and generate predictions based on initial weights.\n",
    "\n",
    "3. **Loss Function and Optimizer**:\n",
    "   - `criterion` defines how to measure model error, using Mean Squared Error (MSE) here, suitable for regression tasks.\n",
    "   - `optimizer`, set as Adam, manages parameter updates. It accesses model parameters (`model.parameters()`) to adjust them during training based on computed gradients.\n",
    "\n",
    "4. **Training Loop**:\n",
    "   - **Zero Gradients**: Before each training iteration, `optimizer.zero_grad()` resets the gradients of all parameters to zero to prevent accumulation from previous steps.\n",
    "   - **Forward Pass**: The input data `inputs` is passed through the model (`model(inputs)`). This calls the `forward` method, where data flows through `fc1`, `relu`, and `fc2` layers to produce an output (`outputs`), which represents the model’s current predictions.\n",
    "   - **Loss Calculation**: The predictions (`outputs`) are compared to the actual `targets` using the loss function `criterion`. This calculation yields a single `loss` value indicating the difference between predictions and the target values.\n",
    "   - **Backward Pass**: Calling `loss.backward()` computes gradients of the loss with respect to each parameter in the model. These gradients are stored in each parameter’s `.grad` attribute, preparing them for updates.\n",
    "   - **Parameter Update**: Finally, `optimizer.step()` adjusts the model’s parameters by applying the gradients stored in each `.grad` attribute. This step fine-tunes the weights to reduce the loss in subsequent iterations.\n",
    "\n",
    "5. **Summary**:\n",
    "   - The `loss.backward()` and `optimizer.step()` functions, while not directly connected, work sequentially to compute and apply gradients to model parameters. Over multiple iterations, this loop (zeroing gradients, forward pass, loss calculation, backward pass, and optimization step) gradually reduces the model’s loss, improving its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding Multiple Epoches"
   ],
   "metadata": {
    "id": "UEJbMTznM6Ti"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the number of training epochs (iterations over the data)\n",
    "num_epochs = 10  # You can increase this number for more extensive training\n",
    "\n",
    "# Training loop (multiple optimization steps)\n",
    "for epoch in range(num_epochs):\n",
    "    # Step 1: Zero the parameter gradients\n",
    "    # - Clears old gradients, preventing accumulation across epochs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    # - Compute the model's predictions for the current inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Step 3: Compute the loss\n",
    "    # - Calculate the loss between predictions and targets\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Print the loss value at the start of each epoch to observe changes\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss before backpropagation: {loss.item()}\")\n",
    "\n",
    "    # Step 4: Backward pass (backpropagation)\n",
    "    # - Computes gradients for all parameters with respect to the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 5: Optimization step\n",
    "    # - Updates the model parameters using the gradients calculated in the backward pass\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss after optimization step for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss after backpropagation: {loss.item()}\\n\")\n",
    "\n",
    "# Note:\n",
    "# - With more epochs, we observe the loss gradually decreasing as the model learns.\n",
    "# - These print statements at each epoch allow us to track the model's learning process.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2da75w7MgnT",
    "outputId": "c764982e-6db8-4ae3-e916-417b4015c2e8"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10 - Loss before backpropagation: 0.060130923986434937\n",
      "Epoch 1/10 - Loss after backpropagation: 0.060130923986434937\n",
      "\n",
      "Epoch 2/10 - Loss before backpropagation: 0.04355727881193161\n",
      "Epoch 2/10 - Loss after backpropagation: 0.04355727881193161\n",
      "\n",
      "Epoch 3/10 - Loss before backpropagation: 0.029477205127477646\n",
      "Epoch 3/10 - Loss after backpropagation: 0.029477205127477646\n",
      "\n",
      "Epoch 4/10 - Loss before backpropagation: 0.018021047115325928\n",
      "Epoch 4/10 - Loss after backpropagation: 0.018021047115325928\n",
      "\n",
      "Epoch 5/10 - Loss before backpropagation: 0.009458276443183422\n",
      "Epoch 5/10 - Loss after backpropagation: 0.009458276443183422\n",
      "\n",
      "Epoch 6/10 - Loss before backpropagation: 0.003955500666052103\n",
      "Epoch 6/10 - Loss after backpropagation: 0.003955500666052103\n",
      "\n",
      "Epoch 7/10 - Loss before backpropagation: 0.0014469847083091736\n",
      "Epoch 7/10 - Loss after backpropagation: 0.0014469847083091736\n",
      "\n",
      "Epoch 8/10 - Loss before backpropagation: 0.0015240202192217112\n",
      "Epoch 8/10 - Loss after backpropagation: 0.0015240202192217112\n",
      "\n",
      "Epoch 9/10 - Loss before backpropagation: 0.003405212890356779\n",
      "Epoch 9/10 - Loss after backpropagation: 0.003405212890356779\n",
      "\n",
      "Epoch 10/10 - Loss before backpropagation: 0.006068442016839981\n",
      "Epoch 10/10 - Loss after backpropagation: 0.006068442016839981\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation: No change in the loss\n",
    "\n",
    "Reasons:\n",
    "  - learning rate is not defined\n",
    "  - initial weights and bias\n",
    "  -"
   ],
   "metadata": {
    "id": "9esE9Le4M-aE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding Learning Rate parameter"
   ],
   "metadata": {
    "id": "fo0D3ePaNM-q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the number of training epochs (iterations over the data)\n",
    "num_epochs = 10  # You can increase this number for more extensive training\n",
    "\n",
    "# Set the learning rate value\n",
    "learning_rate = 0.01  # You can modify this value to tune the model's performance\n",
    "\n",
    "# Define the optimizer with the specified learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (multiple optimization steps)\n",
    "for epoch in range(num_epochs):\n",
    "    # Step 1: Zero the parameter gradients\n",
    "    # - Clears old gradients, preventing accumulation across epochs\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    # - Compute the model's predictions for the current inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Step 3: Compute the loss\n",
    "    # - Calculate the loss between predictions and targets\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Print the loss value at the start of each epoch to observe changes\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss before backpropagation: {loss.item()}\")\n",
    "\n",
    "    # Step 4: Backward pass (backpropagation)\n",
    "    # - Computes gradients for all parameters with respect to the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 5: Optimization step\n",
    "    # - Updates the model parameters using the gradients calculated in the backward pass\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss after optimization step for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss after backpropagation: {loss.item()}\\n\")\n",
    "\n",
    "# Note:\n",
    "# - You can adjust the `learning_rate` value to observe its impact on the model's training.\n",
    "# - Try experimenting with different learning rates (e.g., 0.001, 0.005, 0.1) to see which one works best for your model.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PojLkAxXNTFs",
    "outputId": "9657f115-c654-4804-85dd-a10c3b569fbb"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10 - Loss before backpropagation: 0.00851397030055523\n",
      "Epoch 1/10 - Loss after backpropagation: 0.00851397030055523\n",
      "\n",
      "Epoch 2/10 - Loss before backpropagation: 0.0019002421759068966\n",
      "Epoch 2/10 - Loss after backpropagation: 0.0019002421759068966\n",
      "\n",
      "Epoch 3/10 - Loss before backpropagation: 0.0001043413212755695\n",
      "Epoch 3/10 - Loss after backpropagation: 0.0001043413212755695\n",
      "\n",
      "Epoch 4/10 - Loss before backpropagation: 0.0011314342264086008\n",
      "Epoch 4/10 - Loss after backpropagation: 0.0011314342264086008\n",
      "\n",
      "Epoch 5/10 - Loss before backpropagation: 0.0024990045931190252\n",
      "Epoch 5/10 - Loss after backpropagation: 0.0024990045931190252\n",
      "\n",
      "Epoch 6/10 - Loss before backpropagation: 0.002863857429474592\n",
      "Epoch 6/10 - Loss after backpropagation: 0.002863857429474592\n",
      "\n",
      "Epoch 7/10 - Loss before backpropagation: 0.002259619068354368\n",
      "Epoch 7/10 - Loss after backpropagation: 0.002259619068354368\n",
      "\n",
      "Epoch 8/10 - Loss before backpropagation: 0.001258397358469665\n",
      "Epoch 8/10 - Loss after backpropagation: 0.001258397358469665\n",
      "\n",
      "Epoch 9/10 - Loss before backpropagation: 0.000421887292759493\n",
      "Epoch 9/10 - Loss after backpropagation: 0.000421887292759493\n",
      "\n",
      "Epoch 10/10 - Loss before backpropagation: 8.061066910158843e-05\n",
      "Epoch 10/10 - Loss after backpropagation: 8.061066910158843e-05\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation: No change in the loss\n",
    "\n",
    "Improvments can be done:\n",
    "\n",
    "1. Increase Number of Epochs:\n",
    "- Sometimes the model needs more iterations to converge, especially if the dataset is small or if the model needs more time to learn.\n",
    "\n",
    "2. Use a Different Learning Rate:\n",
    "- The learning rate might be too small, resulting in slow convergence. Try increasing the learning rate a bit to make the optimization process more effective.\n",
    "3. Track Gradient Values:\n",
    "- It's useful to check if the gradients are too small (vanishing gradients) or too large (exploding gradients). If gradients are too small, the learning rate will have little impact.\n",
    "4. Mini-batch Gradient Descent:\n",
    "- Instead of training on a single sample, it's better to train on mini-batches of data. This will make the optimization process smoother and improve the convergence speed."
   ],
   "metadata": {
    "id": "yyVHqBmmNYkx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler  # For learning rate scheduling\n",
    "\n",
    "# Set the number of training epochs (increase for more training)\n",
    "num_epochs = 100  # Increased epochs for more extensive training\n",
    "\n",
    "# Set the learning rate value (increase slightly if necessary)\n",
    "learning_rate = 0.05  # Slightly higher learning rate for faster convergence\n",
    "\n",
    "# Define the optimizer with the adjusted learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, define a learning rate scheduler to reduce the learning rate over time\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Decrease lr every 30 epochs\n",
    "\n",
    "# Training loop with more extensive monitoring and improved learning rate handling\n",
    "for epoch in range(num_epochs):\n",
    "    # Step 1: Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Step 3: Compute the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Print the loss value at the start of each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss before backpropagation: {loss.item()}\")\n",
    "\n",
    "    # Step 4: Backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradient norms to monitor if they are vanishing/exploding\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)  # L2 norm of gradients\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Gradient norm: {total_norm}\")\n",
    "\n",
    "    # Step 5: Optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update learning rate based on the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the loss after optimization step for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss after backpropagation: {loss.item()}\\n\")\n",
    "\n",
    "# Note:\n",
    "# 1. Increased number of epochs for more extensive training.\n",
    "# 2. Learning rate scheduler decreases the learning rate every 30 epochs to fine-tune learning.\n",
    "# 3. Monitoring gradient norms helps track the training process (especially for vanishing/exploding gradient problems).\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OO7fYwqsNnK4",
    "outputId": "6d2a7f11-3938-4cac-f729-7f481f39fc5c"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100 - Loss before backpropagation: 0.00025057472521439195\n",
      "Epoch 1/100 - Gradient norm: 0.03897683940397265\n",
      "Epoch 1/100 - Loss after backpropagation: 0.00025057472521439195\n",
      "\n",
      "Epoch 2/100 - Loss before backpropagation: 0.04002182558178902\n",
      "Epoch 2/100 - Gradient norm: 0.4422040681548823\n",
      "Epoch 2/100 - Loss after backpropagation: 0.04002182558178902\n",
      "\n",
      "Epoch 3/100 - Loss before backpropagation: 0.00637606717646122\n",
      "Epoch 3/100 - Gradient norm: 0.1775083721031119\n",
      "Epoch 3/100 - Loss after backpropagation: 0.00637606717646122\n",
      "\n",
      "Epoch 4/100 - Loss before backpropagation: 0.009266458451747894\n",
      "Epoch 4/100 - Gradient norm: 0.24360702009795468\n",
      "Epoch 4/100 - Loss after backpropagation: 0.009266458451747894\n",
      "\n",
      "Epoch 5/100 - Loss before backpropagation: 0.021586067974567413\n",
      "Epoch 5/100 - Gradient norm: 0.3914085482986961\n",
      "Epoch 5/100 - Loss after backpropagation: 0.021586067974567413\n",
      "\n",
      "Epoch 6/100 - Loss before backpropagation: 0.011280744336545467\n",
      "Epoch 6/100 - Gradient norm: 0.2753226262440575\n",
      "Epoch 6/100 - Loss after backpropagation: 0.011280744336545467\n",
      "\n",
      "Epoch 7/100 - Loss before backpropagation: 0.0012086386559531093\n",
      "Epoch 7/100 - Gradient norm: 0.08048625410995146\n",
      "Epoch 7/100 - Loss after backpropagation: 0.0012086386559531093\n",
      "\n",
      "Epoch 8/100 - Loss before backpropagation: 0.0026310731191188097\n",
      "Epoch 8/100 - Gradient norm: 0.11841740066919088\n",
      "Epoch 8/100 - Loss after backpropagation: 0.0026310731191188097\n",
      "\n",
      "Epoch 9/100 - Loss before backpropagation: 0.008425096981227398\n",
      "Epoch 9/100 - Gradient norm: 0.21080505777202366\n",
      "Epoch 9/100 - Loss after backpropagation: 0.008425096981227398\n",
      "\n",
      "Epoch 10/100 - Loss before backpropagation: 0.010056830942630768\n",
      "Epoch 10/100 - Gradient norm: 0.22870783407428463\n",
      "Epoch 10/100 - Loss after backpropagation: 0.010056830942630768\n",
      "\n",
      "Epoch 11/100 - Loss before backpropagation: 0.006582481320947409\n",
      "Epoch 11/100 - Gradient norm: 0.1867470227896093\n",
      "Epoch 11/100 - Loss after backpropagation: 0.006582481320947409\n",
      "\n",
      "Epoch 12/100 - Loss before backpropagation: 0.0018792725168168545\n",
      "Epoch 12/100 - Gradient norm: 0.1017435895936168\n",
      "Epoch 12/100 - Loss after backpropagation: 0.0018792725168168545\n",
      "\n",
      "Epoch 13/100 - Loss before backpropagation: 6.257748464122415e-05\n",
      "Epoch 13/100 - Gradient norm: 0.015846476416791915\n",
      "Epoch 13/100 - Loss after backpropagation: 6.257748464122415e-05\n",
      "\n",
      "Epoch 14/100 - Loss before backpropagation: 0.0020900426898151636\n",
      "Epoch 14/100 - Gradient norm: 0.11147916299559309\n",
      "Epoch 14/100 - Loss after backpropagation: 0.0020900426898151636\n",
      "\n",
      "Epoch 15/100 - Loss before backpropagation: 0.004862955771386623\n",
      "Epoch 15/100 - Gradient norm: 0.17289961515850288\n",
      "Epoch 15/100 - Loss after backpropagation: 0.004862955771386623\n",
      "\n",
      "Epoch 16/100 - Loss before backpropagation: 0.004949030466377735\n",
      "Epoch 16/100 - Gradient norm: 0.1745176378467768\n",
      "Epoch 16/100 - Loss after backpropagation: 0.004949030466377735\n",
      "\n",
      "Epoch 17/100 - Loss before backpropagation: 0.0025587989948689938\n",
      "Epoch 17/100 - Gradient norm: 0.12410127282541346\n",
      "Epoch 17/100 - Loss after backpropagation: 0.0025587989948689938\n",
      "\n",
      "Epoch 18/100 - Loss before backpropagation: 0.00038893759483471513\n",
      "Epoch 18/100 - Gradient norm: 0.04729694765031852\n",
      "Epoch 18/100 - Loss after backpropagation: 0.00038893759483471513\n",
      "\n",
      "Epoch 19/100 - Loss before backpropagation: 0.0002274872676935047\n",
      "Epoch 19/100 - Gradient norm: 0.033849158227738395\n",
      "Epoch 19/100 - Loss after backpropagation: 0.0002274872676935047\n",
      "\n",
      "Epoch 20/100 - Loss before backpropagation: 0.0015647936379536986\n",
      "Epoch 20/100 - Gradient norm: 0.090517425718966\n",
      "Epoch 20/100 - Loss after backpropagation: 0.0015647936379536986\n",
      "\n",
      "Epoch 21/100 - Loss before backpropagation: 0.002712995745241642\n",
      "Epoch 21/100 - Gradient norm: 0.11866183438681806\n",
      "Epoch 21/100 - Loss after backpropagation: 0.002712995745241642\n",
      "\n",
      "Epoch 22/100 - Loss before backpropagation: 0.0025749062187969685\n",
      "Epoch 22/100 - Gradient norm: 0.11576110357687636\n",
      "Epoch 22/100 - Loss after backpropagation: 0.0025749062187969685\n",
      "\n",
      "Epoch 23/100 - Loss before backpropagation: 0.0013872418785467744\n",
      "Epoch 23/100 - Gradient norm: 0.08568783693513102\n",
      "Epoch 23/100 - Loss after backpropagation: 0.0013872418785467744\n",
      "\n",
      "Epoch 24/100 - Loss before backpropagation: 0.000246427342062816\n",
      "Epoch 24/100 - Gradient norm: 0.03664860029062598\n",
      "Epoch 24/100 - Loss after backpropagation: 0.000246427342062816\n",
      "\n",
      "Epoch 25/100 - Loss before backpropagation: 7.288888446055353e-05\n",
      "Epoch 25/100 - Gradient norm: 0.019696170576863684\n",
      "Epoch 25/100 - Loss after backpropagation: 7.288888446055353e-05\n",
      "\n",
      "Epoch 26/100 - Loss before backpropagation: 0.0007980432128533721\n",
      "Epoch 26/100 - Gradient norm: 0.0668972951434454\n",
      "Epoch 26/100 - Loss after backpropagation: 0.0007980432128533721\n",
      "\n",
      "Epoch 27/100 - Loss before backpropagation: 0.0014887013239786029\n",
      "Epoch 27/100 - Gradient norm: 0.0919651658042253\n",
      "Epoch 27/100 - Loss after backpropagation: 0.0014887013239786029\n",
      "\n",
      "Epoch 28/100 - Loss before backpropagation: 0.0013802223838865757\n",
      "Epoch 28/100 - Gradient norm: 0.08840160250432327\n",
      "Epoch 28/100 - Loss after backpropagation: 0.0013802223838865757\n",
      "\n",
      "Epoch 29/100 - Loss before backpropagation: 0.0006502640899270773\n",
      "Epoch 29/100 - Gradient norm: 0.060137090516535686\n",
      "Epoch 29/100 - Loss after backpropagation: 0.0006502640899270773\n",
      "\n",
      "Epoch 30/100 - Loss before backpropagation: 6.56768461340107e-05\n",
      "Epoch 30/100 - Gradient norm: 0.018684361241273227\n",
      "Epoch 30/100 - Loss after backpropagation: 6.56768461340107e-05\n",
      "\n",
      "Epoch 31/100 - Loss before backpropagation: 9.820297418627888e-05\n",
      "Epoch 31/100 - Gradient norm: 0.02306640856816365\n",
      "Epoch 31/100 - Loss after backpropagation: 9.820297418627888e-05\n",
      "\n",
      "Epoch 32/100 - Loss before backpropagation: 0.0001262938603758812\n",
      "Epoch 32/100 - Gradient norm: 0.026122907755501874\n",
      "Epoch 32/100 - Loss after backpropagation: 0.0001262938603758812\n",
      "\n",
      "Epoch 33/100 - Loss before backpropagation: 0.00014744226064067334\n",
      "Epoch 33/100 - Gradient norm: 0.028196312101873486\n",
      "Epoch 33/100 - Loss after backpropagation: 0.00014744226064067334\n",
      "\n",
      "Epoch 34/100 - Loss before backpropagation: 0.00015958337462507188\n",
      "Epoch 34/100 - Gradient norm: 0.029312559103936726\n",
      "Epoch 34/100 - Loss after backpropagation: 0.00015958337462507188\n",
      "\n",
      "Epoch 35/100 - Loss before backpropagation: 0.0001620025432202965\n",
      "Epoch 35/100 - Gradient norm: 0.02951973360934416\n",
      "Epoch 35/100 - Loss after backpropagation: 0.0001620025432202965\n",
      "\n",
      "Epoch 36/100 - Loss before backpropagation: 0.0001551907043904066\n",
      "Epoch 36/100 - Gradient norm: 0.028884979500518805\n",
      "Epoch 36/100 - Loss after backpropagation: 0.0001551907043904066\n",
      "\n",
      "Epoch 37/100 - Loss before backpropagation: 0.0001405990042258054\n",
      "Epoch 37/100 - Gradient norm: 0.02749121319584806\n",
      "Epoch 37/100 - Loss after backpropagation: 0.0001405990042258054\n",
      "\n",
      "Epoch 38/100 - Loss before backpropagation: 0.00012034942483296618\n",
      "Epoch 38/100 - Gradient norm: 0.025435485799170626\n",
      "Epoch 38/100 - Loss after backpropagation: 0.00012034942483296618\n",
      "\n",
      "Epoch 39/100 - Loss before backpropagation: 9.689976286608726e-05\n",
      "Epoch 39/100 - Gradient norm: 0.022825155807386362\n",
      "Epoch 39/100 - Loss after backpropagation: 9.689976286608726e-05\n",
      "\n",
      "Epoch 40/100 - Loss before backpropagation: 7.273507071658969e-05\n",
      "Epoch 40/100 - Gradient norm: 0.01977564721747696\n",
      "Epoch 40/100 - Loss after backpropagation: 7.273507071658969e-05\n",
      "\n",
      "Epoch 41/100 - Loss before backpropagation: 5.009374945075251e-05\n",
      "Epoch 41/100 - Gradient norm: 0.01640775970771005\n",
      "Epoch 41/100 - Loss after backpropagation: 5.009374945075251e-05\n",
      "\n",
      "Epoch 42/100 - Loss before backpropagation: 3.075104541494511e-05\n",
      "Epoch 42/100 - Gradient norm: 0.012844354264510917\n",
      "Epoch 42/100 - Loss after backpropagation: 3.075104541494511e-05\n",
      "\n",
      "Epoch 43/100 - Loss before backpropagation: 1.589059320394881e-05\n",
      "Epoch 43/100 - Gradient norm: 0.00920991434375711\n",
      "Epoch 43/100 - Loss after backpropagation: 1.589059320394881e-05\n",
      "\n",
      "Epoch 44/100 - Loss before backpropagation: 6.047675014997367e-06\n",
      "Epoch 44/100 - Gradient norm: 0.005634528690456927\n",
      "Epoch 44/100 - Loss after backpropagation: 6.047675014997367e-06\n",
      "\n",
      "Epoch 45/100 - Loss before backpropagation: 1.1372338803994353e-06\n",
      "Epoch 45/100 - Gradient norm: 0.002324260796048816\n",
      "Epoch 45/100 - Loss after backpropagation: 1.1372338803994353e-06\n",
      "\n",
      "Epoch 46/100 - Loss before backpropagation: 5.480725349116256e-07\n",
      "Epoch 46/100 - Gradient norm: 0.0015992258265429223\n",
      "Epoch 46/100 - Loss after backpropagation: 5.480725349116256e-07\n",
      "\n",
      "Epoch 47/100 - Loss before backpropagation: 3.28579926645034e-06\n",
      "Epoch 47/100 - Gradient norm: 0.0042261560004016666\n",
      "Epoch 47/100 - Loss after backpropagation: 3.28579926645034e-06\n",
      "\n",
      "Epoch 48/100 - Loss before backpropagation: 8.14236955193337e-06\n",
      "Epoch 48/100 - Gradient norm: 0.006689091749019343\n",
      "Epoch 48/100 - Loss after backpropagation: 8.14236955193337e-06\n",
      "\n",
      "Epoch 49/100 - Loss before backpropagation: 1.3872137060388923e-05\n",
      "Epoch 49/100 - Gradient norm: 0.00874030818264554\n",
      "Epoch 49/100 - Loss after backpropagation: 1.3872137060388923e-05\n",
      "\n",
      "Epoch 50/100 - Loss before backpropagation: 1.9345225155120715e-05\n",
      "Epoch 50/100 - Gradient norm: 0.010324597342483918\n",
      "Epoch 50/100 - Loss after backpropagation: 1.9345225155120715e-05\n",
      "\n",
      "Epoch 51/100 - Loss before backpropagation: 2.366978515055962e-05\n",
      "Epoch 51/100 - Gradient norm: 0.01142103018712663\n",
      "Epoch 51/100 - Loss after backpropagation: 2.366978515055962e-05\n",
      "\n",
      "Epoch 52/100 - Loss before backpropagation: 2.6264406187692657e-05\n",
      "Epoch 52/100 - Gradient norm: 0.012029596890895168\n",
      "Epoch 52/100 - Loss after backpropagation: 2.6264406187692657e-05\n",
      "\n",
      "Epoch 53/100 - Loss before backpropagation: 2.688869062694721e-05\n",
      "Epoch 53/100 - Gradient norm: 0.01216915837623578\n",
      "Epoch 53/100 - Loss after backpropagation: 2.688869062694721e-05\n",
      "\n",
      "Epoch 54/100 - Loss before backpropagation: 2.5609910153434612e-05\n",
      "Epoch 54/100 - Gradient norm: 0.011872442321023335\n",
      "Epoch 54/100 - Loss after backpropagation: 2.5609910153434612e-05\n",
      "\n",
      "Epoch 55/100 - Loss before backpropagation: 2.2755759346182458e-05\n",
      "Epoch 55/100 - Gradient norm: 0.011186515332923742\n",
      "Epoch 55/100 - Loss after backpropagation: 2.2755759346182458e-05\n",
      "\n",
      "Epoch 56/100 - Loss before backpropagation: 1.8821523553924635e-05\n",
      "Epoch 56/100 - Gradient norm: 0.010168108941919031\n",
      "Epoch 56/100 - Loss after backpropagation: 1.8821523553924635e-05\n",
      "\n",
      "Epoch 57/100 - Loss before backpropagation: 1.4378444575413596e-05\n",
      "Epoch 57/100 - Gradient norm: 0.008881271372905074\n",
      "Epoch 57/100 - Loss after backpropagation: 1.4378444575413596e-05\n",
      "\n",
      "Epoch 58/100 - Loss before backpropagation: 9.985928045352921e-06\n",
      "Epoch 58/100 - Gradient norm: 0.007395070020191326\n",
      "Epoch 58/100 - Loss after backpropagation: 9.985928045352921e-06\n",
      "\n",
      "Epoch 59/100 - Loss before backpropagation: 6.11520681559341e-06\n",
      "Epoch 59/100 - Gradient norm: 0.005780264726876303\n",
      "Epoch 59/100 - Loss after backpropagation: 6.11520681559341e-06\n",
      "\n",
      "Epoch 60/100 - Loss before backpropagation: 3.098865590800415e-06\n",
      "Epoch 60/100 - Gradient norm: 0.0041069385773904385\n",
      "Epoch 60/100 - Loss after backpropagation: 3.098865590800415e-06\n",
      "\n",
      "Epoch 61/100 - Loss before backpropagation: 1.107749312723172e-06\n",
      "Epoch 61/100 - Gradient norm: 0.002444227283668804\n",
      "Epoch 61/100 - Loss after backpropagation: 1.107749312723172e-06\n",
      "\n",
      "Epoch 62/100 - Loss before backpropagation: 9.69780444393109e-07\n",
      "Epoch 62/100 - Gradient norm: 0.002285001512323038\n",
      "Epoch 62/100 - Loss after backpropagation: 9.69780444393109e-07\n",
      "\n",
      "Epoch 63/100 - Loss before backpropagation: 8.460444860247662e-07\n",
      "Epoch 63/100 - Gradient norm: 0.0021322628165815383\n",
      "Epoch 63/100 - Loss after backpropagation: 8.460444860247662e-07\n",
      "\n",
      "Epoch 64/100 - Loss before backpropagation: 7.353642104135361e-07\n",
      "Epoch 64/100 - Gradient norm: 0.001985840908424826\n",
      "Epoch 64/100 - Loss after backpropagation: 7.353642104135361e-07\n",
      "\n",
      "Epoch 65/100 - Loss before backpropagation: 6.36624974958977e-07\n",
      "Epoch 65/100 - Gradient norm: 0.0018455893554273739\n",
      "Epoch 65/100 - Loss after backpropagation: 6.36624974958977e-07\n",
      "\n",
      "Epoch 66/100 - Loss before backpropagation: 5.489320642482198e-07\n",
      "Epoch 66/100 - Gradient norm: 0.001711575336037144\n",
      "Epoch 66/100 - Loss after backpropagation: 5.489320642482198e-07\n",
      "\n",
      "Epoch 67/100 - Loss before backpropagation: 4.713725161309412e-07\n",
      "Epoch 67/100 - Gradient norm: 0.0015837931149934627\n",
      "Epoch 67/100 - Loss after backpropagation: 4.713725161309412e-07\n",
      "\n",
      "Epoch 68/100 - Loss before backpropagation: 4.0297527448274195e-07\n",
      "Epoch 68/100 - Gradient norm: 0.0014620182784762553\n",
      "Epoch 68/100 - Loss after backpropagation: 4.0297527448274195e-07\n",
      "\n",
      "Epoch 69/100 - Loss before backpropagation: 3.428826289564313e-07\n",
      "Epoch 69/100 - Gradient norm: 0.0013461422114285197\n",
      "Epoch 69/100 - Loss after backpropagation: 3.428826289564313e-07\n",
      "\n",
      "Epoch 70/100 - Loss before backpropagation: 2.904067173403746e-07\n",
      "Epoch 70/100 - Gradient norm: 0.0012362780346995444\n",
      "Epoch 70/100 - Loss after backpropagation: 2.904067173403746e-07\n",
      "\n",
      "Epoch 71/100 - Loss before backpropagation: 2.4466899617436866e-07\n",
      "Epoch 71/100 - Gradient norm: 0.0011320338506328428\n",
      "Epoch 71/100 - Loss after backpropagation: 2.4466899617436866e-07\n",
      "\n",
      "Epoch 72/100 - Loss before backpropagation: 2.050175567092083e-07\n",
      "Epoch 72/100 - Gradient norm: 0.0010333833341404724\n",
      "Epoch 72/100 - Loss after backpropagation: 2.050175567092083e-07\n",
      "\n",
      "Epoch 73/100 - Loss before backpropagation: 1.7088406423226843e-07\n",
      "Epoch 73/100 - Gradient norm: 0.0009404154650101303\n",
      "Epoch 73/100 - Loss after backpropagation: 1.7088406423226843e-07\n",
      "\n",
      "Epoch 74/100 - Loss before backpropagation: 1.4156657357489166e-07\n",
      "Epoch 74/100 - Gradient norm: 0.0008527249487933111\n",
      "Epoch 74/100 - Loss after backpropagation: 1.4156657357489166e-07\n",
      "\n",
      "Epoch 75/100 - Loss before backpropagation: 1.1661926180295268e-07\n",
      "Epoch 75/100 - Gradient norm: 0.0007705016182145991\n",
      "Epoch 75/100 - Loss after backpropagation: 1.1661926180295268e-07\n",
      "\n",
      "Epoch 76/100 - Loss before backpropagation: 9.547616741656384e-08\n",
      "Epoch 76/100 - Gradient norm: 0.0006935034349648745\n",
      "Epoch 76/100 - Loss after backpropagation: 9.547616741656384e-08\n",
      "\n",
      "Epoch 77/100 - Loss before backpropagation: 7.7720798685732e-08\n",
      "Epoch 77/100 - Gradient norm: 0.0006217900281801761\n",
      "Epoch 77/100 - Loss after backpropagation: 7.7720798685732e-08\n",
      "\n",
      "Epoch 78/100 - Loss before backpropagation: 6.288613008109678e-08\n",
      "Epoch 78/100 - Gradient norm: 0.0005551230506437261\n",
      "Epoch 78/100 - Loss after backpropagation: 6.288613008109678e-08\n",
      "\n",
      "Epoch 79/100 - Loss before backpropagation: 5.059849783606296e-08\n",
      "Epoch 79/100 - Gradient norm: 0.0004934582107246765\n",
      "Epoch 79/100 - Loss after backpropagation: 5.059849783606296e-08\n",
      "\n",
      "Epoch 80/100 - Loss before backpropagation: 4.054654567653415e-08\n",
      "Epoch 80/100 - Gradient norm: 0.00043695680355973455\n",
      "Epoch 80/100 - Loss after backpropagation: 4.054654567653415e-08\n",
      "\n",
      "Epoch 81/100 - Loss before backpropagation: 3.239496493279148e-08\n",
      "Epoch 81/100 - Gradient norm: 0.0003855234421841474\n",
      "Epoch 81/100 - Loss after backpropagation: 3.239496493279148e-08\n",
      "\n",
      "Epoch 82/100 - Loss before backpropagation: 2.5865476871445026e-08\n",
      "Epoch 82/100 - Gradient norm: 0.0003392121555061154\n",
      "Epoch 82/100 - Loss after backpropagation: 2.5865476871445026e-08\n",
      "\n",
      "Epoch 83/100 - Loss before backpropagation: 2.0719406634839288e-08\n",
      "Epoch 83/100 - Gradient norm: 0.00029823855429949155\n",
      "Epoch 83/100 - Loss after backpropagation: 2.0719406634839288e-08\n",
      "\n",
      "Epoch 84/100 - Loss before backpropagation: 1.6748586517678632e-08\n",
      "Epoch 84/100 - Gradient norm: 0.0002629259192877657\n",
      "Epoch 84/100 - Loss after backpropagation: 1.6748586517678632e-08\n",
      "\n",
      "Epoch 85/100 - Loss before backpropagation: 1.3734236858908844e-08\n",
      "Epoch 85/100 - Gradient norm: 0.0002332888554542779\n",
      "Epoch 85/100 - Loss after backpropagation: 1.3734236858908844e-08\n",
      "\n",
      "Epoch 86/100 - Loss before backpropagation: 1.1512909736666188e-08\n",
      "Epoch 86/100 - Gradient norm: 0.0002095983256560062\n",
      "Epoch 86/100 - Loss after backpropagation: 1.1512909736666188e-08\n",
      "\n",
      "Epoch 87/100 - Loss before backpropagation: 9.944223222646542e-09\n",
      "Epoch 87/100 - Gradient norm: 0.0001920857456813419\n",
      "Epoch 87/100 - Loss after backpropagation: 9.944223222646542e-09\n",
      "\n",
      "Epoch 88/100 - Loss before backpropagation: 8.893218605976472e-09\n",
      "Epoch 88/100 - Gradient norm: 0.0001805097268289805\n",
      "Epoch 88/100 - Loss after backpropagation: 8.893218605976472e-09\n",
      "\n",
      "Epoch 89/100 - Loss before backpropagation: 8.244635196774652e-09\n",
      "Epoch 89/100 - Gradient norm: 0.00017429207411348048\n",
      "Epoch 89/100 - Loss after backpropagation: 8.244635196774652e-09\n",
      "\n",
      "Epoch 90/100 - Loss before backpropagation: 7.904798593472151e-09\n",
      "Epoch 90/100 - Gradient norm: 0.00017252480092522527\n",
      "Epoch 90/100 - Loss after backpropagation: 7.904798593472151e-09\n",
      "\n",
      "Epoch 91/100 - Loss before backpropagation: 7.80878117723205e-09\n",
      "Epoch 91/100 - Gradient norm: 0.00017429713966585753\n",
      "Epoch 91/100 - Loss after backpropagation: 7.80878117723205e-09\n",
      "\n",
      "Epoch 92/100 - Loss before backpropagation: 7.808978352841223e-09\n",
      "Epoch 92/100 - Gradient norm: 0.00017459165787399842\n",
      "Epoch 92/100 - Loss after backpropagation: 7.808978352841223e-09\n",
      "\n",
      "Epoch 93/100 - Loss before backpropagation: 7.80211451001378e-09\n",
      "Epoch 93/100 - Gradient norm: 0.00017478803260570456\n",
      "Epoch 93/100 - Loss after backpropagation: 7.80211451001378e-09\n",
      "\n",
      "Epoch 94/100 - Loss before backpropagation: 7.785212474686887e-09\n",
      "Epoch 94/100 - Gradient norm: 0.00017483247985661298\n",
      "Epoch 94/100 - Loss after backpropagation: 7.785212474686887e-09\n",
      "\n",
      "Epoch 95/100 - Loss before backpropagation: 7.769394017032027e-09\n",
      "Epoch 95/100 - Gradient norm: 0.000174892519784964\n",
      "Epoch 95/100 - Loss after backpropagation: 7.769394017032027e-09\n",
      "\n",
      "Epoch 96/100 - Loss before backpropagation: 7.768505838612327e-09\n",
      "Epoch 96/100 - Gradient norm: 0.00017510696038452447\n",
      "Epoch 96/100 - Loss after backpropagation: 7.768505838612327e-09\n",
      "\n",
      "Epoch 97/100 - Loss before backpropagation: 7.750031727482565e-09\n",
      "Epoch 97/100 - Gradient norm: 0.0001750732153183077\n",
      "Epoch 97/100 - Loss after backpropagation: 7.750031727482565e-09\n",
      "\n",
      "Epoch 98/100 - Loss before backpropagation: 7.73212605054141e-09\n",
      "Epoch 98/100 - Gradient norm: 0.00017504739406357014\n",
      "Epoch 98/100 - Loss after backpropagation: 7.73212605054141e-09\n",
      "\n",
      "Epoch 99/100 - Loss before backpropagation: 7.713685690191596e-09\n",
      "Epoch 99/100 - Gradient norm: 0.00017499109453018128\n",
      "Epoch 99/100 - Loss after backpropagation: 7.713685690191596e-09\n",
      "\n",
      "Epoch 100/100 - Loss before backpropagation: 7.689184400305749e-09\n",
      "Epoch 100/100 - Gradient norm: 0.00017485990287204351\n",
      "Epoch 100/100 - Loss after backpropagation: 7.689184400305749e-09\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation: No change in loss\n",
    "\n"
   ],
   "metadata": {
    "id": "OrsMYLFzOHuB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Use new dataset"
   ],
   "metadata": {
    "id": "gbyj38Z4OKsz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of samples in the dataset\n",
    "num_samples = 100  # Adjust this for more data\n",
    "\n",
    "# Number of input features (matching the input size of the model)\n",
    "input_size = 3\n",
    "\n",
    "# Number of output features (matching the output size of the model)\n",
    "output_size = 2\n",
    "\n",
    "# Generating random input data (100 samples, each with 3 input features)\n",
    "inputs = torch.randn(num_samples, input_size)\n",
    "\n",
    "# Generating corresponding random target values (100 samples, each with 2 target values)\n",
    "targets = torch.randn(num_samples, output_size)\n",
    "\n",
    "# Print the shape of inputs and targets to verify\n",
    "print(f\"Inputs shape: {inputs.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "# Now the model will train on a more complex dataset with multiple samples.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdJSSsx-NnDI",
    "outputId": "19f801de-9a1b-4e92-f3dd-4e606a7684fe"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inputs shape: torch.Size([100, 3])\n",
      "Targets shape: torch.Size([100, 2])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler  # For learning rate scheduling\n",
    "\n",
    "# Set the number of training epochs (increase for more training)\n",
    "num_epochs = 100  # Increased epochs for more extensive training\n",
    "\n",
    "# Set the learning rate value (increase slightly if necessary)\n",
    "learning_rate = 0.05  # Slightly higher learning rate for faster convergence\n",
    "\n",
    "# Define the optimizer with the adjusted learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, define a learning rate scheduler to reduce the learning rate over time\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Decrease lr every 30 epochs\n",
    "\n",
    "# Training loop with more extensive monitoring and improved learning rate handling\n",
    "for epoch in range(num_epochs):\n",
    "    # Step 1: Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Step 3: Compute the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Print the loss value at the start of each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss before backpropagation: {loss.item()}\")\n",
    "\n",
    "    # Step 4: Backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # Print gradient norms to monitor if they are vanishing/exploding\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)  # L2 norm of gradients\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Gradient norm: {total_norm}\")\n",
    "\n",
    "    # Step 5: Optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update learning rate based on the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the loss after optimization step for monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss after backpropagation: {loss.item()}\\n\")\n",
    "\n",
    "# Note:\n",
    "# 1. Increased number of epochs for more extensive training.\n",
    "# 2. Learning rate scheduler decreases the learning rate every 30 epochs to fine-tune learning.\n",
    "# 3. Monitoring gradient norms helps track the training process (especially for vanishing/exploding gradient problems).\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHqQgYkoOSvi",
    "outputId": "d5e22e53-6f2a-4b0e-90bb-e43f66780542"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100 - Loss before backpropagation: 1.2838449478149414\n",
      "Epoch 1/100 - Gradient norm: 0.9909928890254608\n",
      "Epoch 1/100 - Loss after backpropagation: 1.2838449478149414\n",
      "\n",
      "Epoch 2/100 - Loss before backpropagation: 1.153826355934143\n",
      "Epoch 2/100 - Gradient norm: 0.7106370261752049\n",
      "Epoch 2/100 - Loss after backpropagation: 1.153826355934143\n",
      "\n",
      "Epoch 3/100 - Loss before backpropagation: 1.0681909322738647\n",
      "Epoch 3/100 - Gradient norm: 0.47431174213313554\n",
      "Epoch 3/100 - Loss after backpropagation: 1.0681909322738647\n",
      "\n",
      "Epoch 4/100 - Loss before backpropagation: 1.0177624225616455\n",
      "Epoch 4/100 - Gradient norm: 0.2883330799112704\n",
      "Epoch 4/100 - Loss after backpropagation: 1.0177624225616455\n",
      "\n",
      "Epoch 5/100 - Loss before backpropagation: 0.996860146522522\n",
      "Epoch 5/100 - Gradient norm: 0.1869122666699125\n",
      "Epoch 5/100 - Loss after backpropagation: 0.996860146522522\n",
      "\n",
      "Epoch 6/100 - Loss before backpropagation: 0.9919053912162781\n",
      "Epoch 6/100 - Gradient norm: 0.21640779435566565\n",
      "Epoch 6/100 - Loss after backpropagation: 0.9919053912162781\n",
      "\n",
      "Epoch 7/100 - Loss before backpropagation: 0.9919313192367554\n",
      "Epoch 7/100 - Gradient norm: 0.2655566480806975\n",
      "Epoch 7/100 - Loss after backpropagation: 0.9919313192367554\n",
      "\n",
      "Epoch 8/100 - Loss before backpropagation: 0.9895733594894409\n",
      "Epoch 8/100 - Gradient norm: 0.27421560191641114\n",
      "Epoch 8/100 - Loss after backpropagation: 0.9895733594894409\n",
      "\n",
      "Epoch 9/100 - Loss before backpropagation: 0.9842265248298645\n",
      "Epoch 9/100 - Gradient norm: 0.2597957313335512\n",
      "Epoch 9/100 - Loss after backpropagation: 0.9842265248298645\n",
      "\n",
      "Epoch 10/100 - Loss before backpropagation: 0.9780991077423096\n",
      "Epoch 10/100 - Gradient norm: 0.22271082391652514\n",
      "Epoch 10/100 - Loss after backpropagation: 0.9780991077423096\n",
      "\n",
      "Epoch 11/100 - Loss before backpropagation: 0.9721137285232544\n",
      "Epoch 11/100 - Gradient norm: 0.18426053890072164\n",
      "Epoch 11/100 - Loss after backpropagation: 0.9721137285232544\n",
      "\n",
      "Epoch 12/100 - Loss before backpropagation: 0.9664726257324219\n",
      "Epoch 12/100 - Gradient norm: 0.15264943866456437\n",
      "Epoch 12/100 - Loss after backpropagation: 0.9664726257324219\n",
      "\n",
      "Epoch 13/100 - Loss before backpropagation: 0.9608107209205627\n",
      "Epoch 13/100 - Gradient norm: 0.11889230403468044\n",
      "Epoch 13/100 - Loss after backpropagation: 0.9608107209205627\n",
      "\n",
      "Epoch 14/100 - Loss before backpropagation: 0.9564979672431946\n",
      "Epoch 14/100 - Gradient norm: 0.09854240068193558\n",
      "Epoch 14/100 - Loss after backpropagation: 0.9564979672431946\n",
      "\n",
      "Epoch 15/100 - Loss before backpropagation: 0.9535525441169739\n",
      "Epoch 15/100 - Gradient norm: 0.08461656583690848\n",
      "Epoch 15/100 - Loss after backpropagation: 0.9535525441169739\n",
      "\n",
      "Epoch 16/100 - Loss before backpropagation: 0.9514860510826111\n",
      "Epoch 16/100 - Gradient norm: 0.07755271819512631\n",
      "Epoch 16/100 - Loss after backpropagation: 0.9514860510826111\n",
      "\n",
      "Epoch 17/100 - Loss before backpropagation: 0.9502255916595459\n",
      "Epoch 17/100 - Gradient norm: 0.08119774972345106\n",
      "Epoch 17/100 - Loss after backpropagation: 0.9502255916595459\n",
      "\n",
      "Epoch 18/100 - Loss before backpropagation: 0.9491822719573975\n",
      "Epoch 18/100 - Gradient norm: 0.08604681286732631\n",
      "Epoch 18/100 - Loss after backpropagation: 0.9491822719573975\n",
      "\n",
      "Epoch 19/100 - Loss before backpropagation: 0.9481236338615417\n",
      "Epoch 19/100 - Gradient norm: 0.08564211926340912\n",
      "Epoch 19/100 - Loss after backpropagation: 0.9481236338615417\n",
      "\n",
      "Epoch 20/100 - Loss before backpropagation: 0.9470468163490295\n",
      "Epoch 20/100 - Gradient norm: 0.08563087220249442\n",
      "Epoch 20/100 - Loss after backpropagation: 0.9470468163490295\n",
      "\n",
      "Epoch 21/100 - Loss before backpropagation: 0.9455700516700745\n",
      "Epoch 21/100 - Gradient norm: 0.08202369637681706\n",
      "Epoch 21/100 - Loss after backpropagation: 0.9455700516700745\n",
      "\n",
      "Epoch 22/100 - Loss before backpropagation: 0.9438106417655945\n",
      "Epoch 22/100 - Gradient norm: 0.07214149147383533\n",
      "Epoch 22/100 - Loss after backpropagation: 0.9438106417655945\n",
      "\n",
      "Epoch 23/100 - Loss before backpropagation: 0.9417818188667297\n",
      "Epoch 23/100 - Gradient norm: 0.06326080232717234\n",
      "Epoch 23/100 - Loss after backpropagation: 0.9417818188667297\n",
      "\n",
      "Epoch 24/100 - Loss before backpropagation: 0.9396879076957703\n",
      "Epoch 24/100 - Gradient norm: 0.053190345784085144\n",
      "Epoch 24/100 - Loss after backpropagation: 0.9396879076957703\n",
      "\n",
      "Epoch 25/100 - Loss before backpropagation: 0.9377699494361877\n",
      "Epoch 25/100 - Gradient norm: 0.05563462103331986\n",
      "Epoch 25/100 - Loss after backpropagation: 0.9377699494361877\n",
      "\n",
      "Epoch 26/100 - Loss before backpropagation: 0.9359139204025269\n",
      "Epoch 26/100 - Gradient norm: 0.054520717628102566\n",
      "Epoch 26/100 - Loss after backpropagation: 0.9359139204025269\n",
      "\n",
      "Epoch 27/100 - Loss before backpropagation: 0.9341030716896057\n",
      "Epoch 27/100 - Gradient norm: 0.056995847833051935\n",
      "Epoch 27/100 - Loss after backpropagation: 0.9341030716896057\n",
      "\n",
      "Epoch 28/100 - Loss before backpropagation: 0.9321261048316956\n",
      "Epoch 28/100 - Gradient norm: 0.056876444424258966\n",
      "Epoch 28/100 - Loss after backpropagation: 0.9321261048316956\n",
      "\n",
      "Epoch 29/100 - Loss before backpropagation: 0.9301837086677551\n",
      "Epoch 29/100 - Gradient norm: 0.052485203550323364\n",
      "Epoch 29/100 - Loss after backpropagation: 0.9301837086677551\n",
      "\n",
      "Epoch 30/100 - Loss before backpropagation: 0.9282602071762085\n",
      "Epoch 30/100 - Gradient norm: 0.047193614674178486\n",
      "Epoch 30/100 - Loss after backpropagation: 0.9282602071762085\n",
      "\n",
      "Epoch 31/100 - Loss before backpropagation: 0.9266716837882996\n",
      "Epoch 31/100 - Gradient norm: 0.05127700262927689\n",
      "Epoch 31/100 - Loss after backpropagation: 0.9266716837882996\n",
      "\n",
      "Epoch 32/100 - Loss before backpropagation: 0.9264872670173645\n",
      "Epoch 32/100 - Gradient norm: 0.05132927220559896\n",
      "Epoch 32/100 - Loss after backpropagation: 0.9264872670173645\n",
      "\n",
      "Epoch 33/100 - Loss before backpropagation: 0.9262751936912537\n",
      "Epoch 33/100 - Gradient norm: 0.05114931062912863\n",
      "Epoch 33/100 - Loss after backpropagation: 0.9262751936912537\n",
      "\n",
      "Epoch 34/100 - Loss before backpropagation: 0.9260392189025879\n",
      "Epoch 34/100 - Gradient norm: 0.05075372805929762\n",
      "Epoch 34/100 - Loss after backpropagation: 0.9260392189025879\n",
      "\n",
      "Epoch 35/100 - Loss before backpropagation: 0.925783097743988\n",
      "Epoch 35/100 - Gradient norm: 0.05017630317915663\n",
      "Epoch 35/100 - Loss after backpropagation: 0.925783097743988\n",
      "\n",
      "Epoch 36/100 - Loss before backpropagation: 0.9255108833312988\n",
      "Epoch 36/100 - Gradient norm: 0.049603862678558756\n",
      "Epoch 36/100 - Loss after backpropagation: 0.9255108833312988\n",
      "\n",
      "Epoch 37/100 - Loss before backpropagation: 0.9252239465713501\n",
      "Epoch 37/100 - Gradient norm: 0.0488048210524006\n",
      "Epoch 37/100 - Loss after backpropagation: 0.9252239465713501\n",
      "\n",
      "Epoch 38/100 - Loss before backpropagation: 0.9249643087387085\n",
      "Epoch 38/100 - Gradient norm: 0.042822731007821205\n",
      "Epoch 38/100 - Loss after backpropagation: 0.9249643087387085\n",
      "\n",
      "Epoch 39/100 - Loss before backpropagation: 0.9247063398361206\n",
      "Epoch 39/100 - Gradient norm: 0.04222180416497274\n",
      "Epoch 39/100 - Loss after backpropagation: 0.9247063398361206\n",
      "\n",
      "Epoch 40/100 - Loss before backpropagation: 0.9244564771652222\n",
      "Epoch 40/100 - Gradient norm: 0.041438631953901885\n",
      "Epoch 40/100 - Loss after backpropagation: 0.9244564771652222\n",
      "\n",
      "Epoch 41/100 - Loss before backpropagation: 0.924213707447052\n",
      "Epoch 41/100 - Gradient norm: 0.041105544701537614\n",
      "Epoch 41/100 - Loss after backpropagation: 0.924213707447052\n",
      "\n",
      "Epoch 42/100 - Loss before backpropagation: 0.923973560333252\n",
      "Epoch 42/100 - Gradient norm: 0.0408224296599425\n",
      "Epoch 42/100 - Loss after backpropagation: 0.923973560333252\n",
      "\n",
      "Epoch 43/100 - Loss before backpropagation: 0.9237362146377563\n",
      "Epoch 43/100 - Gradient norm: 0.04043333036497344\n",
      "Epoch 43/100 - Loss after backpropagation: 0.9237362146377563\n",
      "\n",
      "Epoch 44/100 - Loss before backpropagation: 0.9235013723373413\n",
      "Epoch 44/100 - Gradient norm: 0.040199166551367654\n",
      "Epoch 44/100 - Loss after backpropagation: 0.9235013723373413\n",
      "\n",
      "Epoch 45/100 - Loss before backpropagation: 0.9232676029205322\n",
      "Epoch 45/100 - Gradient norm: 0.03994591244930603\n",
      "Epoch 45/100 - Loss after backpropagation: 0.9232676029205322\n",
      "\n",
      "Epoch 46/100 - Loss before backpropagation: 0.9230430722236633\n",
      "Epoch 46/100 - Gradient norm: 0.038847872170866514\n",
      "Epoch 46/100 - Loss after backpropagation: 0.9230430722236633\n",
      "\n",
      "Epoch 47/100 - Loss before backpropagation: 0.9228183627128601\n",
      "Epoch 47/100 - Gradient norm: 0.03941075076908435\n",
      "Epoch 47/100 - Loss after backpropagation: 0.9228183627128601\n",
      "\n",
      "Epoch 48/100 - Loss before backpropagation: 0.9225853085517883\n",
      "Epoch 48/100 - Gradient norm: 0.03900603053798852\n",
      "Epoch 48/100 - Loss after backpropagation: 0.9225853085517883\n",
      "\n",
      "Epoch 49/100 - Loss before backpropagation: 0.9223502278327942\n",
      "Epoch 49/100 - Gradient norm: 0.03852392152477497\n",
      "Epoch 49/100 - Loss after backpropagation: 0.9223502278327942\n",
      "\n",
      "Epoch 50/100 - Loss before backpropagation: 0.922111451625824\n",
      "Epoch 50/100 - Gradient norm: 0.03898676113078898\n",
      "Epoch 50/100 - Loss after backpropagation: 0.922111451625824\n",
      "\n",
      "Epoch 51/100 - Loss before backpropagation: 0.9218670725822449\n",
      "Epoch 51/100 - Gradient norm: 0.038483560460086885\n",
      "Epoch 51/100 - Loss after backpropagation: 0.9218670725822449\n",
      "\n",
      "Epoch 52/100 - Loss before backpropagation: 0.9216189384460449\n",
      "Epoch 52/100 - Gradient norm: 0.03795708524023302\n",
      "Epoch 52/100 - Loss after backpropagation: 0.9216189384460449\n",
      "\n",
      "Epoch 53/100 - Loss before backpropagation: 0.9213677048683167\n",
      "Epoch 53/100 - Gradient norm: 0.03742197225042793\n",
      "Epoch 53/100 - Loss after backpropagation: 0.9213677048683167\n",
      "\n",
      "Epoch 54/100 - Loss before backpropagation: 0.9211142659187317\n",
      "Epoch 54/100 - Gradient norm: 0.036892826877250026\n",
      "Epoch 54/100 - Loss after backpropagation: 0.9211142659187317\n",
      "\n",
      "Epoch 55/100 - Loss before backpropagation: 0.9208618998527527\n",
      "Epoch 55/100 - Gradient norm: 0.036946686760335475\n",
      "Epoch 55/100 - Loss after backpropagation: 0.9208618998527527\n",
      "\n",
      "Epoch 56/100 - Loss before backpropagation: 0.9206135272979736\n",
      "Epoch 56/100 - Gradient norm: 0.03647678252993074\n",
      "Epoch 56/100 - Loss after backpropagation: 0.9206135272979736\n",
      "\n",
      "Epoch 57/100 - Loss before backpropagation: 0.9203634858131409\n",
      "Epoch 57/100 - Gradient norm: 0.03604304772554559\n",
      "Epoch 57/100 - Loss after backpropagation: 0.9203634858131409\n",
      "\n",
      "Epoch 58/100 - Loss before backpropagation: 0.920113205909729\n",
      "Epoch 58/100 - Gradient norm: 0.03565710207801861\n",
      "Epoch 58/100 - Loss after backpropagation: 0.920113205909729\n",
      "\n",
      "Epoch 59/100 - Loss before backpropagation: 0.9198676347732544\n",
      "Epoch 59/100 - Gradient norm: 0.03621974184184572\n",
      "Epoch 59/100 - Loss after backpropagation: 0.9198676347732544\n",
      "\n",
      "Epoch 60/100 - Loss before backpropagation: 0.9196191430091858\n",
      "Epoch 60/100 - Gradient norm: 0.03600772394612731\n",
      "Epoch 60/100 - Loss after backpropagation: 0.9196191430091858\n",
      "\n",
      "Epoch 61/100 - Loss before backpropagation: 0.9193697571754456\n",
      "Epoch 61/100 - Gradient norm: 0.03485014675890185\n",
      "Epoch 61/100 - Loss after backpropagation: 0.9193697571754456\n",
      "\n",
      "Epoch 62/100 - Loss before backpropagation: 0.9193448424339294\n",
      "Epoch 62/100 - Gradient norm: 0.034832276774248896\n",
      "Epoch 62/100 - Loss after backpropagation: 0.9193448424339294\n",
      "\n",
      "Epoch 63/100 - Loss before backpropagation: 0.9193196892738342\n",
      "Epoch 63/100 - Gradient norm: 0.034814411668752716\n",
      "Epoch 63/100 - Loss after backpropagation: 0.9193196892738342\n",
      "\n",
      "Epoch 64/100 - Loss before backpropagation: 0.9192944169044495\n",
      "Epoch 64/100 - Gradient norm: 0.03478013093431181\n",
      "Epoch 64/100 - Loss after backpropagation: 0.9192944169044495\n",
      "\n",
      "Epoch 65/100 - Loss before backpropagation: 0.9192690253257751\n",
      "Epoch 65/100 - Gradient norm: 0.034760263116081803\n",
      "Epoch 65/100 - Loss after backpropagation: 0.9192690253257751\n",
      "\n",
      "Epoch 66/100 - Loss before backpropagation: 0.9192434549331665\n",
      "Epoch 66/100 - Gradient norm: 0.03474001303716537\n",
      "Epoch 66/100 - Loss after backpropagation: 0.9192434549331665\n",
      "\n",
      "Epoch 67/100 - Loss before backpropagation: 0.9192174673080444\n",
      "Epoch 67/100 - Gradient norm: 0.03471939228155503\n",
      "Epoch 67/100 - Loss after backpropagation: 0.9192174673080444\n",
      "\n",
      "Epoch 68/100 - Loss before backpropagation: 0.9191911816596985\n",
      "Epoch 68/100 - Gradient norm: 0.03469844654891336\n",
      "Epoch 68/100 - Loss after backpropagation: 0.9191911816596985\n",
      "\n",
      "Epoch 69/100 - Loss before backpropagation: 0.9191648364067078\n",
      "Epoch 69/100 - Gradient norm: 0.03467719479986075\n",
      "Epoch 69/100 - Loss after backpropagation: 0.9191648364067078\n",
      "\n",
      "Epoch 70/100 - Loss before backpropagation: 0.9191389679908752\n",
      "Epoch 70/100 - Gradient norm: 0.03286805823311933\n",
      "Epoch 70/100 - Loss after backpropagation: 0.9191389679908752\n",
      "\n",
      "Epoch 71/100 - Loss before backpropagation: 0.919116199016571\n",
      "Epoch 71/100 - Gradient norm: 0.03285366056355181\n",
      "Epoch 71/100 - Loss after backpropagation: 0.919116199016571\n",
      "\n",
      "Epoch 72/100 - Loss before backpropagation: 0.9190932512283325\n",
      "Epoch 72/100 - Gradient norm: 0.03283879641228675\n",
      "Epoch 72/100 - Loss after backpropagation: 0.9190932512283325\n",
      "\n",
      "Epoch 73/100 - Loss before backpropagation: 0.9190701246261597\n",
      "Epoch 73/100 - Gradient norm: 0.0328234669817677\n",
      "Epoch 73/100 - Loss after backpropagation: 0.9190701246261597\n",
      "\n",
      "Epoch 74/100 - Loss before backpropagation: 0.9190472960472107\n",
      "Epoch 74/100 - Gradient norm: 0.0327258800426553\n",
      "Epoch 74/100 - Loss after backpropagation: 0.9190472960472107\n",
      "\n",
      "Epoch 75/100 - Loss before backpropagation: 0.9190266132354736\n",
      "Epoch 75/100 - Gradient norm: 0.031660726676920534\n",
      "Epoch 75/100 - Loss after backpropagation: 0.9190266132354736\n",
      "\n",
      "Epoch 76/100 - Loss before backpropagation: 0.9190061688423157\n",
      "Epoch 76/100 - Gradient norm: 0.031643864554864995\n",
      "Epoch 76/100 - Loss after backpropagation: 0.9190061688423157\n",
      "\n",
      "Epoch 77/100 - Loss before backpropagation: 0.9189856052398682\n",
      "Epoch 77/100 - Gradient norm: 0.03162645347184861\n",
      "Epoch 77/100 - Loss after backpropagation: 0.9189856052398682\n",
      "\n",
      "Epoch 78/100 - Loss before backpropagation: 0.9189649224281311\n",
      "Epoch 78/100 - Gradient norm: 0.0316085417684178\n",
      "Epoch 78/100 - Loss after backpropagation: 0.9189649224281311\n",
      "\n",
      "Epoch 79/100 - Loss before backpropagation: 0.9189441204071045\n",
      "Epoch 79/100 - Gradient norm: 0.03159019791342135\n",
      "Epoch 79/100 - Loss after backpropagation: 0.9189441204071045\n",
      "\n",
      "Epoch 80/100 - Loss before backpropagation: 0.9189231395721436\n",
      "Epoch 80/100 - Gradient norm: 0.031571463956566584\n",
      "Epoch 80/100 - Loss after backpropagation: 0.9189231395721436\n",
      "\n",
      "Epoch 81/100 - Loss before backpropagation: 0.9189021587371826\n",
      "Epoch 81/100 - Gradient norm: 0.031552394896460584\n",
      "Epoch 81/100 - Loss after backpropagation: 0.9189021587371826\n",
      "\n",
      "Epoch 82/100 - Loss before backpropagation: 0.9188810586929321\n",
      "Epoch 82/100 - Gradient norm: 0.03153305118792792\n",
      "Epoch 82/100 - Loss after backpropagation: 0.9188810586929321\n",
      "\n",
      "Epoch 83/100 - Loss before backpropagation: 0.9188598394393921\n",
      "Epoch 83/100 - Gradient norm: 0.03151344986008935\n",
      "Epoch 83/100 - Loss after backpropagation: 0.9188598394393921\n",
      "\n",
      "Epoch 84/100 - Loss before backpropagation: 0.9188387393951416\n",
      "Epoch 84/100 - Gradient norm: 0.03149366636884447\n",
      "Epoch 84/100 - Loss after backpropagation: 0.9188387393951416\n",
      "\n",
      "Epoch 85/100 - Loss before backpropagation: 0.9188173413276672\n",
      "Epoch 85/100 - Gradient norm: 0.03147372211712416\n",
      "Epoch 85/100 - Loss after backpropagation: 0.9188173413276672\n",
      "\n",
      "Epoch 86/100 - Loss before backpropagation: 0.9187960028648376\n",
      "Epoch 86/100 - Gradient norm: 0.03145365354126749\n",
      "Epoch 86/100 - Loss after backpropagation: 0.9187960028648376\n",
      "\n",
      "Epoch 87/100 - Loss before backpropagation: 0.9187744855880737\n",
      "Epoch 87/100 - Gradient norm: 0.03143349528428161\n",
      "Epoch 87/100 - Loss after backpropagation: 0.9187744855880737\n",
      "\n",
      "Epoch 88/100 - Loss before backpropagation: 0.9187529683113098\n",
      "Epoch 88/100 - Gradient norm: 0.03141326208634407\n",
      "Epoch 88/100 - Loss after backpropagation: 0.9187529683113098\n",
      "\n",
      "Epoch 89/100 - Loss before backpropagation: 0.9187316298484802\n",
      "Epoch 89/100 - Gradient norm: 0.031361582344196726\n",
      "Epoch 89/100 - Loss after backpropagation: 0.9187316298484802\n",
      "\n",
      "Epoch 90/100 - Loss before backpropagation: 0.9187103509902954\n",
      "Epoch 90/100 - Gradient norm: 0.031341400263405456\n",
      "Epoch 90/100 - Loss after backpropagation: 0.9187103509902954\n",
      "\n",
      "Epoch 91/100 - Loss before backpropagation: 0.9186888337135315\n",
      "Epoch 91/100 - Gradient norm: 0.03132125563641572\n",
      "Epoch 91/100 - Loss after backpropagation: 0.9186888337135315\n",
      "\n",
      "Epoch 92/100 - Loss before backpropagation: 0.9186866283416748\n",
      "Epoch 92/100 - Gradient norm: 0.03131923967104996\n",
      "Epoch 92/100 - Loss after backpropagation: 0.9186866283416748\n",
      "\n",
      "Epoch 93/100 - Loss before backpropagation: 0.9186845421791077\n",
      "Epoch 93/100 - Gradient norm: 0.03131722342424514\n",
      "Epoch 93/100 - Loss after backpropagation: 0.9186845421791077\n",
      "\n",
      "Epoch 94/100 - Loss before backpropagation: 0.918682336807251\n",
      "Epoch 94/100 - Gradient norm: 0.03131519603919769\n",
      "Epoch 94/100 - Loss after backpropagation: 0.918682336807251\n",
      "\n",
      "Epoch 95/100 - Loss before backpropagation: 0.9186801314353943\n",
      "Epoch 95/100 - Gradient norm: 0.03131317958164207\n",
      "Epoch 95/100 - Loss after backpropagation: 0.9186801314353943\n",
      "\n",
      "Epoch 96/100 - Loss before backpropagation: 0.9186780452728271\n",
      "Epoch 96/100 - Gradient norm: 0.03131113686932866\n",
      "Epoch 96/100 - Loss after backpropagation: 0.9186780452728271\n",
      "\n",
      "Epoch 97/100 - Loss before backpropagation: 0.9186757802963257\n",
      "Epoch 97/100 - Gradient norm: 0.031309108798921424\n",
      "Epoch 97/100 - Loss after backpropagation: 0.9186757802963257\n",
      "\n",
      "Epoch 98/100 - Loss before backpropagation: 0.918673574924469\n",
      "Epoch 98/100 - Gradient norm: 0.0313070620809391\n",
      "Epoch 98/100 - Loss after backpropagation: 0.918673574924469\n",
      "\n",
      "Epoch 99/100 - Loss before backpropagation: 0.9186713695526123\n",
      "Epoch 99/100 - Gradient norm: 0.0313050174962058\n",
      "Epoch 99/100 - Loss after backpropagation: 0.9186713695526123\n",
      "\n",
      "Epoch 100/100 - Loss before backpropagation: 0.9186692237854004\n",
      "Epoch 100/100 - Gradient norm: 0.03130295897240299\n",
      "Epoch 100/100 - Loss after backpropagation: 0.9186692237854004\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation: It seems that the loss is gradually decreasing, which is a good sign that the model is learning. However, you're still seeing no immediate difference between the loss before and after backpropagation within each epoch, which is expected since the loss won't change within a single epoch.\n",
    "\n",
    "\n",
    "Why the Loss Doesn't Change After Backpropagation in the Same Epoch?\n",
    "  - Loss Calculation: When you calculate the loss before backpropagation, it’s based on the model’s current state (parameters).\n",
    "  - Backpropagation: This step calculates the gradients but does not directly affect the loss for the current epoch. It adjusts the model's weights, but the loss will reflect those changes only in the next epoch, after the parameters have been updated.\n",
    "  - Optimizer Step: When the optimizer adjusts the model's parameters (weights), this will only affect the next forward pass, and you'll observe the loss change after that.\n",
    "\n",
    "\n",
    "\n",
    "  Modified code for detailed output"
   ],
   "metadata": {
    "id": "phxn6w_kO2Jx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the number of epochs and batch size for training\n",
    "num_epochs = 100\n",
    "batch_size = 10  # Define batch size (you can adjust this based on your needs)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0  # To track the total loss over all mini-batches in the epoch\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Get the mini-batch of input data and targets\n",
    "        input_batch = inputs[i:i + batch_size]\n",
    "        target_batch = targets[i:i + batch_size]\n",
    "\n",
    "        # Step 1: Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Forward pass (pass the mini-batch through the model)\n",
    "        outputs = model(input_batch)\n",
    "\n",
    "        # Step 3: Compute the loss (compare the mini-batch outputs with the target batch)\n",
    "        loss = criterion(outputs, target_batch)\n",
    "\n",
    "        # Accumulate the total loss for this epoch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Step 4: Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Optimization step (update model parameters)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Optionally, adjust the learning rate after each epoch if you're using a learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the total loss after the epoch for overall monitoring\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Total Loss after epoch: {total_loss/num_samples}\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-yG1t72PADB",
    "outputId": "0c986ae4-da0b-4a36-8aba-b24e3480fb10"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100 - Total Loss after epoch: 0.09187738716602326\n",
      "\n",
      "Epoch 2/100 - Total Loss after epoch: 0.0918707513809204\n",
      "\n",
      "Epoch 3/100 - Total Loss after epoch: 0.09186817646026611\n",
      "\n",
      "Epoch 4/100 - Total Loss after epoch: 0.09186650216579437\n",
      "\n",
      "Epoch 5/100 - Total Loss after epoch: 0.09186520516872405\n",
      "\n",
      "Epoch 6/100 - Total Loss after epoch: 0.0918640685081482\n",
      "\n",
      "Epoch 7/100 - Total Loss after epoch: 0.09186302781105042\n",
      "\n",
      "Epoch 8/100 - Total Loss after epoch: 0.09186204850673675\n",
      "\n",
      "Epoch 9/100 - Total Loss after epoch: 0.09186112105846406\n",
      "\n",
      "Epoch 10/100 - Total Loss after epoch: 0.09186022579669953\n",
      "\n",
      "Epoch 11/100 - Total Loss after epoch: 0.0918593567609787\n",
      "\n",
      "Epoch 12/100 - Total Loss after epoch: 0.09185851633548736\n",
      "\n",
      "Epoch 13/100 - Total Loss after epoch: 0.09185769379138947\n",
      "\n",
      "Epoch 14/100 - Total Loss after epoch: 0.09185688376426697\n",
      "\n",
      "Epoch 15/100 - Total Loss after epoch: 0.09185609817504883\n",
      "\n",
      "Epoch 16/100 - Total Loss after epoch: 0.09185531735420227\n",
      "\n",
      "Epoch 17/100 - Total Loss after epoch: 0.0918545526266098\n",
      "\n",
      "Epoch 18/100 - Total Loss after epoch: 0.09185377657413482\n",
      "\n",
      "Epoch 19/100 - Total Loss after epoch: 0.09185303568840027\n",
      "\n",
      "Epoch 20/100 - Total Loss after epoch: 0.09185230314731598\n",
      "\n",
      "Epoch 21/100 - Total Loss after epoch: 0.09184959471225738\n",
      "\n",
      "Epoch 22/100 - Total Loss after epoch: 0.09184952199459076\n",
      "\n",
      "Epoch 23/100 - Total Loss after epoch: 0.09184944927692414\n",
      "\n",
      "Epoch 24/100 - Total Loss after epoch: 0.09184938251972198\n",
      "\n",
      "Epoch 25/100 - Total Loss after epoch: 0.0918493115901947\n",
      "\n",
      "Epoch 26/100 - Total Loss after epoch: 0.09184923827648163\n",
      "\n",
      "Epoch 27/100 - Total Loss after epoch: 0.09184916317462921\n",
      "\n",
      "Epoch 28/100 - Total Loss after epoch: 0.0918490982055664\n",
      "\n",
      "Epoch 29/100 - Total Loss after epoch: 0.09184902131557465\n",
      "\n",
      "Epoch 30/100 - Total Loss after epoch: 0.0918489521741867\n",
      "\n",
      "Epoch 31/100 - Total Loss after epoch: 0.09184888839721679\n",
      "\n",
      "Epoch 32/100 - Total Loss after epoch: 0.09184881687164306\n",
      "\n",
      "Epoch 33/100 - Total Loss after epoch: 0.09184874415397644\n",
      "\n",
      "Epoch 34/100 - Total Loss after epoch: 0.0918486785888672\n",
      "\n",
      "Epoch 35/100 - Total Loss after epoch: 0.09184861183166504\n",
      "\n",
      "Epoch 36/100 - Total Loss after epoch: 0.09184854328632355\n",
      "\n",
      "Epoch 37/100 - Total Loss after epoch: 0.0918484753370285\n",
      "\n",
      "Epoch 38/100 - Total Loss after epoch: 0.09184841394424438\n",
      "\n",
      "Epoch 39/100 - Total Loss after epoch: 0.09184834063053131\n",
      "\n",
      "Epoch 40/100 - Total Loss after epoch: 0.09184827506542206\n",
      "\n",
      "Epoch 41/100 - Total Loss after epoch: 0.09184820711612701\n",
      "\n",
      "Epoch 42/100 - Total Loss after epoch: 0.09184813976287842\n",
      "\n",
      "Epoch 43/100 - Total Loss after epoch: 0.09184807479381561\n",
      "\n",
      "Epoch 44/100 - Total Loss after epoch: 0.09184800207614899\n",
      "\n",
      "Epoch 45/100 - Total Loss after epoch: 0.09184793591499328\n",
      "\n",
      "Epoch 46/100 - Total Loss after epoch: 0.09184787213802338\n",
      "\n",
      "Epoch 47/100 - Total Loss after epoch: 0.09184780538082123\n",
      "\n",
      "Epoch 48/100 - Total Loss after epoch: 0.09184773743152619\n",
      "\n",
      "Epoch 49/100 - Total Loss after epoch: 0.09184767603874207\n",
      "\n",
      "Epoch 50/100 - Total Loss after epoch: 0.09184760570526124\n",
      "\n",
      "Epoch 51/100 - Total Loss after epoch: 0.09184736669063569\n",
      "\n",
      "Epoch 52/100 - Total Loss after epoch: 0.09184735596179962\n",
      "\n",
      "Epoch 53/100 - Total Loss after epoch: 0.09184735238552094\n",
      "\n",
      "Epoch 54/100 - Total Loss after epoch: 0.09184734761714936\n",
      "\n",
      "Epoch 55/100 - Total Loss after epoch: 0.09184734046459198\n",
      "\n",
      "Epoch 56/100 - Total Loss after epoch: 0.09184732794761657\n",
      "\n",
      "Epoch 57/100 - Total Loss after epoch: 0.09184732258319855\n",
      "\n",
      "Epoch 58/100 - Total Loss after epoch: 0.09184731721878052\n",
      "\n",
      "Epoch 59/100 - Total Loss after epoch: 0.09184731006622314\n",
      "\n",
      "Epoch 60/100 - Total Loss after epoch: 0.09184730172157288\n",
      "\n",
      "Epoch 61/100 - Total Loss after epoch: 0.0918472957611084\n",
      "\n",
      "Epoch 62/100 - Total Loss after epoch: 0.09184728801250458\n",
      "\n",
      "Epoch 63/100 - Total Loss after epoch: 0.091847283244133\n",
      "\n",
      "Epoch 64/100 - Total Loss after epoch: 0.09184727430343628\n",
      "\n",
      "Epoch 65/100 - Total Loss after epoch: 0.09184727430343628\n",
      "\n",
      "Epoch 66/100 - Total Loss after epoch: 0.09184726476669311\n",
      "\n",
      "Epoch 67/100 - Total Loss after epoch: 0.09184725403785705\n",
      "\n",
      "Epoch 68/100 - Total Loss after epoch: 0.09184724807739258\n",
      "\n",
      "Epoch 69/100 - Total Loss after epoch: 0.09184723913669586\n",
      "\n",
      "Epoch 70/100 - Total Loss after epoch: 0.09184723734855652\n",
      "\n",
      "Epoch 71/100 - Total Loss after epoch: 0.09184722900390625\n",
      "\n",
      "Epoch 72/100 - Total Loss after epoch: 0.09184722065925598\n",
      "\n",
      "Epoch 73/100 - Total Loss after epoch: 0.0918472182750702\n",
      "\n",
      "Epoch 74/100 - Total Loss after epoch: 0.09184721350669861\n",
      "\n",
      "Epoch 75/100 - Total Loss after epoch: 0.0918472021818161\n",
      "\n",
      "Epoch 76/100 - Total Loss after epoch: 0.09184719800949097\n",
      "\n",
      "Epoch 77/100 - Total Loss after epoch: 0.09184719264507293\n",
      "\n",
      "Epoch 78/100 - Total Loss after epoch: 0.09184718608856202\n",
      "\n",
      "Epoch 79/100 - Total Loss after epoch: 0.09184717893600464\n",
      "\n",
      "Epoch 80/100 - Total Loss after epoch: 0.09184717714786529\n",
      "\n",
      "Epoch 81/100 - Total Loss after epoch: 0.09184714794158935\n",
      "\n",
      "Epoch 82/100 - Total Loss after epoch: 0.09184714674949646\n",
      "\n",
      "Epoch 83/100 - Total Loss after epoch: 0.0918471485376358\n",
      "\n",
      "Epoch 84/100 - Total Loss after epoch: 0.09184714794158935\n",
      "\n",
      "Epoch 85/100 - Total Loss after epoch: 0.09184714257717133\n",
      "\n",
      "Epoch 86/100 - Total Loss after epoch: 0.09184714317321778\n",
      "\n",
      "Epoch 87/100 - Total Loss after epoch: 0.09184714257717133\n",
      "\n",
      "Epoch 88/100 - Total Loss after epoch: 0.09184714257717133\n",
      "\n",
      "Epoch 89/100 - Total Loss after epoch: 0.09184714317321778\n",
      "\n",
      "Epoch 90/100 - Total Loss after epoch: 0.09184714078903199\n",
      "\n",
      "Epoch 91/100 - Total Loss after epoch: 0.09184714376926423\n",
      "\n",
      "Epoch 92/100 - Total Loss after epoch: 0.09184714019298554\n",
      "\n",
      "Epoch 93/100 - Total Loss after epoch: 0.09184714198112488\n",
      "\n",
      "Epoch 94/100 - Total Loss after epoch: 0.09184714257717133\n",
      "\n",
      "Epoch 95/100 - Total Loss after epoch: 0.09184714019298554\n",
      "\n",
      "Epoch 96/100 - Total Loss after epoch: 0.09184713780879974\n",
      "\n",
      "Epoch 97/100 - Total Loss after epoch: 0.0918471360206604\n",
      "\n",
      "Epoch 98/100 - Total Loss after epoch: 0.09184713780879974\n",
      "\n",
      "Epoch 99/100 - Total Loss after epoch: 0.09184713661670685\n",
      "\n",
      "Epoch 100/100 - Total Loss after epoch: 0.09184713661670685\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation: The model is learning; Training takes place\n",
    "\n",
    "**SUCCESS**"
   ],
   "metadata": {
    "id": "webbgdBMPTPn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "fbjT8F1vqu5P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "hMGBu3zTqu2s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "EwuRxX9tqu0B"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "2BqWoRGJquxW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "CqC6do8_quuj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "z0JwDdstqur5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ke5bpjn8qupT"
   }
  }
 ]
}
