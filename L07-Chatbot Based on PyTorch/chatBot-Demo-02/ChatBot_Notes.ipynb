{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45fKHWou2zvi"
      },
      "source": [
        "# ChatBot Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3aNNeY3Xl3F"
      },
      "source": [
        "### 1. **Understanding the Basics of Chatbots**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Grasp the core concept of chatbots and how they operate.\n",
        "- Understand different types of chatbots: retrieval-based vs. generative.\n",
        "- Learn the high-level architecture of generative models and how sequence-to-sequence (Seq2Seq) models work.\n",
        "- Explore recent advancements and research in conversational AI.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VsqzBaXlzt"
      },
      "source": [
        "\n",
        "#### **1.1 What is a Chatbot?**\n",
        "- A chatbot is a software that can simulate a conversation with users using natural language.\n",
        "- They can be used in various applications like customer service, virtual assistants, and more.\n",
        "\n",
        "**Types of Chatbots:**\n",
        "1. **Retrieval-Based**: Selects a predefined response based on the input.\n",
        "   - Example: Rule-based systems or intent-based systems.\n",
        "2. **Generative-Based**: Generates a response from scratch based on input using machine learning models.\n",
        "   - Example: Neural conversational models.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCNQ4Zp9XlxE"
      },
      "source": [
        "\n",
        "#### **1.2 How Do Chatbots Work?**\n",
        "- **Natural Language Processing (NLP)**: Used to understand and process human language.\n",
        "- **Sequence-to-Sequence (Seq2Seq) Models**: Commonly used in generative chatbots to convert an input sequence (e.g., user query) into an output sequence (e.g., bot response).\n",
        "- **Training Data**: Chatbots need training data, like dialogues between two individuals (Cornell Movie Corpus in the tutorial).\n",
        "  \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpugBgM2pCGC"
      },
      "source": [
        "Observations are in https://colab.research.google.com/drive/1q-eCb_z9MS1LEg1EGSPxp3GrtI5HdCdQ#scrollTo=kcI0EdyDqYtf&line=1&uniqifier=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSiOibWpXluW"
      },
      "source": [
        "\n",
        "#### **1.3 Architecture of a Generative Chatbot**\n",
        "1. **Input**: User input or query.\n",
        "2. **Preprocessing**: Text preprocessing like tokenization, removing special characters, converting to lowercase, etc.\n",
        "3. **Encoder-Decoder Model**: A neural network-based model that:\n",
        "   - **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
        "   - **Decoder**: Decodes the context vector into an output sequence (bot’s response).\n",
        "4. **Postprocessing**: Converts model’s output (tokens) into a human-readable response.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUjZcs0xtYbE"
      },
      "source": [
        "More Observations are in https://colab.research.google.com/drive/1q-eCb_z9MS1LEg1EGSPxp3GrtI5HdCdQ#scrollTo=4ixhXz_VtK8c&line=1&uniqifier=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVhdqkiVXlrj"
      },
      "source": [
        "\n",
        "#### **1.4 Understanding Sequence-to-Sequence Models**\n",
        "- **Sequence-to-Sequence (Seq2Seq)**: Used in tasks like translation, summarization, and chatbots.\n",
        "- **Encoder**: Takes the input sequence and outputs a context vector.\n",
        "- **Decoder**: Generates an output sequence using the context vector from the encoder.\n",
        "\n",
        "**Example: Basic Seq2Seq Process in Code**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample sequence data (Input and Output)\n",
        "# Here, 'input_sentence' represents a sample input sequence for the chatbot,\n",
        "# while 'output_sentence' represents the corresponding response or output sequence.\n",
        "input_sentence = ['hello', 'how', 'are', 'you']\n",
        "output_sentence = ['i', 'am', 'fine']\n"
      ],
      "metadata": {
        "id": "9_9qMGeEJ__D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA4DFe3jYF9n",
        "outputId": "e49067b3-2c27-4e1f-e81d-f3853da46c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: [0, 1, 2, 3]\n",
            "Output Sequence: [4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# Convert words to indices for the chatbot\n",
        "# We create a word-to-index mapping for both input and output words.\n",
        "# This is required because neural networks work with numbers, not text directly.\n",
        "word_to_index = {word: i for i, word in enumerate(input_sentence + output_sentence)}\n",
        "\n",
        "# Simulate input and output sequences as lists of word indices.\n",
        "# 'input_seq' represents the input sentence in index form,\n",
        "# and 'output_seq' represents the output sentence in index form.\n",
        "input_seq = [word_to_index[word] for word in input_sentence]\n",
        "output_seq = [word_to_index[word] for word in output_sentence]\n",
        "\n",
        "# Print the converted sequences to verify them.\n",
        "print(f\"Input Sequence: {input_seq}\")  # Example output: [0, 1, 2, 3]\n",
        "print(f\"Output Sequence: {output_seq}\")  # Example output: [4, 5, 6]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a basic encoder structure using a neural network\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer: maps input word indices to dense vectors of size 'hidden_size'.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # GRU (Gated Recurrent Unit): a type of RNN used to process the sequential data.\n",
        "        # Takes in the embedded input and produces an output and hidden state.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Convert input word index into its embedded vector representation.\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Reshape to (1, 1, hidden_size)\n",
        "        # Pass the embedded vector and hidden state through the GRU.\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        # The GRU returns an output tensor and an updated hidden state.\n",
        "        return output, hidden\n",
        "\n",
        "# Define a basic decoder structure using a neural network\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer: similar to the encoder, maps output word indices to dense vectors.\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # GRU (Gated Recurrent Unit): processes the embedded input and hidden state.\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        # Linear layer: converts the GRU output to a vector of size 'output_size' (word space).\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Embed the input word index into its vector representation.\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        # Pass the embedded input and hidden state through the GRU.\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        # Convert the GRU's output to the desired output word space using a linear layer.\n",
        "        output = self.out(output[0])  # Output shape: (1, output_size)\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "77JU9PdeKIUY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define sizes for the encoder and decoder\n",
        "# 'input_size' is the number of unique words in the input sequence (vocabulary size of the input).\n",
        "input_size = len(input_sentence)  # This remains the same\n",
        "vocab_size = len(word_to_index)  # Total vocabulary size for both input and output\n",
        "output_size = vocab_size  # Adjusting output size to handle the full range of indices\n",
        "\n",
        "# Define hidden_size here\n",
        "hidden_size = 256  # You can choose any appropriate value for the hidden size\n",
        "\n",
        "# Instantiate the encoder and decoder models\n",
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(hidden_size, output_size)\n",
        "\n",
        "# Initialize the hidden state for the GRU. The shape is (num_layers, batch_size, hidden_size).\n",
        "# Here, num_layers is 1 and batch_size is 1.\n",
        "hidden = torch.zeros(1, 1, hidden_size)\n",
        "\n",
        "# Encode the first word of the input sentence (index form) using the encoder.\n",
        "# We feed the first word of the 'input_seq' and the initial hidden state.\n",
        "encoder_output, encoder_hidden = encoder(torch.tensor([input_seq[0]]), hidden)\n",
        "print(f\"Encoder Output: {encoder_output}\")  # Encoded representation of the first word.\n",
        "print(f\"Encoder Hidden State: {encoder_hidden}\")  # Hidden state after processing the first word.\n",
        "\n",
        "# Decode the first word of the output sequence (index form) using the decoder.\n",
        "# The decoder uses the hidden state from the encoder as its initial hidden state.\n",
        "decoder_output, decoder_hidden = decoder(torch.tensor([output_seq[0]]), encoder_hidden)\n",
        "print(f\"Decoder Output: {decoder_output}\")  # Prediction of the first output word.\n",
        "print(f\"Decoder Hidden State: {decoder_hidden}\")  # Hidden state after decoding the first word.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWONKUccKO-1",
        "outputId": "bf1c9287-5532-4614-9cf5-4c2a7c64f81b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output: tensor([[[-0.0087, -0.4041, -0.2698, -0.2807,  0.1964, -0.4463, -0.2079,\n",
            "          -0.1532, -0.1282, -0.1324, -0.3476, -0.1443,  0.0540,  0.3132,\n",
            "           0.2016,  0.2159,  0.2734, -0.1296,  0.0673, -0.0083,  0.1732,\n",
            "          -0.0351, -0.2200,  0.1174,  0.2866,  0.0224,  0.0543,  0.3543,\n",
            "          -0.1644, -0.2400, -0.2027, -0.3075, -0.0224,  0.2182,  0.5625,\n",
            "           0.1046, -0.1946, -0.1661, -0.3401,  0.3801,  0.2512,  0.3851,\n",
            "          -0.4156, -0.3835,  0.1161, -0.5293,  0.0791,  0.0214, -0.3265,\n",
            "           0.0753, -0.3283, -0.4208, -0.0178,  0.4873, -0.1483,  0.0176,\n",
            "           0.3698,  0.2985,  0.1629,  0.5148, -0.3065,  0.1101, -0.0260,\n",
            "           0.5003, -0.4612, -0.1352,  0.4697,  0.0030,  0.3285,  0.2762,\n",
            "           0.2986,  0.0604,  0.0320,  0.1874,  0.0536,  0.0856,  0.2190,\n",
            "          -0.2376,  0.3642, -0.3321, -0.1803, -0.2777,  0.1966,  0.1136,\n",
            "           0.3771,  0.6404,  0.5673,  0.0390,  0.0549, -0.2431, -0.0099,\n",
            "           0.1469,  0.1243,  0.2669, -0.3664,  0.0371,  0.0053, -0.2979,\n",
            "           0.2857,  0.2870,  0.1846,  0.0172, -0.0839, -0.1121, -0.4206,\n",
            "           0.1954,  0.5630, -0.1123, -0.1041, -0.3304, -0.4918, -0.1772,\n",
            "           0.2779, -0.2265, -0.0257, -0.2388,  0.3336, -0.3757,  0.3744,\n",
            "          -0.2683, -0.4657, -0.3978, -0.4973,  0.2175,  0.4468, -0.0198,\n",
            "          -0.2494,  0.0595,  0.1270, -0.3461, -0.3377,  0.3039, -0.0800,\n",
            "           0.3463, -0.0744,  0.0643,  0.1432, -0.4191,  0.0135, -0.5247,\n",
            "           0.2754,  0.2369,  0.1969,  0.2996, -0.0969, -0.0290, -0.0107,\n",
            "          -0.1642,  0.0422, -0.2442,  0.4404, -0.0894,  0.2151, -0.3070,\n",
            "           0.0178,  0.6573, -0.6984, -0.1563, -0.5468,  0.2999, -0.0225,\n",
            "          -0.4436, -0.0727, -0.0538, -0.2907,  0.5355,  0.0096,  0.0169,\n",
            "          -0.1679, -0.0204, -0.0283,  0.2733,  0.1316,  0.4953, -0.0672,\n",
            "          -0.5616, -0.1312, -0.1275, -0.1711, -0.1600,  0.1308,  0.1525,\n",
            "           0.1408, -0.0788,  0.1678, -0.3230, -0.5111, -0.1481, -0.1826,\n",
            "          -0.3216,  0.2317, -0.2426,  0.2298, -0.0863, -0.0027,  0.3756,\n",
            "          -0.2431, -0.1583,  0.0427, -0.0050, -0.1423,  0.1792,  0.3410,\n",
            "           0.3459,  0.0925,  0.2421, -0.4610, -0.0666, -0.0778, -0.3259,\n",
            "           0.3210,  0.1987,  0.2626, -0.0551, -0.2203,  0.0717,  0.0372,\n",
            "           0.2314, -0.3083, -0.0738, -0.1024, -0.2300,  0.3847,  0.0609,\n",
            "          -0.0382,  0.1703,  0.2729,  0.4159, -0.0433,  0.0118, -0.4334,\n",
            "          -0.3660,  0.1524, -0.3485, -0.0400, -0.3663,  0.2227, -0.1575,\n",
            "          -0.2160,  0.1019, -0.1523,  0.2257, -0.2400,  0.2393, -0.0339,\n",
            "          -0.3946,  0.2546, -0.4185, -0.0201, -0.1851,  0.1690,  0.0445,\n",
            "          -0.4724,  0.1554,  0.3713,  0.3971]]], grad_fn=<StackBackward0>)\n",
            "Encoder Hidden State: tensor([[[-0.0087, -0.4041, -0.2698, -0.2807,  0.1964, -0.4463, -0.2079,\n",
            "          -0.1532, -0.1282, -0.1324, -0.3476, -0.1443,  0.0540,  0.3132,\n",
            "           0.2016,  0.2159,  0.2734, -0.1296,  0.0673, -0.0083,  0.1732,\n",
            "          -0.0351, -0.2200,  0.1174,  0.2866,  0.0224,  0.0543,  0.3543,\n",
            "          -0.1644, -0.2400, -0.2027, -0.3075, -0.0224,  0.2182,  0.5625,\n",
            "           0.1046, -0.1946, -0.1661, -0.3401,  0.3801,  0.2512,  0.3851,\n",
            "          -0.4156, -0.3835,  0.1161, -0.5293,  0.0791,  0.0214, -0.3265,\n",
            "           0.0753, -0.3283, -0.4208, -0.0178,  0.4873, -0.1483,  0.0176,\n",
            "           0.3698,  0.2985,  0.1629,  0.5148, -0.3065,  0.1101, -0.0260,\n",
            "           0.5003, -0.4612, -0.1352,  0.4697,  0.0030,  0.3285,  0.2762,\n",
            "           0.2986,  0.0604,  0.0320,  0.1874,  0.0536,  0.0856,  0.2190,\n",
            "          -0.2376,  0.3642, -0.3321, -0.1803, -0.2777,  0.1966,  0.1136,\n",
            "           0.3771,  0.6404,  0.5673,  0.0390,  0.0549, -0.2431, -0.0099,\n",
            "           0.1469,  0.1243,  0.2669, -0.3664,  0.0371,  0.0053, -0.2979,\n",
            "           0.2857,  0.2870,  0.1846,  0.0172, -0.0839, -0.1121, -0.4206,\n",
            "           0.1954,  0.5630, -0.1123, -0.1041, -0.3304, -0.4918, -0.1772,\n",
            "           0.2779, -0.2265, -0.0257, -0.2388,  0.3336, -0.3757,  0.3744,\n",
            "          -0.2683, -0.4657, -0.3978, -0.4973,  0.2175,  0.4468, -0.0198,\n",
            "          -0.2494,  0.0595,  0.1270, -0.3461, -0.3377,  0.3039, -0.0800,\n",
            "           0.3463, -0.0744,  0.0643,  0.1432, -0.4191,  0.0135, -0.5247,\n",
            "           0.2754,  0.2369,  0.1969,  0.2996, -0.0969, -0.0290, -0.0107,\n",
            "          -0.1642,  0.0422, -0.2442,  0.4404, -0.0894,  0.2151, -0.3070,\n",
            "           0.0178,  0.6573, -0.6984, -0.1563, -0.5468,  0.2999, -0.0225,\n",
            "          -0.4436, -0.0727, -0.0538, -0.2907,  0.5355,  0.0096,  0.0169,\n",
            "          -0.1679, -0.0204, -0.0283,  0.2733,  0.1316,  0.4953, -0.0672,\n",
            "          -0.5616, -0.1312, -0.1275, -0.1711, -0.1600,  0.1308,  0.1525,\n",
            "           0.1408, -0.0788,  0.1678, -0.3230, -0.5111, -0.1481, -0.1826,\n",
            "          -0.3216,  0.2317, -0.2426,  0.2298, -0.0863, -0.0027,  0.3756,\n",
            "          -0.2431, -0.1583,  0.0427, -0.0050, -0.1423,  0.1792,  0.3410,\n",
            "           0.3459,  0.0925,  0.2421, -0.4610, -0.0666, -0.0778, -0.3259,\n",
            "           0.3210,  0.1987,  0.2626, -0.0551, -0.2203,  0.0717,  0.0372,\n",
            "           0.2314, -0.3083, -0.0738, -0.1024, -0.2300,  0.3847,  0.0609,\n",
            "          -0.0382,  0.1703,  0.2729,  0.4159, -0.0433,  0.0118, -0.4334,\n",
            "          -0.3660,  0.1524, -0.3485, -0.0400, -0.3663,  0.2227, -0.1575,\n",
            "          -0.2160,  0.1019, -0.1523,  0.2257, -0.2400,  0.2393, -0.0339,\n",
            "          -0.3946,  0.2546, -0.4185, -0.0201, -0.1851,  0.1690,  0.0445,\n",
            "          -0.4724,  0.1554,  0.3713,  0.3971]]], grad_fn=<StackBackward0>)\n",
            "Decoder Output: tensor([[ 0.3265, -0.1519,  0.2046,  0.0941, -0.0622, -0.0367,  0.0465]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Decoder Hidden State: tensor([[[-1.5624e-02, -5.1088e-01, -2.4391e-01, -4.0097e-01, -1.3758e-01,\n",
            "          -4.1682e-01,  2.1907e-01, -2.7783e-01,  2.1142e-01, -6.7860e-02,\n",
            "          -2.7946e-01,  2.3339e-01,  1.2981e-01, -9.2847e-03, -3.4939e-01,\n",
            "          -3.0340e-01,  1.9488e-01, -1.5141e-01, -2.6368e-02, -1.3412e-01,\n",
            "          -2.6452e-02, -2.2388e-02, -2.0319e-01, -2.7027e-01,  9.5772e-02,\n",
            "           1.3794e-01,  3.3516e-01,  4.2341e-01,  2.3153e-01,  3.5808e-01,\n",
            "           2.8639e-02, -3.6029e-01, -3.2882e-02,  2.3984e-02,  5.0985e-01,\n",
            "           6.5837e-01, -3.2804e-01, -2.0664e-01, -1.1206e-01,  1.9051e-01,\n",
            "           2.1839e-01,  2.0012e-01,  2.9143e-02, -3.5985e-03, -1.9281e-01,\n",
            "          -8.2818e-02, -2.0029e-01, -1.1460e-01, -3.8053e-01,  2.0952e-01,\n",
            "          -1.7423e-01, -4.6152e-01,  6.1179e-02,  5.3244e-01, -4.2583e-01,\n",
            "          -2.0293e-01,  1.7722e-01,  2.3994e-01,  3.4521e-01,  4.0253e-01,\n",
            "          -2.4105e-01, -2.4600e-01, -1.4649e-01,  7.5783e-02, -4.6440e-01,\n",
            "          -2.0652e-01,  1.7571e-01,  5.7753e-01,  4.4242e-01,  2.7828e-01,\n",
            "           5.2881e-02, -1.0291e-01,  2.6033e-01,  1.2419e-01,  3.1528e-01,\n",
            "           4.0663e-01, -2.6811e-01, -8.5214e-02,  2.9863e-01,  2.7390e-01,\n",
            "          -2.1560e-01, -5.6038e-01, -4.5259e-02, -4.2421e-01, -2.5676e-01,\n",
            "           5.1372e-01,  4.4652e-01,  1.8782e-01, -6.3628e-02, -3.5304e-01,\n",
            "          -1.1299e-01, -4.5261e-01,  5.0680e-02, -1.0245e-01, -2.3895e-01,\n",
            "           1.3638e-01, -4.6225e-01,  1.5896e-01, -2.7130e-01,  1.7473e-01,\n",
            "           2.7856e-01,  2.1474e-01, -1.7150e-01,  3.0799e-01, -2.9599e-01,\n",
            "          -1.2267e-01,  3.2042e-01,  1.5990e-01, -1.5240e-01, -2.6253e-01,\n",
            "          -5.1959e-01, -8.9954e-02, -5.6038e-01, -1.8806e-01, -4.8174e-01,\n",
            "           1.6581e-01, -1.0520e-01, -1.4197e-01,  2.0009e-01,  7.2259e-02,\n",
            "          -1.2468e-01, -5.6894e-02,  5.6744e-04,  2.1793e-01, -1.5265e-01,\n",
            "           6.6463e-02, -4.3206e-01,  5.9563e-02,  5.3024e-01, -1.9155e-01,\n",
            "          -2.9281e-01,  2.4382e-01, -7.3305e-02, -3.5125e-01,  2.3576e-01,\n",
            "           1.0067e-01,  4.5642e-01, -1.5349e-01,  1.1571e-01, -5.9903e-01,\n",
            "           1.8764e-01,  2.5481e-01,  2.4447e-01,  3.6576e-01, -2.3821e-01,\n",
            "          -3.0310e-01, -1.2629e-01, -2.6193e-01,  2.3832e-02,  3.1396e-01,\n",
            "           4.9054e-01, -5.5096e-01,  2.8221e-01, -9.9188e-02, -2.9565e-01,\n",
            "          -3.5853e-02, -3.9217e-01, -2.2572e-01, -1.6325e-01,  1.8706e-01,\n",
            "           2.7521e-01, -2.8849e-01,  1.2808e-01,  6.6229e-02,  1.7621e-01,\n",
            "          -1.5342e-01,  2.8504e-01, -1.4555e-01,  2.0693e-01,  6.3709e-03,\n",
            "          -3.4123e-01,  1.8269e-01,  2.0077e-01,  1.0432e-01,  4.3957e-01,\n",
            "          -5.7979e-01,  1.7708e-01,  1.0883e-01, -2.2166e-01,  3.3090e-01,\n",
            "           8.3801e-02,  3.9853e-01,  4.6350e-01, -2.0502e-01, -2.0114e-01,\n",
            "          -4.3279e-01, -4.9298e-01, -2.3800e-01, -2.5258e-01, -2.0719e-01,\n",
            "          -5.5100e-01,  7.0401e-02,  5.9523e-02, -2.7117e-01,  2.2568e-01,\n",
            "           1.2200e-02,  2.9442e-01, -1.5582e-01, -1.0103e-01,  4.5593e-01,\n",
            "           2.0027e-01, -1.8315e-01,  2.6061e-01,  5.1349e-02,  1.7357e-01,\n",
            "          -1.2499e-01, -5.8838e-01, -6.2570e-03,  1.9623e-01, -3.3342e-01,\n",
            "           2.2926e-01, -3.8716e-01, -5.2496e-01,  1.6977e-01, -3.6432e-01,\n",
            "          -9.3249e-02, -3.7306e-01,  2.1827e-02,  3.5185e-01, -4.7443e-02,\n",
            "           1.6360e-01,  2.4902e-01,  6.0304e-01,  1.0921e-01,  3.5224e-01,\n",
            "          -3.0718e-01,  3.9540e-02,  1.6726e-01, -3.0636e-01,  7.9096e-02,\n",
            "          -3.1638e-01, -4.8514e-01, -2.3611e-01, -2.2683e-01, -1.2102e-01,\n",
            "          -8.6287e-02,  5.9244e-02, -3.2294e-01, -1.7660e-01,  1.9511e-01,\n",
            "          -3.3976e-01, -2.3818e-01, -3.1160e-01,  3.3124e-02, -3.8872e-03,\n",
            "           2.7005e-02, -2.0988e-02, -8.4152e-02, -1.2151e-01, -6.8671e-02,\n",
            "           5.7208e-02,  3.5057e-01, -1.8758e-01,  1.4637e-01,  3.5631e-01,\n",
            "           5.3311e-01]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKyYUylgXloh"
      },
      "source": [
        "\n",
        "#### **1.5 Observations from Current Research**\n",
        "- **Transformer Models**: Transformers (like GPT, BERT) have largely replaced traditional Seq2Seq models in the latest chatbots. Transformers handle long dependencies more efficiently.\n",
        "- **Large Language Models (LLMs)**: Chatbots like **ChatGPT** and **Google’s Meena** leverage LLMs that are pre-trained on massive datasets and fine-tuned for specific tasks.\n",
        "- **Multimodal Chatbots**: Ongoing research integrates text, image, and voice inputs for creating more interactive and natural chatbots.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ8WfAVbxZrC"
      },
      "source": [
        "More in https://colab.research.google.com/drive/1q-eCb_z9MS1LEg1EGSPxp3GrtI5HdCdQ#scrollTo=P8jekgGcqvBY&line=1&uniqifier=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTI6xV91Xllw"
      },
      "source": [
        "This section forms the foundation for understanding the chatbot's inner workings and the neural architectures used to build one. You can proceed to the next sections after grasping these concepts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EH8LJirXli6"
      },
      "source": [
        "### 2. **Familiarize Yourself with PyTorch**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand PyTorch basics such as tensors, modules, and autograd.\n",
        "- Learn to define and train neural networks.\n",
        "- Get comfortable with using GPU for faster computation.\n",
        "- Understand the importance of optimizers and loss functions in training models.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRlfxfIjXlf-"
      },
      "source": [
        "\n",
        "#### **2.1 Understanding PyTorch Tensors**\n",
        "- **Tensors**: Tensors are the core data structures in PyTorch (similar to NumPy arrays but with GPU acceleration).\n",
        "- **Operations**: You can perform arithmetic operations, matrix multiplication, reshaping, and many other operations on tensors.\n",
        "\n",
        "**Example: Basic Operations on Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gFvkJ1vYRa-",
        "outputId": "6831c69b-f556-4cc6-eb34-64c901745d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of tensors: tensor([5., 7., 9.])\n",
            "Dot product of tensors: 32.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create two tensors\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "# Perform operations\n",
        "sum_result = a + b\n",
        "dot_product = torch.dot(a, b)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Sum of tensors: {sum_result}\")\n",
        "print(f\"Dot product of tensors: {dot_product}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yotf04q2XldR"
      },
      "source": [
        "\n",
        "#### **2.2 Autograd: Automatic Differentiation**\n",
        "- **Autograd**: PyTorch's automatic differentiation engine that helps compute gradients for backpropagation.\n",
        "- **Requires Grad**: You can specify which tensors require gradients during computations, allowing the calculation of gradients during optimization.\n",
        "\n",
        "**Example: Using Autograd for Gradient Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bq7QGiSYUHH",
        "outputId": "a89cfd2d-a5aa-4151-972b-847a5e9ee692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient of y with respect to x: 6.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create tensor with requires_grad=True\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x ** 2  # y = x^2\n",
        "\n",
        "# Perform backpropagation to calculate gradient\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient (dy/dx)\n",
        "print(f\"Gradient of y with respect to x: {x.grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPgvlVcLypZ1"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- PyTorch tracks operations on tensors with `requires_grad=True` and stores the gradient of the function.\n",
        "- This is crucial for optimizing model parameters during training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPCRsQVOXlab"
      },
      "source": [
        "\n",
        "#### **2.3 Building and Training Neural Networks**\n",
        "- **Modules**: In PyTorch, models are built using the `nn.Module` class.\n",
        "- **Forward Pass**: Define how data flows through the network.\n",
        "- **Training Loop**: Include forward pass, loss computation, and backpropagation in the loop.\n",
        "\n",
        "**Example: Simple Neural Network Model**   Check https://colab.research.google.com/drive/1q-eCb_z9MS1LEg1EGSPxp3GrtI5HdCdQ#scrollTo=xNaU8IKaqu8A&line=1&uniqifier=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz5hdR0EXlU5"
      },
      "source": [
        "\n",
        "#### **2.4 Utilizing GPU for Faster Computation**\n",
        "- PyTorch provides easy-to-use functionality for moving data and models to GPU for faster training.\n",
        "- **`torch.cuda.is_available()`**: Checks if a GPU is available.\n",
        "- **`model.to(device)`**: Moves the model to GPU if available.\n",
        "\n",
        "**Example: Moving Tensors to GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiPzSSbwYbOR",
        "outputId": "336c1313-2d9d-4e2b-ed64-b597891e3955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Input tensor device: cpu\n",
            "Model device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move tensors to GPU\n",
        "input_tensor = torch.randn(3).to(device)\n",
        "target_tensor = torch.tensor([0.0, 1.0]).to(device)\n",
        "\n",
        "# Move model to GPU\n",
        "model = SimpleNN(input_size=3, hidden_size=5, output_size=2).to(device)\n",
        "\n",
        "# Print tensor and device info\n",
        "print(f\"Input tensor device: {input_tensor.device}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teM3PWUfXlSK"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- If a GPU is available, both tensors and models should be moved to the device for training.\n",
        "- Leveraging GPU can dramatically speed up model training, especially for large datasets.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-rG9P1yXlPl"
      },
      "source": [
        "\n",
        "#### **2.5 Optimization and Loss Functions**\n",
        "- **Optimizers**: Used to adjust model weights during training. Examples include **Adam**, **SGD**, **RMSProp**.\n",
        "- **Loss Functions**: Measure the error between predictions and actual values. Examples include **MSELoss** (for regression) and **CrossEntropyLoss** (for classification).\n",
        "\n",
        "**Example: Using Optimizer and Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_esfPwHYed5",
        "outputId": "5796759b-9ab6-4085-9fc8-8856ec93ce85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.7571845054626465\n"
          ]
        }
      ],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Example forward pass with dummy data\n",
        "inputs = torch.randn(5, 3).to(device)  # Batch of 5, 3 features each\n",
        "# The target values should be within the range [0, output_size - 1]\n",
        "# Since output_size is 2, valid target values are 0 and 1.\n",
        "targets = torch.tensor([0, 1, 1, 1, 0]).to(device)  # Batch of 5 targets, changed 2 to 1\n",
        "\n",
        "# Forward pass and loss calculation\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaVFLP8XlM5"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- Different tasks require different loss functions and optimizers for efficient training.\n",
        "- Fine-tuning the learning rate and optimizer parameters can significantly impact training performance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_TsUWewXlKQ"
      },
      "source": [
        "\n",
        "#### **2.6 Observations from Current Research**\n",
        "- **Advanced Optimizers**: Modern optimizers like **AdamW** (used in transformers) provide better generalization by addressing issues like weight decay.\n",
        "- **Loss Function Innovations**: Loss functions like **Focal Loss** help focus on difficult-to-classify examples, especially useful in imbalanced datasets.\n",
        "- **Gradient Accumulation**: Used in large-scale models to enable training on smaller GPUs by accumulating gradients over multiple batches.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjwYUZCbXlHg"
      },
      "source": [
        "\n",
        "This section covers the core PyTorch features you need to understand before diving into chatbot-specific tasks like Seq2Seq models. Proceeding with this foundation will make later steps easier to implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PxOHlcd2t49"
      },
      "source": [
        "# Demonstration of ChatBot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdRkIluMXlEy"
      },
      "source": [
        "### 3. **Prepare and Load the Dataset**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Download and prepare the **Cornell Movie-Dialogs Corpus** for training the chatbot.\n",
        "- Learn how to preprocess raw text data.\n",
        "- Extract conversation pairs from the dataset.\n",
        "- Organize the dataset for model training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "garPDnfoXlCG"
      },
      "source": [
        "\n",
        "#### **3.1 Downloading the Dataset**\n",
        "- The **Cornell Movie-Dialogs Corpus** is a popular dataset containing dialogues from movie scripts.\n",
        "- It includes 220,579 conversational exchanges between 10,292 pairs of movie characters, making it a great resource for training chatbots.\n",
        "\n",
        "**Download the Dataset:**\n",
        "- [Cornell Movie Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
        "- Unzip the dataset and place it in a directory accessible for your script.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cFS3SmE6rvb",
        "outputId": "349f8b44-2bc6-4cf6-b3a9-29a4ad5298a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: convokit in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.2.2)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.4.8)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.3.9)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n",
            "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.6.0)\n",
            "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.8)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.5)\n",
            "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n",
            "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.7.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (6.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2024.9.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.4.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install convokit # Install the convokit library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVzbjPar4OAC",
        "outputId": "0fe02fe4-bf58-418f-a6ca-5e5b37e39857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# Download and load the Cornell Movie Dialogs Corpus\n",
        "corpus = Corpus(filename=download(\"movie-corpus\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuSMPgPRXk_Y"
      },
      "source": [
        "\n",
        "#### **3.2 Exploring the Dataset Structure**\n",
        "- The dataset consists of several files, but we focus on `movie_lines.txt` and `movie_conversations.txt`.\n",
        "   - **`movie_lines.txt`**: Contains individual lines of dialogue.\n",
        "   - **`movie_conversations.txt`**: Contains conversation pairs that link lines together.\n",
        "\n",
        "**Example of Loading the Dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDijKYL7Nbe",
        "outputId": "735a69d1-f4b9-490d-b989-06908dcdc9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Corpus Summary ===\n",
            "Number of Speakers: 9035\n",
            "Number of Utterances: 304713\n",
            "Number of Conversations: 83097\n",
            "\n",
            "=== List of Speakers ===\n",
            "['u0', 'u2', 'u3', 'u4', 'u5']\n",
            "\n",
            "=== Number of Conversations ===\n",
            "83097\n",
            "\n",
            "=== Example Conversation ===\n",
            "u7588: No.\n",
            "u7586: Whoever she is, she doesn't give up, does she?\n",
            "\n",
            "=== Example Utterance ===\n",
            "Speaker: u2095\n",
            "Text: Awwh, Charlie.\n",
            "\n",
            "=== Example Utterance Metadata ===\n",
            "Conversation ID: L389797\n",
            "Reply to: L389799\n",
            "Timestamp: None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print summary statistics of the corpus\n",
        "print(\"=== Corpus Summary ===\")\n",
        "corpus.print_summary_stats()\n",
        "\n",
        "# Explore some basic properties of the dataset\n",
        "\n",
        "# Accessing the list of speakers in the corpus\n",
        "print(\"\\n=== List of Speakers ===\")\n",
        "# Get a list of speaker IDs\n",
        "speaker_ids = list(corpus.speakers)\n",
        "# Print the first 5 speaker IDs\n",
        "print(speaker_ids[:5]) # Show first 5 speakers\n",
        "\n",
        "\n",
        "# Accessing the list of conversations in the corpus\n",
        "print(\"\\n=== Number of Conversations ===\")\n",
        "print(len(corpus.conversations))  # Total number of conversations\n",
        "\n",
        "# Access an example conversation\n",
        "example_convo = corpus.random_conversation()  # Get a random conversation\n",
        "print(\"\\n=== Example Conversation ===\")\n",
        "for utterance in example_convo.get_utterance_ids():\n",
        "    print(f\"{corpus.get_utterance(utterance).speaker.id}: {corpus.get_utterance(utterance).text}\")\n",
        "\n",
        "# Access an example utterance (dialogue)\n",
        "print(\"\\n=== Example Utterance ===\")\n",
        "example_utterance = corpus.random_utterance()  # Get a random utterance\n",
        "print(f\"Speaker: {example_utterance.speaker.id}\")\n",
        "print(f\"Text: {example_utterance.text}\")\n",
        "\n",
        "# Get metadata for an utterance\n",
        "print(\"\\n=== Example Utterance Metadata ===\")\n",
        "print(f\"Conversation ID: {example_utterance.conversation_id}\")\n",
        "print(f\"Reply to: {example_utterance.reply_to}\")\n",
        "print(f\"Timestamp: {example_utterance.timestamp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiLLQISP_2LM"
      },
      "source": [
        "\n",
        "**Directory Exploration in Python**:\n",
        "\n",
        "You can list the files and directories within the dataset using Python's built-in functions like `os.listdir()` or `os.walk()`. Here's a basic script to explore the structure of the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAOFmKFbApfY",
        "outputId": "61734753-4cbc-4851-9d94-90dba87aba4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory: /root/.convokit/downloads/movie-corpus\n",
            "Subdirectories: []\n",
            "Files: ['corpus.json', 'speakers.json', 'conversations.json', 'index.json', 'utterances.jsonl']\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the directory to the downloaded movie-corpus path\n",
        "corpus_dir = \"/root/.convokit/downloads/movie-corpus\"  # Adjust this if the path changes\n",
        "\n",
        "# List all files and directories in the corpus directory\n",
        "for root, dirs, files in os.walk(corpus_dir):\n",
        "    print(f\"Directory: {root}\")          # Prints the directory path\n",
        "    print(f\"Subdirectories: {dirs}\")     # Prints the list of subdirectories within the current directory\n",
        "    print(f\"Files: {files}\")             # Prints the list of files in the current directory\n",
        "    print(\"=\"*50)                        # Separator for clarity between directories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVI1p2dyACXM"
      },
      "source": [
        "**File Structure**:\n",
        "\n",
        "Typically, the **Cornell Movie Dialogs Corpus** includes the following key files:\n",
        "- **`movie_lines.txt`**: Contains the text of the movie lines (utterances).\n",
        "- **`movie_conversations.txt`**: Contains the conversation structure, including which lines belong to each conversation.\n",
        "- **`movie_titles_metadata.txt`**: Contains metadata about the movies, such as movie ID, title, and release year.\n",
        "- **`character_metadata.txt`**: Contains metadata about the characters, such as character ID, name, and gender.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB5Lkm-zAGhB"
      },
      "source": [
        "**Understanding the Format of Key Files**:\n",
        "\n",
        "- **`movie_lines.txt`**:\n",
        "  - Format: `lineID +++$+++ characterID +++$+++ movieID +++$+++ characterName +++$+++ text`\n",
        "  - Example:\n",
        "    ```\n",
        "    L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
        "    ```\n",
        "- **`movie_conversations.txt`**:\n",
        "  - Format: `characterID1 +++$+++ characterID2 +++$+++ movieID +++$+++ list_of_lineIDs`\n",
        "  - Example:\n",
        "    ```\n",
        "    u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb__EB-hD0Vl"
      },
      "source": [
        "**Abstract of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5HV-D4iD3gP",
        "outputId": "4e6001cd-ed2b-446a-bcb2-e493238ce08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== corpus.json ===\n",
            "Keys in corpus.json: ['url', 'name']\n",
            "Abstract: This file likely contains metadata about the entire dataset and its organization.\n",
            "\n",
            "=== speakers.json ===\n",
            "Number of speakers: 9035\n",
            "Example speaker data: ('u0', {'meta': {'character_name': 'BIANCA', 'movie_idx': 'm0', 'movie_name': '10 things i hate about you', 'gender': 'f', 'credit_pos': '4'}, 'vectors': []})\n",
            "Abstract: Contains metadata for each speaker, such as ID, name, gender, and movie they appear in.\n",
            "\n",
            "=== conversations.json ===\n",
            "Number of conversations: 83097\n",
            "Example conversation ID: L1044\n",
            "Example conversation: {'meta': {'movie_idx': 'm0', 'movie_name': '10 things i hate about you', 'release_year': '1999', 'rating': '6.90', 'votes': '62847', 'genre': \"['comedy', 'romance']\"}, 'vectors': []}\n",
            "Abstract: This file contains conversations between speakers, listing utterance IDs that make up the conversation.\n",
            "\n",
            "=== index.json ===\n",
            "Keys in index.json: ['utterances-index', 'speakers-index', 'conversations-index', 'overall-index', 'version', 'vectors']\n",
            "Abstract: Likely contains an index mapping between various parts of the dataset, such as linking conversations to speakers or utterances.\n",
            "\n",
            "=== utterances.jsonl ===\n",
            "Number of utterances: 304713\n",
            "Example utterance: {'id': 'L1045', 'conversation_id': 'L1044', 'text': 'They do not!', 'speaker': 'u0', 'meta': {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PRP', 'dep': 'nsubj', 'up': 1, 'dn': []}, {'tok': 'do', 'tag': 'VBP', 'dep': 'ROOT', 'dn': [0, 2, 3]}, {'tok': 'not', 'tag': 'RB', 'dep': 'neg', 'up': 1, 'dn': []}, {'tok': '!', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]}]}, 'reply-to': 'L1044', 'timestamp': None, 'vectors': []}\n",
            "Abstract: Contains individual utterances (dialogue lines) with metadata like speaker ID, text, and conversation ID.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to load and provide abstract for corpus.json\n",
        "def abstract_corpus(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        corpus_data = json.load(f)\n",
        "    print(\"\\n=== corpus.json ===\")\n",
        "    print(f\"Keys in corpus.json: {list(corpus_data.keys())}\")\n",
        "    print(\"Abstract: This file likely contains metadata about the entire dataset and its organization.\")\n",
        "    return corpus_data\n",
        "\n",
        "# Function to load and provide abstract for speakers.json\n",
        "def abstract_speakers(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        speakers_data = json.load(f)\n",
        "    print(\"\\n=== speakers.json ===\")\n",
        "    print(f\"Number of speakers: {len(speakers_data)}\")\n",
        "    print(f\"Example speaker data: {list(speakers_data.items())[0]}\")\n",
        "    print(\"Abstract: Contains metadata for each speaker, such as ID, name, gender, and movie they appear in.\")\n",
        "    return speakers_data\n",
        "\n",
        "# Function to load and provide abstract for conversations.json\n",
        "def abstract_conversations(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        conversations_data = json.load(f)\n",
        "    print(\"\\n=== conversations.json ===\")\n",
        "    print(f\"Number of conversations: {len(conversations_data)}\")\n",
        "    example_convo_id = list(conversations_data.keys())[0]\n",
        "    print(f\"Example conversation ID: {example_convo_id}\")\n",
        "    print(f\"Example conversation: {conversations_data[example_convo_id]}\")\n",
        "    print(\"Abstract: This file contains conversations between speakers, listing utterance IDs that make up the conversation.\")\n",
        "    return conversations_data\n",
        "\n",
        "# Function to load and provide abstract for index.json\n",
        "def abstract_index(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        index_data = json.load(f)\n",
        "    print(\"\\n=== index.json ===\")\n",
        "    print(f\"Keys in index.json: {list(index_data.keys())}\")\n",
        "    print(\"Abstract: Likely contains an index mapping between various parts of the dataset, such as linking conversations to speakers or utterances.\")\n",
        "    return index_data\n",
        "\n",
        "# Function to load and provide abstract for utterances.jsonl\n",
        "def abstract_utterances(file_name):\n",
        "    utterances = []\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            utterances.append(json.loads(line))\n",
        "    print(\"\\n=== utterances.jsonl ===\")\n",
        "    print(f\"Number of utterances: {len(utterances)}\")\n",
        "    print(f\"Example utterance: {utterances[0]}\")\n",
        "    print(\"Abstract: Contains individual utterances (dialogue lines) with metadata like speaker ID, text, and conversation ID.\")\n",
        "    return utterances\n",
        "\n",
        "# File paths\n",
        "corpus_dir = \"/root/.convokit/downloads/movie-corpus\"\n",
        "corpus_file = f\"{corpus_dir}/corpus.json\"\n",
        "speakers_file = f\"{corpus_dir}/speakers.json\"\n",
        "conversations_file = f\"{corpus_dir}/conversations.json\"\n",
        "index_file = f\"{corpus_dir}/index.json\"\n",
        "utterances_file = f\"{corpus_dir}/utterances.jsonl\"\n",
        "\n",
        "# Load and print abstracts\n",
        "corpus_data = abstract_corpus(corpus_file)\n",
        "speakers_data = abstract_speakers(speakers_file)\n",
        "conversations_data = abstract_conversations(conversations_file)\n",
        "index_data = abstract_index(index_file)\n",
        "utterances_data = abstract_utterances(utterances_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At0P8apOXk8t"
      },
      "source": [
        "\n",
        "#### **3.3 Preprocessing the Dataset**\n",
        "- **Tokenization**: Split text into individual tokens (words).\n",
        "- **Normalization**: Convert text to lowercase, remove special characters, and handle contractions.\n",
        "- **Filtering**: Remove sentences that are too long or too short for efficient model training.\n",
        "\n",
        "**Example of Preprocessing Functions:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npX7QQg4-hmk",
        "outputId": "378404cb-f1ae-4202-8857-07640d2dd4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Hello, how are you? I'm fine!\n",
            "Normalized: hello how are you ? i m fine !\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Function to convert Unicode characters to ASCII\n",
        "def unicode_to_ascii(s):\n",
        "    # This line normalizes the string `s` into its decomposed form (NFD),\n",
        "    # which separates characters from their diacritical marks (e.g., \"é\" becomes \"e\" + accent).\n",
        "    # Then, it filters out characters that belong to the 'Mn' category (mark, nonspacing),\n",
        "    # i.e., the diacritical marks themselves are removed.\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Function to normalize a string by making it lowercase and removing non-letter characters\n",
        "def normalize_string(s):\n",
        "    # First, convert the input string to ASCII using the `unicode_to_ascii` function.\n",
        "    # This helps remove any special characters or accents in Unicode format.\n",
        "    s = unicode_to_ascii(s.lower().strip())  # Converts to lowercase and strips leading/trailing spaces.\n",
        "\n",
        "    # The following line ensures that punctuation marks (e.g., `.`, `!`, `?`)\n",
        "    # are properly spaced from words by adding a space before them.\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "\n",
        "    # This line replaces any character that is not a letter (a-z or A-Z),\n",
        "    # a period, an exclamation point, or a question mark with a space.\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "\n",
        "    # Finally, this line collapses multiple spaces into a single space and\n",
        "    # removes any trailing/leading spaces from the result.\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "\n",
        "    # Return the fully normalized string.\n",
        "    return s\n",
        "\n",
        "# Example usage of the normalization function on a sample sentence\n",
        "sentence = \"Hello, how are you? I'm fine!\"  # Original sentence with punctuation and uppercase letters.\n",
        "print(f\"Original: {sentence}\")\n",
        "\n",
        "# Output the normalized sentence, where punctuation is spaced and the sentence is lowercase.\n",
        "print(f\"Normalized: {normalize_string(sentence)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7X5nRUEXk6C"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- Text normalization helps reduce noise and makes the data more consistent for training.\n",
        "- Regular expressions and Unicode normalization ensure compatibility across different text formats.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl7Kcd7IXk3N"
      },
      "source": [
        "\n",
        "#### **3.4 Extracting Conversation Pairs**\n",
        "- **Conversation Pairs**: Extract pairs of sentences where one follows the other in a conversation.\n",
        "- These pairs will be used as the input (query) and output (response) for training the chatbot.\n",
        "\n",
        "**Example: Extracting Sentence Pairs**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tRtZ0kiz-77H"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Function to load utterances from the utterances.jsonl file\n",
        "def load_utterances(file_name):\n",
        "    # Dictionary to store utterances with utterance ID as the key\n",
        "    utterances = {}\n",
        "\n",
        "    # Open the utterances.jsonl file (JSON Lines format)\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        # Each line is a separate JSON object representing an utterance\n",
        "        for line in f:\n",
        "            utterance = json.loads(line.strip())\n",
        "            utterances[utterance['id']] = utterance  # Store the entire utterance object\n",
        "\n",
        "    # Return the dictionary of utterances\n",
        "    return utterances\n",
        "\n",
        "# Function to load conversations from the conversations.json file\n",
        "def load_conversations(file_name, utterances):\n",
        "    # List to store conversations\n",
        "    conversations = []\n",
        "\n",
        "    # Open and load the conversations.json file\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        conversations_data = json.load(f)\n",
        "\n",
        "        # Loop through each conversation in the JSON file\n",
        "        for conversation_id in conversations_data:\n",
        "            # Fetch all utterances with the same conversation_id\n",
        "            conversation = [utterances[utt_id]['text'] for utt_id in utterances if utterances[utt_id]['conversation_id'] == conversation_id]\n",
        "\n",
        "            if conversation:  # Check if the conversation has any utterances\n",
        "                conversations.append(conversation)\n",
        "\n",
        "    # Return the list of conversations (each conversation is a list of utterances)\n",
        "    return conversations\n",
        "\n",
        "# Function to extract sentence pairs (QA pairs) from conversations\n",
        "def extract_sentence_pairs(conversations):\n",
        "    # List to store question-answer pairs\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Loop through each conversation\n",
        "    for conversation in conversations:\n",
        "        # For each pair of consecutive utterances in the conversation, create a QA pair\n",
        "        for i in range(len(conversation) - 1):\n",
        "            qa_pairs.append([conversation[i], conversation[i + 1]])\n",
        "\n",
        "    # Return the list of QA pairs\n",
        "    return qa_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Paths to the dataset files\n",
        "corpus_dir = \"/root/.convokit/downloads/movie-corpus\"  # Path to the dataset directory\n",
        "utterances_file = f\"{corpus_dir}/utterances.jsonl\"\n",
        "conversations_file = f\"{corpus_dir}/conversations.json\"\n",
        "\n",
        "# Load utterances and conversations\n",
        "utterances = load_utterances(utterances_file)  # Load utterances from the utterances.jsonl file\n",
        "\n",
        "print(f\"Loaded {len(utterances)} utterances.\")\n",
        "print(f\"Example utterance: {utterances['L1045']}\")\n",
        "print(f\"Example utterance text: {utterances['L1045']['text']}\")\n",
        "print(f\"Example utterance conversation ID: {utterances['L1045']['conversation_id']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDstY95nRpiR",
        "outputId": "813b6eae-afde-47c7-a384-ca6185f968f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 304713 utterances.\n",
            "Example utterance: {'id': 'L1045', 'conversation_id': 'L1044', 'text': 'They do not!', 'speaker': 'u0', 'meta': {'movie_id': 'm0', 'parsed': [{'rt': 1, 'toks': [{'tok': 'They', 'tag': 'PRP', 'dep': 'nsubj', 'up': 1, 'dn': []}, {'tok': 'do', 'tag': 'VBP', 'dep': 'ROOT', 'dn': [0, 2, 3]}, {'tok': 'not', 'tag': 'RB', 'dep': 'neg', 'up': 1, 'dn': []}, {'tok': '!', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]}]}, 'reply-to': 'L1044', 'timestamp': None, 'vectors': []}\n",
            "Example utterance text: They do not!\n",
            "Example utterance conversation ID: L1044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load conversations\n",
        "conversations = load_conversations(conversations_file, utterances)  # Load  conversations\n",
        "print(f\"Loaded {len(conversations)} conversations.\")\n",
        "print(f\"Example conversation: {conversations[0]}\")\n"
      ],
      "metadata": {
        "id": "y6_2hjFLRvYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract question-answer pairs from the loaded conversations\n",
        "qa_pairs = extract_sentence_pairs(conversations)\n",
        "print(f\"Extracted {len(qa_pairs)} QA pairs.\")\n",
        "print(f\"Example QA pair: {qa_pairs[0]}\")\n",
        "# Print the first 3 examples of the QA pairs to see some of the data\n",
        "print(f\"Example pairs: {qa_pairs[:3]}\")\n"
      ],
      "metadata": {
        "id": "TMurcW9xR0G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDnEsaco-9WY"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- Conversations are linked together through utterance IDs, allowing us to create input-output sentence pairs for chatbot training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM2D2xERXk0V"
      },
      "source": [
        "\n",
        "#### **3.5 Saving the Preprocessed Data**\n",
        "- After preprocessing, save the data in a structured format (e.g., CSV or JSON) for future use.\n",
        "\n",
        "**Example: Saving Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to save extracted sentence pairs to a file\n",
        "def save_pairs(pairs, file_name):\n",
        "    # Open the file for writing in 'w' mode with UTF-8 encoding\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        # Use a CSV writer object with tab delimiter\n",
        "        writer = csv.writer(f, delimiter='\\t')\n",
        "\n",
        "        # Loop through each pair (question-answer pair)\n",
        "        for pair in pairs:\n",
        "            # Write each pair as a row in the file\n",
        "            writer.writerow(pair)\n",
        "\n",
        "# Path to save the formatted data\n",
        "output_file = f\"{corpus_dir}/formatted_movie_lines.txt\"\n",
        "\n",
        "# Save the formatted data (QA pairs) to the specified output file\n",
        "save_pairs(qa_pairs, output_file)\n",
        "\n",
        "# Print confirmation that the data has been saved\n",
        "print(f\"Formatted data saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "H13TR814SFWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x39QFtO4Xkxt"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- The preprocessed data will be used to feed the model in the upcoming steps.\n",
        "- Storing the formatted data allows you to quickly reload it for model training without repeating preprocessing steps.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0aOqnIiXku8"
      },
      "source": [
        "\n",
        "#### **3.6 Observations from Current Research**\n",
        "- **Data Augmentation**: Current research in conversational AI focuses on augmenting dialogue datasets using methods like back-translation and paraphrasing to improve model generalization.\n",
        "- **Self-Supervised Learning**: Models like **GPT-3** and **BERT** utilize self-supervised learning, where the model learns from large unlabelled text datasets, eliminating the need for fully paired conversation datasets.\n",
        "- **Multilingual Data**: Using multilingual dialogue datasets is a growing trend, allowing chatbots to handle multiple languages simultaneously, which broadens usability.\n",
        "\n",
        "---\n",
        "\n",
        "This section prepares the dataset for training the chatbot model. With clean and structured data, you are now ready to move forward and learn about sequence-to-sequence models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWlKt3L7XksM"
      },
      "source": [
        "### 4. **Learn Sequence-to-Sequence (Seq2Seq) Models**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand the concept of **Sequence-to-Sequence (Seq2Seq)** models.\n",
        "- Learn how the **encoder** and **decoder** components work together.\n",
        "- Explore the challenges with basic Seq2Seq models, such as information bottlenecks.\n",
        "- Implement a basic Seq2Seq model using GRU (Gated Recurrent Units).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwu34JWrXkpi"
      },
      "source": [
        "\n",
        "#### **4.1 Introduction to Seq2Seq Models**\n",
        "- **Seq2Seq Models**: Used to convert one sequence into another, often employed in machine translation, text summarization, and chatbots.\n",
        "- **Architecture**:\n",
        "  1. **Encoder**: Reads the input sequence and encodes it into a fixed-size context vector (latent space representation).\n",
        "  2. **Decoder**: Takes the context vector from the encoder and generates the output sequence (response).\n",
        "  \n",
        "**Key Components**:\n",
        "- **Input Sequence**: A series of words representing the user query.\n",
        "- **Context Vector**: The compressed information that the decoder uses to generate a response.\n",
        "- **Output Sequence**: The chatbot’s response generated based on the context vector.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UKUREArXkmy"
      },
      "source": [
        "\n",
        "#### **4.2 Encoder-Decoder Model**\n",
        "- **Encoder**: Reads the input sentence word by word and produces a hidden state (context vector).\n",
        "- **Decoder**: Predicts the next word in the sequence based on the context vector and previously predicted words.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICPucW6ZXkkX"
      },
      "source": [
        "\n",
        "#### **4.3 Challenges in Basic Seq2Seq Models**\n",
        "- **Information Bottleneck**: In simple Seq2Seq models, the entire input sequence must be compressed into a single context vector. This can cause problems with longer sequences, leading to loss of information.\n",
        "  \n",
        "**Example: The Bottleneck Problem**\n",
        "```python\n",
        "# Long input sequence causes difficulty in representing all information in one context vector.\n",
        "input_sequence = \"This is a very long sequence that the encoder must compress into a small fixed-size vector.\"\n",
        "```\n",
        "- **Solution**: Using **attention mechanisms**, which allow the decoder to focus on different parts of the input sequence during the decoding process (covered in later sections).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDZeRoGfXkhw"
      },
      "source": [
        "\n",
        "#### **4.4 Implementing a Basic Seq2Seq Model**\n",
        "\n",
        "**Encoder**:\n",
        "- The encoder processes the input sequence and returns the hidden states (context vector) to the decoder.\n",
        "\n",
        "**Decoder**:\n",
        "- The decoder generates an output sequence based on the context vector.\n",
        "\n",
        "**Example: Implementing Seq2Seq with GRU**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Encoder class using GRU (Gated Recurrent Unit)\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Store the hidden size (dimension of hidden state)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer converts input word indices (of size `input_size`) to dense vectors (of size `hidden_size`)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # GRU layer for sequence modeling, input and output size = `hidden_size`\n",
        "        # This GRU processes the embedded input and returns an output and hidden state\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    # Forward pass for the Encoder: input is a word index, hidden is the previous hidden state\n",
        "    def forward(self, input, hidden):\n",
        "        # Embedding the input word index (converting the word index to its dense representation)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Reshape to (1, 1, hidden_size)\n",
        "\n",
        "        # Pass the embedded input through the GRU along with the previous hidden state\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # Return both the GRU output and the updated hidden state\n",
        "        return output, hidden\n",
        "\n",
        "    # Initializes the hidden state for the GRU (typically all zeros at the start)\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size)  # Shape (1, 1, hidden_size), where 1 is batch size\n",
        "\n",
        "# Define the Decoder class using GRU\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Store the hidden size (dimension of hidden state)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embedding layer converts output word indices (of size `output_size`) to dense vectors (of size `hidden_size`)\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # GRU layer that processes the embedded input and produces an output and hidden state\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Linear layer to map the GRU output to the vocabulary space (output_size = number of words in vocabulary)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Softmax layer to normalize the output, converting it into a log-probability distribution over the vocabulary\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Forward pass for the Decoder: input is a word index, hidden is the previous hidden state\n",
        "    def forward(self, input, hidden):\n",
        "        # Embedding the input word index (converting the word index to its dense representation)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Reshape to (1, 1, hidden_size)\n",
        "\n",
        "        # Pass the embedded input through the GRU along with the previous hidden state\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        # Map the GRU output to the output space (vocabulary) using the Linear layer\n",
        "        output = self.out(output[0])  # Shape: (1, output_size)\n",
        "\n",
        "        # Apply the softmax to get log-probabilities over the vocabulary\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        # Return both the output and the updated hidden state\n",
        "        return output, hidden\n",
        "\n",
        "# Example sizes for a chatbot (or any sequence-to-sequence model)\n",
        "input_size = 10  # Number of words in the input vocabulary\n",
        "output_size = 10  # Number of words in the output vocabulary\n",
        "hidden_size = 16  # Arbitrary hidden layer size, controlling the dimensionality of the model's internal state\n",
        "\n",
        "# Instantiate encoder and decoder models with given input/output sizes and hidden layer size\n",
        "encoder = Encoder(input_size, hidden_size)\n",
        "decoder = Decoder(hidden_size, output_size)\n",
        "\n",
        "# Example input sequence (word indices)\n",
        "# Here we're simulating a sequence of four word indices, e.g., sentence: [1, 2, 3, 4]\n",
        "input_sequence = torch.tensor([1, 2, 3, 4])  # Word indices in the input sentence\n",
        "input_length = input_sequence.size(0)  # Length of the input sequence\n",
        "\n",
        "# Initialize the hidden state for the encoder (typically all zeros at the start of the sequence)\n",
        "encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "# Encode the input sequence\n",
        "# Loop over each word in the input sequence and pass it through the encoder\n",
        "for i in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_sequence[i], encoder_hidden)\n",
        "\n",
        "# Initialize decoder input and hidden state\n",
        "# Decoder input is the <SOS> (start-of-sequence) token, typically index 0 in the vocabulary\n",
        "decoder_input = torch.tensor([0])  # Start with the <SOS> token\n",
        "\n",
        "# The decoder's initial hidden state is the final hidden state from the encoder\n",
        "decoder_hidden = encoder_hidden\n",
        "\n",
        "# Decode the sequence: predict one word at a time (for 5 words, as an example)\n",
        "for _ in range(5):  # Simulate predicting 5 words\n",
        "    # Pass the decoder input and hidden state into the decoder\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "    # `topk(1)` returns the index of the most probable next word from the decoder output (vocabulary space)\n",
        "    topv, topi = decoder_output.topk(1)  # topv = value, topi = index of the top prediction\n",
        "\n",
        "    # The predicted word index is used as the next input for the decoder\n",
        "    decoder_input = topi.squeeze().detach()  # Detach from graph to avoid backprop\n",
        "\n",
        "    # Print the predicted word index (for demonstration purposes)\n",
        "    print(f\"Predicted word index: {decoder_input.item()}\")  # .item() extracts the scalar value from the tensor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfZztsRWVcfq",
        "outputId": "d4866c30-0a8b-4433-c6d6-652b57505593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted word index: 6\n",
            "Predicted word index: 4\n",
            "Predicted word index: 4\n",
            "Predicted word index: 4\n",
            "Predicted word index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIsNGFkTLVin"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- The encoder processes the input one word at a time and generates a context vector.\n",
        "- The decoder uses the context vector to predict the next word in the output sequence.\n",
        "- In this example, we predict the next word using the `topk` function, which selects the most probable word.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOc_PrDPLVfH"
      },
      "source": [
        "\n",
        "#### **4.5 Current Research on Seq2Seq Models**\n",
        "- **Transformers**: While traditional Seq2Seq models use RNNs (like GRU/LSTM), transformers have largely replaced them for most tasks due to their ability to handle longer sequences efficiently.\n",
        "   - **Self-Attention Mechanism**: Allows the model to attend to different parts of the input sequence dynamically.\n",
        "   - **Examples**: BERT, GPT, T5 models are based on transformers and provide state-of-the-art performance in conversational AI tasks.\n",
        "  \n",
        "- **Pretrained Models**: Researchers are leveraging large pre-trained Seq2Seq models like **T5** (Text-to-Text Transfer Transformer) and **BART** to fine-tune for specific tasks like chatbots, improving both accuracy and generalization.\n",
        "\n",
        "---\n",
        "\n",
        "With this understanding of Seq2Seq models, you can now explore attention mechanisms in the next section, which improve the model’s ability to focus on important parts of the input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B96oF247LVcW"
      },
      "source": [
        "### 5. **Attention Mechanisms**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand the limitations of basic Seq2Seq models and how attention mechanisms address them.\n",
        "- Learn about the **Luong Attention** mechanism (used in this chatbot tutorial).\n",
        "- Implement attention in Seq2Seq models to improve performance.\n",
        "- Observe how attention improves the model’s ability to handle longer and complex sequences.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZo1ShZLVZy"
      },
      "source": [
        "\n",
        "#### **5.1 What is Attention?**\n",
        "- In basic Seq2Seq models, the entire input sequence is encoded into a single context vector. This works poorly for long sequences, as information from earlier parts of the sequence can be lost or degraded.\n",
        "- **Attention Mechanism**: Allows the decoder to \"attend\" to different parts of the input sequence at each time step. Instead of relying solely on the final context vector, attention computes a weighted sum of all encoder hidden states, allowing the model to focus on the most relevant parts of the input.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-B0gvxeLVXI"
      },
      "source": [
        "\n",
        "#### **5.2 Luong Attention Mechanism**\n",
        "- The **Luong Attention** mechanism is an extension of the original attention mechanism, designed for improved performance in machine translation and other Seq2Seq tasks.\n",
        "- **Global Attention**: Luong attention uses all encoder hidden states to compute attention, focusing on relevant parts of the input sequence for each output word.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glhd1sd2LVUf"
      },
      "source": [
        "\n",
        "#### **5.3 Score Functions in Luong Attention**\n",
        "Luong et al. proposed different score functions to calculate the attention weights:\n",
        "1. **Dot**: Computes the dot product between the decoder’s hidden state and each encoder hidden state.\n",
        "2. **General**: Applies a linear transformation to the encoder hidden state before computing the dot product.\n",
        "3. **Concat**: Concatenates the decoder hidden state with the encoder hidden state, applies a linear transformation, and then passes it through a non-linearity.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q5gVtbvLVRy"
      },
      "source": [
        "\n",
        "#### **5.4 Implementing Luong Attention Mechanism**\n",
        "\n",
        "**Attention Layer**:\n",
        "- The attention layer calculates attention weights and applies them to the encoder hidden states to compute the final context vector.\n",
        "\n",
        "**Example: Implementing Attention Mechanism in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the Luong Attention class\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        # Store the attention method (dot, general, or concat) and hidden size\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # If the method is 'general', initialize a Linear layer to perform a transformation on the encoder outputs\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        # If the method is 'concat', initialize a Linear layer to combine hidden and encoder outputs\n",
        "        # and a vector `v` for computing the final score\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            # `v` is a learnable parameter that helps in scoring\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    # Dot product score function (hidden and encoder_output should have compatible shapes)\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        # Calculate the dot product of hidden and encoder output over the feature dimension (dim=2)\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    # General score function (applies a linear transformation to the encoder output before dot product)\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        # Apply the linear transformation to the encoder output (as in 'general' attention)\n",
        "        energy = self.attn(encoder_output)\n",
        "        # Calculate the dot product between the hidden state and the transformed encoder output\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    # Concat score function (concatenates hidden state and encoder output, then applies transformation)\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        # Concatenate the expanded hidden state and encoder output along the last dimension\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        # Apply element-wise multiplication with the parameter vector `v` and sum along the feature dimension\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    # Forward function to calculate attention weights for the given hidden state and encoder outputs\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Select the appropriate scoring function based on the specified method\n",
        "        if self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose the attention energies to switch dimensions from (seq_len, batch_size) to (batch_size, seq_len)\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return softmax-normalized attention weights along the sequence dimension (dim=1)\n",
        "        # The shape of the returned attention weights will be (batch_size, 1, seq_len)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "# Example input data for demonstration purposes\n",
        "hidden_size = 256  # Dimensionality of hidden states\n",
        "seq_length = 10    # Length of the sequence (number of encoder outputs)\n",
        "batch_size = 1     # Number of samples in a batch\n",
        "\n",
        "# Initialize hidden states and encoder outputs (random values for demonstration)\n",
        "hidden = torch.randn(batch_size, 1, hidden_size)  # Hidden state of the decoder, shape (batch_size, 1, hidden_size)\n",
        "encoder_outputs = torch.randn(seq_length, batch_size, hidden_size)  # Encoder outputs, shape (seq_len, batch_size, hidden_size)\n",
        "\n",
        "# Initialize Luong attention mechanism (using 'dot' method)\n",
        "attn = LuongAttention('dot', hidden_size)\n",
        "\n",
        "# Calculate attention weights for the decoder hidden state given the encoder outputs\n",
        "attn_weights = attn(hidden, encoder_outputs)\n",
        "\n",
        "# Print the calculated attention weights (for each time step in the sequence)\n",
        "print(f\"Attention weights: {attn_weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM565q_PVxw5",
        "outputId": "029f6968-6c3f-4927-c27e-17ea0d2f6820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([[[1.0000e+00, 3.5767e-16, 1.3094e-31, 3.0301e-14, 1.3298e-26,\n",
            "          4.5273e-22, 2.7123e-18, 1.9256e-13, 9.4392e-22, 2.7738e-23]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The attention weights indicate how much importance (attention) the decoder gives to each part of the input sequence.\n",
        "- Attention enables the decoder to focus on relevant parts of the input, which improves performance, especially with long sequences.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "55WMAyJrV0Dm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwbHiADSLVPP"
      },
      "source": [
        "\n",
        "#### **5.5 Integrating Attention with Decoder**\n",
        "- The decoder uses the attention weights to compute a weighted sum of the encoder outputs, which becomes the context vector for generating the next output word.\n",
        "- In each step, the decoder not only relies on its hidden state but also on this dynamically updated context vector.\n",
        "\n",
        "**Example: Decoder with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        # Store the parameters: attention model type, hidden size, output size, number of layers, dropout rate\n",
        "        self.attn_model = attn_model  # The attention mechanism ('dot', 'general', or 'concat')\n",
        "        self.hidden_size = hidden_size  # Size of the hidden state (and embedding dimension)\n",
        "        self.output_size = output_size  # Vocabulary size (number of possible output words)\n",
        "        self.n_layers = n_layers        # Number of layers in the GRU (default is 1)\n",
        "        self.dropout = dropout          # Dropout rate to prevent overfitting (used for regularization)\n",
        "\n",
        "        # Embedding layer: converts word indices into dense vectors of size `hidden_size`\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # GRU: Takes embedded input and the previous hidden state to produce the next hidden state\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Linear layer: Maps from the hidden state (contextualized by attention) to the output space (vocabulary size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Attention mechanism (using Luong Attention): initialized with the selected attention type\n",
        "        self.attn = LuongAttention(attn_model, hidden_size)\n",
        "\n",
        "    # Forward pass through the decoder with attention\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # Step 1: Embed the input word index (converts the word index to a dense vector)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # Shape: (1, 1, hidden_size)\n",
        "\n",
        "        # Step 2: Pass the embedded word through the GRU, along with the previous hidden state\n",
        "        rnn_output, hidden = self.gru(embedded, hidden)  # Shape: (1, 1, hidden_size)\n",
        "\n",
        "        # Step 3: Calculate attention weights using the RNN output and the encoder outputs\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)  # Shape: (batch_size, 1, seq_len)\n",
        "\n",
        "        # Step 4: Multiply the attention weights with encoder outputs to create the context vector\n",
        "        # `bmm` performs a batch matrix multiplication (attention weights * encoder outputs)\n",
        "        # Transpose encoder_outputs to (batch_size, seq_len, hidden_size) for matrix multiplication\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # Shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Step 5: Concatenate the context vector with the GRU output (along the feature dimension)\n",
        "        concat_output = torch.cat((rnn_output, context), -1)  # Shape: (1, 1, hidden_size * 2)\n",
        "\n",
        "        # Step 6: Pass the concatenated output through the final Linear layer to predict the next word\n",
        "        # The following is the modified line, with the transform (Linear layer) applied\n",
        "        output = self.out(nn.Linear(self.hidden_size * 2, self.hidden_size)(concat_output.squeeze(0))) # Shape: (1, output_size)\n",
        "\n",
        "\n",
        "        # Step 7: Apply log softmax to get the probability distribution over the output vocabulary\n",
        "        output = F.log_softmax(output, dim=1)  # Shape: (1, output_size)\n",
        "\n",
        "        # Return the predicted output, updated hidden state, and attention weights\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "# Example usage of the attention decoder\n",
        "# Assuming the hidden_size, output_size, and encoder_outputs are already defined (from an encoder)\n",
        "attn_decoder = AttnDecoderRNN('dot', hidden_size, output_size)\n",
        "\n",
        "# Example input to the decoder (word index `0` as the <SOS> token)\n",
        "decoder_output, decoder_hidden, attn_weights = attn_decoder(torch.tensor([0]), hidden, encoder_outputs)\n",
        "\n",
        "# Print the predicted output, hidden state, and attention weights\n",
        "print(f\"Decoder output: {decoder_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDtPyKwvV_aO",
        "outputId": "fa5fcbe1-c47b-4758-9b14-53e89dee3f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output: tensor([[-2.6780, -2.6758, -2.3391, -2.2452, -2.2561, -2.2698, -2.5174, -2.3698,\n",
            "         -1.7887, -2.2010]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0_BnTzauZD"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The attention-enabled decoder computes a weighted context vector using attention scores and uses it to generate a better response for the chatbot.\n",
        "- This leads to more accurate and contextually relevant responses in longer conversations.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v15r67EiauWp"
      },
      "source": [
        "\n",
        "#### **5.6 Observations from Current Research**\n",
        "- **Bahdanau Attention (Local Attention)**: Introduced the first attention mechanism, where only a subset of the encoder's hidden states is attended to at each decoding step. The focus shifts as the decoder generates each word.\n",
        "- **Transformer Models**: Attention mechanisms evolved into **self-attention** in transformers, which calculate attention across all words in both the input and output sequences. This is the basis of models like **BERT** and **GPT**.\n",
        "- **Pre-trained Models**: Transformers like **T5**, **BART**, and **GPT-3** now incorporate complex attention mechanisms that excel in many natural language tasks, including conversational AI.\n",
        "\n",
        "---\n",
        "\n",
        "By integrating attention mechanisms into Seq2Seq models, you can significantly improve chatbot performance, especially for long and complex conversations. In the next section, you'll learn how to train the chatbot using mini-batches and implement the full training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkid-NFSauT4"
      },
      "source": [
        "### 6. **Model Building and Training**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand how to build a Seq2Seq model with attention for the chatbot.\n",
        "- Learn the concept of mini-batches to speed up training and utilize GPU efficiently.\n",
        "- Implement the training loop, including loss calculation, backpropagation, and gradient clipping.\n",
        "- Learn how to use teacher forcing for better convergence during training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-k0eTKDauOR"
      },
      "source": [
        "\n",
        "#### **6.1 Building the Full Model: Encoder, Attention, and Decoder**\n",
        "- In this step, we combine the encoder, attention mechanism, and decoder into a complete Seq2Seq model.\n",
        "\n",
        "**Example: Building Full Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, attn_model, n_layers=1, dropout=0.1):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "\n",
        "        # Initialize the encoder and attention-based decoder\n",
        "        self.encoder = Encoder(input_size, hidden_size)  # Encoder to process the input sequence\n",
        "        self.attn_decoder = AttnDecoderRNN(attn_model, hidden_size, output_size, n_layers, dropout)  # Decoder with attention\n",
        "\n",
        "    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
        "        # Initialize the hidden state of the encoder (set to zeros)\n",
        "        encoder_hidden = self.encoder.init_hidden()\n",
        "\n",
        "        # Initialize a tensor to store all encoder outputs\n",
        "        encoder_outputs = torch.zeros(MAX_LENGTH, self.encoder.hidden_size)  # MAX_LENGTH is the length of the input sequence\n",
        "\n",
        "        # Encode the input sequence by passing each word through the encoder\n",
        "        for ei in range(input_seq.size(0)):  # Loop over all words in the input sequence\n",
        "            encoder_output, encoder_hidden = self.encoder(input_seq[ei], encoder_hidden)  # Forward pass for each word\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]  # Save encoder output for attention later\n",
        "\n",
        "        # Decoder setup: initialize decoder input with the <SOS> token\n",
        "        decoder_input = torch.tensor([[SOS_token]])  # Start-of-sequence token (SOS_token must be predefined)\n",
        "        decoder_hidden = encoder_hidden  # The initial hidden state of the decoder is the final hidden state of the encoder\n",
        "\n",
        "        decoded_words = []  # List to store the output sequence (predicted words)\n",
        "\n",
        "        # Loop over the target sequence length to decode each word\n",
        "        for di in range(target_seq.size(0)):\n",
        "            # Pass the current decoder input and hidden state through the attention decoder\n",
        "            decoder_output, decoder_hidden, attn_weights = self.attn_decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            # Get the top prediction (most likely word) from the decoder output\n",
        "            topv, topi = decoder_output.topk(1)  # `topi` contains the index of the predicted word\n",
        "            decoded_words.append(topi.item())  # Append the predicted word index to the output sequence\n",
        "\n",
        "            # Determine if we should use teacher forcing\n",
        "            if random.random() < teacher_forcing_ratio:\n",
        "                # Teacher forcing: use the actual next word from the target sequence as the next input\n",
        "                decoder_input = target_seq[di].unsqueeze(0)\n",
        "            else:\n",
        "                # No teacher forcing: use the predicted word as the next input to the decoder\n",
        "                decoder_input = topi.squeeze().detach()  # Detach from the graph to prevent backprop on the predicted word\n",
        "\n",
        "        # Return the decoded words (predicted output sequence)\n",
        "        return decoded_words\n"
      ],
      "metadata": {
        "id": "KO9tGBlnWMI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISy84993auIU"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The encoder processes the input sequence and provides a context vector.\n",
        "- The attention-based decoder uses this context vector and previous predictions (or teacher forcing) to generate the output sequence.\n",
        "- The model is built to handle different input-output lengths, making it suitable for dialogue modeling.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OefgZ2qhauCc"
      },
      "source": [
        "\n",
        "#### **6.2 Mini-Batches and Padding**\n",
        "- When training a chatbot model, we process multiple sentence pairs (mini-batches) at once to improve efficiency, especially when using GPUs.\n",
        "- Since sentences in the dataset are of variable lengths, they must be padded to the length of the longest sentence in the batch.\n",
        "\n",
        "**Steps for Handling Mini-Batches:**\n",
        "1. **Zero-Padding**: Sentences shorter than the longest sentence in the batch are padded with a special padding token.\n",
        "2. **Batching**: Sentences of varying lengths are grouped into batches for faster processing.\n",
        "\n",
        "**Example: Handling Mini-Batches**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.15.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VGWDYS3sirrl",
        "outputId": "b6723d83-53a7-4dc4-f94c-df8660876ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Collecting torch==2.0.0 (from torchtext==0.15.1)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.26.4)\n",
            "Collecting torchdata==0.6.0 (from torchtext==0.15.1)\n",
            "  Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (919 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
            "Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m635.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 triton-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "5bfaf78b037a40eb92a04884fe6d2fac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "\n",
        "PAD_token = 0  # Define PAD_token with a value, typically 0\n",
        "\n",
        "# Function to pad sequences to the same length (zero-padding)\n",
        "def zero_padding(l, fillvalue=PAD_token):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - l: List of sequences (where each sequence is a list of word indices).\n",
        "    - fillvalue: Token to use for padding (typically PAD_token, e.g., 0).\n",
        "\n",
        "    Returns:\n",
        "    - A list of sequences padded to the length of the longest sequence in the batch,\n",
        "      with shorter sequences padded using the `fillvalue`.\n",
        "\n",
        "    Explanation:\n",
        "    - `zip_longest` takes multiple sequences and pads them with the `fillvalue` (PAD_token)\n",
        "      so that all sequences are of the same length.\n",
        "    \"\"\"\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "# Function to create a binary mask (1 for actual token, 0 for padding token)\n",
        "def binary_matrix(l, value=PAD_token):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - l: List of sequences.\n",
        "    - value: Token to consider as padding (typically PAD_token, e.g., 0).\n",
        "\n",
        "    Returns:\n",
        "    - A binary matrix where 1 indicates an actual token and 0 indicates a padding token.\n",
        "\n",
        "    Explanation:\n",
        "    - This function creates a binary mask for each sequence in the batch where a `1`\n",
        "      indicates a valid token and `0` indicates a padding token.\n",
        "    - This is useful for attention mechanisms to ignore padded positions.\n",
        "    \"\"\"\n",
        "    return [[0 if token == value else 1 for token in seq] for seq in l]\n",
        "\n",
        "# Convert a list of sentences into padded tensor sequences\n",
        "def input_var(l, voc):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - l: List of sentences (strings).\n",
        "    - voc: Vocabulary object used to convert words to indices.\n",
        "\n",
        "    Returns:\n",
        "    - pad_var: Tensor of padded sequences of word indices (of shape [max_seq_len, batch_size]).\n",
        "    - lengths: Tensor containing the lengths of each sequence in the batch (without padding).\n",
        "\n",
        "    Explanation:\n",
        "    - `indexes_from_sentence(voc, sentence)` is a helper function that converts a sentence\n",
        "      (a list of words) into a list of word indices using the provided vocabulary (`voc`).\n",
        "    - The function first converts each sentence to a list of word indices (`indexes_batch`).\n",
        "    - It calculates the actual length of each sequence (used later in RNNs for dynamic processing).\n",
        "    - The sequences are then padded to the length of the longest sequence using `zero_padding`.\n",
        "    - Finally, the padded list is converted to a PyTorch tensor (`pad_var`).\n",
        "    \"\"\"\n",
        "    indexes_batch = [indexes_from_sentence(voc, sentence) for sentence in l]  # Convert each sentence to indices\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])  # Length of each sequence\n",
        "    pad_list = zero_padding(indexes_batch)  # Pad sequences to the same length\n",
        "    pad_var = torch.tensor(pad_list, dtype=torch.long)  # Convert to tensor\n",
        "    return pad_var, lengths\n",
        "\n",
        "# Assume this is how a basic vocabulary is defined using the torchtext library\n",
        "def yield_tokens(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield sentence.split()\n",
        "\n",
        "\n",
        "# Example input sentences\n",
        "input_sentences = ['hello how are you', 'i am fine thank you', 'what about you']\n",
        "# Build vocabulary using build_vocab_from_iterator\n",
        "voc = build_vocab_from_iterator(yield_tokens(input_sentences), specials=[\"<unk>\"])\n",
        "voc.set_default_index(voc[\"<unk>\"])\n",
        "\n",
        "# Convert input sentences to padded tensors and lengths\n",
        "input_var, lengths = input_var(input_sentences, voc)\n",
        "\n",
        "\n",
        "# Create PAD_token after creating the vocabulary, using the <unk> index\n",
        "PAD_token = voc['<unk>']\n",
        "\n",
        "# Print padded sequences and their original lengths\n",
        "print(f\"Padded Sequences:\\n{input_var}\")\n",
        "print(f\"Lengths: {lengths}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "1HbSD3cdWptz",
        "outputId": "172faa5c-013f-472b-b282-2b715a5c084e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/usr/local/lib/python3.10/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0d6c66a0497a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m_TEXT_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://download.pytorch.org/models/text/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# This import is for initializing the methods registered via PyBind11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# This has to happen after the base library is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.10/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG89Lbphat_7"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The sentences are padded to have the same length, making them suitable for processing in a mini-batch.\n",
        "- A **binary mask** is used to ignore the padding tokens during loss calculation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAeOJmvaat8_"
      },
      "source": [
        "\n",
        "#### **6.3 Defining the Loss Function and Masking**\n",
        "- When training with padded sequences, only the non-padded elements should contribute to the loss.\n",
        "- We create a custom loss function that ignores the padded parts using a binary mask.\n",
        "\n",
        "**Masked Loss Function:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_nll_loss(output, target, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - output: Tensor of model predictions (logits or log probabilities),\n",
        "              of shape [batch_size, num_classes].\n",
        "    - target: Tensor of target labels (ground truth indices), of shape [batch_size].\n",
        "    - mask: Binary tensor that indicates which positions are valid (non-padded).\n",
        "            This has the same length as the batch size.\n",
        "\n",
        "    Returns:\n",
        "    - loss: The masked negative log-likelihood (NLL) loss, computed only for the valid tokens (those indicated by the mask).\n",
        "    - n_total: The total number of valid tokens (non-padded) considered in the loss computation.\n",
        "\n",
        "    Explanation:\n",
        "    - This function calculates the NLL loss but only considers valid (non-padded) positions in the sequence.\n",
        "    - Masked tokens (i.e., padding) are ignored in the loss calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    # `n_total` is the total number of valid (non-padded) tokens, calculated by summing the mask values\n",
        "    n_total = mask.sum()\n",
        "\n",
        "    # Use `torch.gather` to retrieve the predicted log-probabilities for the correct target classes\n",
        "    # `target.view(-1, 1)` reshapes the target tensor to be compatible with gather\n",
        "    # `torch.gather(output, 1, target.view(-1, 1))` selects the predicted probability for the correct class\n",
        "    cross_entropy = -torch.log(torch.gather(output, 1, target.view(-1, 1)).squeeze(1))\n",
        "\n",
        "    # Apply the mask to select only valid (non-padded) positions\n",
        "    # `masked_select(mask)` extracts the loss for valid tokens\n",
        "    loss = cross_entropy.masked_select(mask).mean()  # The mean of the valid loss values\n",
        "\n",
        "    # Return the masked loss and the total number of valid tokens\n",
        "    return loss, n_total.item()\n",
        "\n",
        "# Example usage:\n",
        "# `output`: Simulated output from a model, representing predicted probabilities for each class\n",
        "# `target`: Ground truth labels (class indices)\n",
        "# `mask`: Binary mask indicating which tokens are valid (1 for valid tokens, 0 for padding)\n",
        "output = torch.tensor([[0.5, 0.5], [0.7, 0.3], [0.9, 0.1]])  # Dummy model output (batch_size=3, num_classes=2)\n",
        "target = torch.tensor([1, 0, 1])  # Target class indices for each element in the batch\n",
        "mask = torch.tensor([1, 1, 0], dtype=torch.bool)  # Mask indicating that the third sequence is padded (ignored)\n",
        "\n",
        "# Calculate the masked NLL loss\n",
        "loss, n_total = masked_nll_loss(output, target, mask)\n",
        "\n",
        "# Print the computed loss and the total number of valid tokens\n",
        "print(f\"Loss: {loss}, Valid tokens: {n_total}\")\n"
      ],
      "metadata": {
        "id": "bTVfD-l2Wvj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g0LRkuSat2h"
      },
      "source": [
        "\n",
        "#### **6.4 Training Loop**\n",
        "- The training loop involves passing inputs through the encoder and decoder, calculating the loss, and updating model parameters.\n",
        "- **Teacher Forcing**: At a certain probability, the correct target word is passed as the next input to the decoder to guide it during training. This helps the model converge faster.\n",
        "\n",
        "**Example: Training Loop with Teacher Forcing**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_var, target_var, mask, max_target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - input_var: Tensor containing the input sequence batch (word indices).\n",
        "    - target_var: Tensor containing the target sequence batch (word indices).\n",
        "    - mask: Tensor mask that indicates valid positions in the target sequence (to ignore padding).\n",
        "    - max_target_len: Maximum length of the target sequence (all sequences are padded to this length).\n",
        "    - encoder: The encoder model.\n",
        "    - decoder: The decoder model with attention.\n",
        "    - encoder_optimizer: Optimizer for the encoder parameters.\n",
        "    - decoder_optimizer: Optimizer for the decoder parameters.\n",
        "    - batch_size: Size of the batch being processed.\n",
        "    - clip: Gradient clipping value to prevent exploding gradients.\n",
        "\n",
        "    This function trains the Seq2Seq model with attention on a batch of input-output pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Zero out gradients for both encoder and decoder optimizers to prevent gradient accumulation\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass through the encoder model\n",
        "    encoder_hidden = encoder.init_hidden()  # Initialize the encoder's hidden state (often zeros)\n",
        "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)  # Get encoder outputs and final hidden state\n",
        "\n",
        "    # Prepare the decoder input, which starts with the <SOS> token for each sequence in the batch\n",
        "    decoder_input = torch.tensor([[SOS_token for _ in range(batch_size)]])\n",
        "\n",
        "    # The decoder's initial hidden state is the final hidden state from the encoder\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]  # Only take the layers used in the decoder\n",
        "\n",
        "    loss = 0  # Initialize loss\n",
        "    print_losses = []  # List to store loss values for each timestep\n",
        "    n_totals = 0  # Total number of non-padded tokens (valid positions)\n",
        "\n",
        "    # Decide whether to use teacher forcing (using ground truth next word) or not\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # If using teacher forcing (feed ground truth as next input)\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):  # Loop over each timestep in the target sequence\n",
        "            # Forward pass through the decoder\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Teacher forcing: feed the next target word as input\n",
        "            decoder_input = target_var[t].view(1, -1)\n",
        "\n",
        "            # Calculate loss for this timestep using masked NLL loss\n",
        "            mask_loss, n_total = masked_nll_loss(decoder_output, target_var[t], mask[t])\n",
        "            loss += mask_loss  # Accumulate the total loss\n",
        "            print_losses.append(mask_loss.item() * n_total)  # Track loss for printing/debugging\n",
        "            n_totals += n_total  # Update total count of valid tokens\n",
        "\n",
        "    # If not using teacher forcing (feed decoder's predicted word as next input)\n",
        "    else:\n",
        "        for t in range(max_target_len):  # Loop over each timestep in the target sequence\n",
        "            # Forward pass through the decoder\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Get the predicted word (top-1 prediction)\n",
        "            _, topi = decoder_output.topk(1)  # `topi` is the index of the predicted word\n",
        "            decoder_input = topi.squeeze().detach()  # Detach the predicted word to avoid backpropagation\n",
        "\n",
        "            # Calculate loss for this timestep using masked NLL loss\n",
        "            mask_loss, n_total = masked_nll_loss(decoder_output, target_var[t], mask[t])\n",
        "            loss += mask_loss  # Accumulate the total loss\n",
        "            print_losses.append(mask_loss.item() * n_total)  # Track loss for printing/debugging\n",
        "            n_totals += n_total  # Update total count of valid tokens\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Update model parameters\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return the average loss per token (total loss divided by the number of valid tokens)\n",
        "    return sum(print_losses) / n_totals\n"
      ],
      "metadata": {
        "id": "UumB6pMdW6L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWd-B0nDatxA"
      },
      "source": [
        "\n",
        "#### **6.5 Observations from Current Research**\n",
        "- **Gradient Accumulation**: For training large models on smaller GPUs, gradient accumulation is used to simulate a large batch size by accumulating gradients over multiple smaller batches.\n",
        "- **Scheduled Sampling**: Researchers have found that teacher forcing can lead\n",
        "\n",
        " to issues at inference time. **Scheduled sampling** is a technique that gradually reduces the probability of using the true target as input during training, transitioning the model to rely on its predictions.\n",
        "- **Pre-trained Models**: Current state-of-the-art models such as **GPT-3**, **T5**, and **BART** use transformers instead of traditional Seq2Seq architectures. These models are pre-trained on massive datasets and fine-tuned for specific tasks, offering significantly better performance.\n",
        "\n",
        "---\n",
        "\n",
        "This section covers how to build and train the chatbot model efficiently using mini-batches, padding, teacher forcing, and gradient clipping. In the next section, you'll learn how to implement greedy search decoding to interact with the trained chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIdvVyMMatq8"
      },
      "source": [
        "### 7. **Greedy Search Decoding**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand how to use **greedy search** to generate responses from the chatbot during inference.\n",
        "- Learn the difference between greedy search and other decoding strategies (e.g., beam search).\n",
        "- Implement a decoding function to interact with the trained chatbot.\n",
        "- Analyze the strengths and weaknesses of greedy search.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sv6D1IHbm7G"
      },
      "source": [
        "\n",
        "#### **7.1 What is Greedy Search?**\n",
        "- **Greedy Search Decoding**: At each step of decoding, the model selects the token with the highest probability as the next word in the sequence.\n",
        "- This process continues until a special `<EOS>` (End of Sentence) token is generated or the maximum sequence length is reached.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6yzCI0bnAM"
      },
      "source": [
        "\n",
        "#### **7.2 How Does Greedy Search Work?**\n",
        "- In the greedy search, the decoder:\n",
        "   - Takes the most probable word predicted at the current time step.\n",
        "   - Feeds this word back into the decoder as the next input.\n",
        "   - Continues generating words one by one until the `<EOS>` token is produced.\n",
        "\n",
        "**Challenges with Greedy Search**:\n",
        "- **Limited Exploration**: Greedy search only considers the highest-probability word at each time step. It doesn’t explore other potential sequences, leading to suboptimal responses.\n",
        "- **Repetitiveness**: The chatbot may repeat phrases or get stuck in a loop due to local high-probability words.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9QnNLbObnDf"
      },
      "source": [
        "\n",
        "#### **7.3 Implementing Greedy Search in PyTorch**\n",
        "\n",
        "**Greedy Search Decoding Function**:\n",
        "- The decoder uses its hidden state and context vector from the encoder to predict the next word in the sequence.\n",
        "- At each step, the predicted word with the highest probability is used as input for the next decoding step.\n",
        "\n",
        "**Example: Greedy Search Decoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - encoder: The encoder model (usually an RNN/GRU/LSTM-based model).\n",
        "        - decoder: The decoder model with attention (usually an RNN/GRU/LSTM-based model).\n",
        "\n",
        "        Greedy search decoder class that generates a response by choosing the most probable word\n",
        "        at each time step (greedy decoding).\n",
        "        \"\"\"\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - input_seq: The input sequence tensor (word indices).\n",
        "        - input_length: The length of the input sequence.\n",
        "        - max_length: The maximum length of the response/output sequence.\n",
        "\n",
        "        Returns:\n",
        "        - decoded_words: List of word indices corresponding to the decoded sequence.\n",
        "\n",
        "        Explanation:\n",
        "        - This method encodes the input sequence using the encoder, and then decodes it\n",
        "          step-by-step, using greedy search to choose the most probable word at each time step.\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Encode the input sequence\n",
        "        # Initialize the hidden state of the encoder\n",
        "        encoder_hidden = self.encoder.init_hidden()\n",
        "\n",
        "        # Pass the input sequence through the encoder to get the encoder outputs and the final hidden state\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, encoder_hidden)\n",
        "\n",
        "        # Step 2: Initialize the decoder input (start with <SOS> token) and hidden state\n",
        "        # <SOS> token indicates the start of the sentence in sequence models\n",
        "        decoder_input = torch.ones(1, 1, dtype=torch.long) * SOS_token  # Initialize decoder input with <SOS>\n",
        "\n",
        "        # The decoder's initial hidden state is set to the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Step 3: Initialize a list to store the decoded words (generated output sequence)\n",
        "        decoded_words = []\n",
        "\n",
        "        # Step 4: Greedily decode one word at a time up to `max_length`\n",
        "        for _ in range(max_length):\n",
        "            # Pass the decoder input and hidden state through the decoder\n",
        "            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            # Get the most probable word (top-1 prediction) from the decoder's output\n",
        "            topv, topi = decoder_output.topk(1)  # `topi` contains the index of the predicted word\n",
        "\n",
        "            # If the predicted word is <EOS> (end of sentence), stop the decoding process\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')  # Append <EOS> token and break the loop\n",
        "                break\n",
        "            else:\n",
        "                # Otherwise, append the predicted word index to the decoded sequence\n",
        "                decoded_words.append(topi.item())\n",
        "\n",
        "            # Set the predicted word as the next input to the decoder for the next time step\n",
        "            decoder_input = topi.squeeze().detach()  # Detach from the computation graph\n",
        "\n",
        "        # Return the list of decoded word indices\n",
        "        return decoded_words\n",
        "\n",
        "# Example usage of GreedySearchDecoder\n",
        "\n",
        "# Define the example input sequence (sequence of word indices)\n",
        "input_seq = torch.tensor([1, 2, 3, 4])  # Simulated tokenized input sequence\n",
        "input_length = torch.tensor([4])  # Length of the input sequence (batch_size is 1)\n",
        "max_length = 10  # Maximum length of the response sequence\n",
        "\n",
        "# Initialize encoder and decoder models (you should have the Encoder and AttnDecoderRNN classes defined elsewhere)\n",
        "encoder = Encoder(input_size=10, hidden_size=256)  # Input size: 10, Hidden size: 256\n",
        "decoder = AttnDecoderRNN('dot', 256, 10)  # Attention decoder with dot-product attention, hidden_size=256, output_size=10\n",
        "\n",
        "# Initialize GreedySearchDecoder with the encoder and decoder models\n",
        "greedy_decoder = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Perform greedy decoding to get the output sequence\n",
        "decoded_words = greedy_decoder(input_seq, input_length, max_length)\n",
        "\n",
        "# Print the decoded word indices\n",
        "print(f\"Decoded words: {decoded_words}\")\n"
      ],
      "metadata": {
        "id": "UhktCBr2XFqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUaKOO0LbnKX"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "- The greedy search decoder generates a response by choosing the highest-probability word at each step.\n",
        "- It stops once the `<EOS>` token is predicted or when it reaches the maximum sequence length.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXc8aQJbnSD"
      },
      "source": [
        "\n",
        "#### **7.4 Key Considerations in Greedy Search**\n",
        "- **Efficiency**: Greedy search is fast and simple to implement. It does not require storing multiple sequences like beam search.\n",
        "- **Lack of Diversity**: Since greedy search only explores one possible sequence, the generated responses may lack diversity and context.\n",
        "- **Stuck in Local Optima**: The chatbot might settle on locally optimal words, leading to subpar overall responses.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zTaaAc7bnUn"
      },
      "source": [
        "\n",
        "#### **7.5 Beam Search vs. Greedy Search**\n",
        "- **Beam Search**: Unlike greedy search, beam search keeps track of multiple hypotheses (sequences) and explores several possible responses at each step. It’s more computationally expensive but produces better results by balancing between local and global optimizations.\n",
        "  \n",
        "**Comparison:**\n",
        "1. **Greedy Search**: Only considers the single highest probability word at each step.\n",
        "2. **Beam Search**: Keeps the top-N most likely sequences at each step, improving the overall quality of the generated response.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9GhZEvbnap"
      },
      "source": [
        "\n",
        "#### **7.6 Current Research on Decoding Strategies**\n",
        "- **Diverse Beam Search**: Modern research has focused on improving beam search by introducing diversity-promoting mechanisms that encourage varied responses, reducing repetitiveness and increasing conversational richness.\n",
        "- **Nucleus Sampling**: Nucleus sampling (also known as top-p sampling) is a stochastic decoding method that samples words from the most probable subset of the distribution. This approach is used in large models like GPT-3 to generate more diverse and human-like responses.\n",
        "  \n",
        "---\n",
        "\n",
        "This section covered greedy search decoding, a fundamental method for generating chatbot responses. Next, you'll explore how to evaluate and test the trained chatbot model, ensuring that it generates relevant and coherent responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwbOQwxvbngc"
      },
      "source": [
        "### 8. **Evaluate and Test the Model**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Learn how to evaluate the chatbot model’s performance after training.\n",
        "- Implement an evaluation function to interact with the trained chatbot in real-time.\n",
        "- Understand key metrics for evaluating chatbot performance.\n",
        "- Explore the importance of qualitative testing and user feedback in chatbot development.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlTCdz6lbnmI"
      },
      "source": [
        "\n",
        "#### **8.1 Real-Time Chatbot Interaction**\n",
        "- After training, it’s important to interact with the chatbot to ensure it generates coherent and relevant responses.\n",
        "- Inference mode (evaluation mode) is different from training mode. The model does not require the target sequence during inference and instead generates responses based on its predictions.\n",
        "\n",
        "**Steps for Evaluation:**\n",
        "1. Preprocess the input sentence.\n",
        "2. Convert the sentence to a tensor (sequence of word indices).\n",
        "3. Pass the input sequence through the encoder.\n",
        "4. Use a decoder (with greedy search or beam search) to generate a response.\n",
        "5. Convert the output sequence (indices) back into words.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWGik1iWbnpK"
      },
      "source": [
        "\n",
        "#### **8.2 Implementing the Evaluation Function**\n",
        "\n",
        "**Example: Evaluate Function for Chatbot**\n",
        "```python\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=10):\n",
        "    # Preprocess input sentence\n",
        "    indexes_batch = [indexes_from_sentence(voc, sentence)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    input_batch = torch.tensor(indexes_batch, dtype=torch.long).transpose(0, 1)\n",
        "\n",
        "    # Move to evaluation mode\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Pass input batch through encoder\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "        encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
        "\n",
        "        # Use greedy search (or beam search) for decoding\n",
        "        decoded_words = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    # Convert word indices back to words\n",
        "    return ' '.join([voc.index2word[word] for word in decoded_words if word != EOS_token])\n",
        "\n",
        "# Example sentence\n",
        "input_sentence = \"How are you?\"\n",
        "output_sentence = evaluate(encoder, decoder, greedy_decoder, voc, input_sentence)\n",
        "print(f\"Bot response: {output_sentence}\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6SynJfWbnsA"
      },
      "source": [
        "\n",
        "**Explanation:**\n",
        "- **Preprocessing**: The input sentence is tokenized, converted to indices, and then padded to ensure it fits the batch.\n",
        "- **Inference**: The model switches to inference mode using `encoder.eval()` and `decoder.eval()` to disable gradients.\n",
        "- **Decoding**: The model generates a response using the greedy search decoder.\n",
        "- **Postprocessing**: The generated word indices are mapped back to words to form the chatbot’s response.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnZ17_Wbnvj"
      },
      "source": [
        "\n",
        "#### **8.3 Testing the Chatbot**\n",
        "- To test the chatbot, you can interact with it using various input sentences and observe its responses.\n",
        "- Testing should cover:\n",
        "   1. **Simple Queries**: Evaluate if the chatbot responds coherently to straightforward questions (e.g., \"What is your name?\").\n",
        "   2. **Complex Queries**: Test with longer or more ambiguous input to see how the chatbot handles context (e.g., \"Tell me a story about a hero.\").\n",
        "   3. **Edge Cases**: Evaluate the chatbot’s behavior with out-of-vocabulary words, unexpected input, or incomplete sentences.\n",
        "\n",
        "**Example of Interaction Testing:**\n",
        "```python\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input == 'exit':\n",
        "        break\n",
        "\n",
        "    # Get bot response\n",
        "    bot_response = evaluate(encoder, decoder, greedy_decoder, voc, user_input)\n",
        "    print(f\"Bot: {bot_response}\")\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- The chatbot continues to respond to user input until the user types \"exit\".\n",
        "- This interaction loop simulates a conversation between the user and the chatbot.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZRMQBzNbny-"
      },
      "source": [
        "\n",
        "#### **8.4 Metrics for Evaluating Chatbot Performance**\n",
        "\n",
        "- **Quantitative Metrics**:\n",
        "   1. **BLEU Score (Bilingual Evaluation Understudy)**: Compares the n-grams of the generated response with reference responses. Widely used in machine translation.\n",
        "   2. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Measures overlap between the predicted and reference responses, focusing on recall.\n",
        "   3. **Perplexity**: Measures how well the chatbot predicts the next word in a sequence. Lower perplexity indicates better predictive performance.\n",
        "\n",
        "**Example: Calculating BLEU Score**\n",
        "```python\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference response\n",
        "reference = [['i', 'am', 'fine']]\n",
        "# Model-generated response\n",
        "candidate = ['i', 'am', 'good']\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```bash\n",
        "BLEU Score: 0.7598356856515925\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- BLEU score evaluates how similar the model’s generated response is to the reference response.\n",
        "- While BLEU score is commonly used, it has limitations and may not fully capture conversational quality.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ec2ziMybn2r"
      },
      "source": [
        "\n",
        "#### **8.5 Qualitative Evaluation**\n",
        "- **Qualitative Testing**: Involves evaluating the chatbot’s responses subjectively by interacting with it.\n",
        "- **Human Feedback**: User testing and feedback provide valuable insights into how coherent, relevant, and engaging the chatbot is.\n",
        "  \n",
        "**Key Questions for Qualitative Evaluation**:\n",
        "1. Does the chatbot provide meaningful responses?\n",
        "2. Can it maintain context across multiple exchanges?\n",
        "3. Does it handle ambiguous or unexpected input gracefully?\n",
        "4. Are the responses diverse and non-repetitive?\n",
        "\n",
        "**Observations**:\n",
        "- **User Experience**: A good chatbot should feel conversational and natural.\n",
        "- **Coherence**: The chatbot should be able to maintain context across multiple exchanges, especially in multi-turn conversations.\n",
        "- **Diversity**: Avoiding repetitive responses improves the chatbot’s ability to engage users.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVrzm90sbn7X"
      },
      "source": [
        "\n",
        "#### **8.6 Observations from Current Research**\n",
        "- **Interactive Evaluation**: Researchers are exploring interactive evaluation methods where chatbots are tested by engaging with real users in dynamic environments, leading to more realistic evaluation outcomes.\n",
        "- **User-Centric Metrics**: Recent advancements propose user-centric metrics, such as **user satisfaction** and **engagement scores**, which are derived from real interactions rather than reference-based metrics like BLEU.\n",
        "- **Reinforcement Learning for Dialogue**: Some chatbots are fine-tuned using reinforcement learning, where the chatbot learns to maximize rewards based on user feedback or predefined objectives, leading to more coherent and context-aware conversations.\n",
        "\n",
        "---\n",
        "\n",
        "This section covers how to test and evaluate the chatbot after training, ensuring it generates meaningful responses. In the next section, you’ll explore methods for improving the chatbot, including experimenting with advanced decoding techniques and fine-tuning the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdc0Jk7zcAk8"
      },
      "source": [
        "### 9. **Experiment with Improvements**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Understand how to experiment with different techniques to improve chatbot performance.\n",
        "- Explore advanced decoding strategies like beam search and sampling.\n",
        "- Learn how fine-tuning and transfer learning can enhance chatbot capabilities.\n",
        "- Experiment with different hyperparameters and training strategies to optimize the model.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YYyTJmYcAiH"
      },
      "source": [
        "\n",
        "#### **9.1 Beam Search Decoding**\n",
        "- **Beam Search**: Unlike greedy search, beam search maintains multiple possible sequences at each step, allowing the decoder to explore several paths before selecting the best output sequence.\n",
        "- Beam search is more computationally expensive but produces more diverse and coherent responses than greedy search.\n",
        "\n",
        "**How Beam Search Works**:\n",
        "1. At each time step, instead of selecting the highest probability word, beam search keeps the top-N most probable sequences.\n",
        "2. At the next step, it extends these top sequences by selecting the most probable next word.\n",
        "3. The search continues until all sequences reach an `<EOS>` token or the maximum length.\n",
        "\n",
        "**Example: Beam Search Implementation**\n",
        "```python\n",
        "class BeamSearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, beam_width=3):\n",
        "        super(BeamSearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Encode the input sequence\n",
        "        encoder_hidden = self.encoder.init_hidden()\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, encoder_hidden)\n",
        "\n",
        "        # Initialize beam search\n",
        "        beams = [(torch.ones(1, 1, dtype=torch.long) * SOS_token, encoder_hidden, 0.0)]  # (sequence, hidden, score)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            candidates = []\n",
        "            for seq, hidden, score in beams:\n",
        "                decoder_output, hidden, _ = self.decoder(seq[-1], hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(self.beam_width)\n",
        "                for i in range(self.beam_width):\n",
        "                    next_seq = torch.cat([seq, topi[i].view(1, 1)], dim=0)\n",
        "                    next_score = score + topv[i].item()\n",
        "                    candidates.append((next_seq, hidden, next_score))\n",
        "\n",
        "            # Keep only top N sequences (beam width)\n",
        "            beams = sorted(candidates, key=lambda x: x[2], reverse=True)[:self.beam_width]\n",
        "\n",
        "            # Stop if all sequences end with <EOS>\n",
        "            if all(seq[-1].item() == EOS_token for seq, _, _ in beams):\n",
        "                break\n",
        "\n",
        "        # Return the sequence with the highest score\n",
        "        return beams[0][0]\n",
        "\n",
        "# Example usage of beam search\n",
        "beam_decoder = BeamSearchDecoder(encoder, decoder)\n",
        "decoded_sequence = beam_decoder(input_seq, input_length, max_length)\n",
        "print(f\"Beam search decoded sequence: {decoded_sequence}\")\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```bash\n",
        "Beam search decoded sequence: [1, 2, 4, 9, '<EOS>']\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- Beam search considers multiple candidate sequences at each decoding step, improving the overall quality of the generated response.\n",
        "- The `beam_width` parameter controls how many alternative sequences the model explores at each step.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3o-ejMwcAdy"
      },
      "source": [
        "\n",
        "#### **9.2 Sampling Methods: Top-k and Nucleus (Top-p) Sampling**\n",
        "- **Top-k Sampling**: Instead of always picking the highest probability word, top-k sampling randomly selects from the top-k most probable words, adding randomness to the decoding process.\n",
        "- **Nucleus Sampling (Top-p Sampling)**: Instead of limiting the selection to the top-k words, nucleus sampling dynamically chooses from the smallest set of words whose cumulative probability exceeds a threshold `p`. This creates more diverse and context-aware responses.\n",
        "\n",
        "**Example: Top-k Sampling Implementation**\n",
        "```python\n",
        "def top_k_sampling(decoder_output, k=10):\n",
        "    probabilities = torch.softmax(decoder_output, dim=1)\n",
        "    topv, topi = torch.topk(probabilities, k)\n",
        "    indices = topi.squeeze()\n",
        "    selected_idx = torch.multinomial(probabilities, 1)\n",
        "    return selected_idx.item()\n",
        "\n",
        "# Example decoder output\n",
        "decoder_output = torch.randn(1, 10)  # 10 possible words\n",
        "predicted_word = top_k_sampling(decoder_output, k=5)\n",
        "print(f\"Predicted word index with top-k sampling: {predicted_word}\")\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```bash\n",
        "Predicted word index with top-k sampling: 3\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **Top-k sampling** introduces diversity by randomly selecting from the top-k most probable words instead of the single most probable one.\n",
        "- This reduces the risk of repetitive or deterministic responses.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMow5FddcAWp"
      },
      "source": [
        "\n",
        "#### **9.3 Fine-Tuning and Transfer Learning**\n",
        "- **Fine-Tuning**: Once a chatbot is trained, it can be further fine-tuned on a smaller dataset to improve performance for a specific domain (e.g., customer service, healthcare).\n",
        "   - Fine-tuning adjusts pre-trained model weights to adapt to new, domain-specific conversations.\n",
        "  \n",
        "**Steps for Fine-Tuning**:\n",
        "1. Load a pre-trained model.\n",
        "2. Freeze earlier layers (optional) and fine-tune the final layers with domain-specific data.\n",
        "\n",
        "**Example: Fine-Tuning a Pre-trained Model**\n",
        "```python\n",
        "# Freeze the encoder and fine-tune only the decoder\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Fine-tune the decoder with new training data\n",
        "for epoch in range(fine_tune_epochs):\n",
        "    loss = train(input_var, target_var, mask, max_target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "    print(f\"Fine-tuning loss: {loss}\")\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- Fine-tuning can be done with a smaller learning rate and fewer epochs to adapt the model to a new domain without overfitting.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfjmiuLZcARH"
      },
      "source": [
        "\n",
        "#### **9.4 Hyperparameter Tuning**\n",
        "- Experimenting with different hyperparameters can significantly improve chatbot performance:\n",
        "   - **Learning Rate**: A lower learning rate may help achieve more stable convergence during training.\n",
        "   - **Dropout**: Adding dropout to the model helps prevent overfitting.\n",
        "   - **Batch Size**: Adjusting the batch size influences the speed and stability of training.\n",
        "\n",
        "**Example: Tuning Learning Rate and Dropout**\n",
        "```python\n",
        "# Experimenting with learning rate and dropout\n",
        "decoder = AttnDecoderRNN(attn_model='dot', hidden_size=256, output_size=10, dropout=0.3)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- Experimenting with different dropout rates and learning rates can help find the right balance between underfitting and overfitting.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcI_SgmYcAKP"
      },
      "source": [
        "\n",
        "#### **9.5 Current Research and Advanced Techniques**\n",
        "- **Reinforcement Learning**: Some chatbots are fine-tuned using reinforcement learning, where the model receives feedback from users or from a reward function and improves over time. This leads to more dynamic and engaging conversations.\n",
        "- **Memory-Augmented Networks**: Chatbots are increasingly using external memory modules to store and retrieve information, improving their ability to maintain context over long conversations.\n",
        "- **Pre-Trained Large Language Models**: Pre-trained models like **GPT-3**, **BERT**, and **T5** dominate the chatbot landscape due to their ability to generalize and handle a wide range of topics. Fine-tuning these models on domain-specific data can yield impressive results.\n",
        "\n",
        "---\n",
        "\n",
        "This section highlights various ways to improve chatbot performance through advanced decoding methods, fine-tuning, and experimentation. In the final section, you'll explore deployment options and how to make your chatbot accessible in real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkFP7Ko6cAGR"
      },
      "source": [
        "### 10. **Deploy the Chatbot**\n",
        "\n",
        "**Key Learning Points:**\n",
        "- Learn how to deploy the trained chatbot model in real-world applications.\n",
        "- Understand different deployment options, such as web interfaces, cloud services, and APIs.\n",
        "- Explore how to save and load the trained model for future use.\n",
        "- Consider scalability, accessibility, and performance in deployment.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.1 Saving and Loading the Trained Model**\n",
        "- After training the chatbot, it is essential to save the model’s weights and architecture so that it can be loaded later for inference or further training.\n",
        "  \n",
        "**Steps for Saving and Loading the Model**:\n",
        "1. **Saving**: Save both the encoder and decoder models, along with their optimizers, to a file.\n",
        "2. **Loading**: Reload the models from the saved files for inference or future use.\n",
        "\n",
        "**Example: Saving and Loading the Model**\n",
        "```python\n",
        "# Saving the trained encoder, decoder, and optimizers\n",
        "def save_model(encoder, decoder, encoder_optimizer, decoder_optimizer, file_path):\n",
        "    torch.save({\n",
        "        'encoder_state_dict': encoder.state_dict(),\n",
        "        'decoder_state_dict': decoder.state_dict(),\n",
        "        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "        'decoder_optimizer_state_dict': decoder_optimizer.state_dict()\n",
        "    }, file_path)\n",
        "    print(f\"Model saved at {file_path}\")\n",
        "\n",
        "# Loading the saved models\n",
        "def load_model(file_path, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
        "    checkpoint = torch.load(file_path)\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
        "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
        "    encoder.eval()  # Set models to evaluation mode\n",
        "    decoder.eval()\n",
        "    print(f\"Model loaded from {file_path}\")\n",
        "\n",
        "# Save the model\n",
        "save_model(encoder, decoder, encoder_optimizer, decoder_optimizer, 'chatbot_model.pth')\n",
        "\n",
        "# Load the model\n",
        "load_model('chatbot_model.pth', encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **Saving**: Saves the current state of the model (weights and optimizer states) to a file.\n",
        "- **Loading**: Restores the model’s weights and optimizer states for further use in inference or continued training.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.2 Building a Web Interface for the Chatbot**\n",
        "- One common way to deploy a chatbot is by building a web interface where users can interact with it in real-time.\n",
        "- A lightweight web framework like **Flask** can be used to create a simple web interface for the chatbot.\n",
        "\n",
        "**Steps for Building a Web Interface**:\n",
        "1. Create a web server that can handle user inputs.\n",
        "2. Load the trained chatbot model and use it to generate responses based on user queries.\n",
        "3. Serve the chatbot responses on a web page.\n",
        "\n",
        "**Example: Flask Application for Chatbot**\n",
        "```python\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "load_model('chatbot_model.pth', encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json['message']\n",
        "    response = evaluate(encoder, decoder, greedy_decoder, voc, user_input)\n",
        "    return jsonify({'response': response})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- The **Flask** app listens for POST requests at the `/chat` endpoint.\n",
        "- When a user sends a message, it processes the message using the trained chatbot and returns a response.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.3 Deploying on Cloud Platforms**\n",
        "- Deploying the chatbot to a cloud platform ensures scalability and accessibility, allowing users to interact with the chatbot over the internet.\n",
        "- Popular cloud platforms include **AWS (Amazon Web Services)**, **Google Cloud**, and **Microsoft Azure**.\n",
        "\n",
        "**Steps for Deploying on AWS**:\n",
        "1. **Create an EC2 Instance**: Set up a server that will host the chatbot.\n",
        "2. **Install Dependencies**: Install the necessary software (PyTorch, Flask, etc.) on the EC2 instance.\n",
        "3. **Deploy Flask App**: Run the Flask web app on the EC2 instance and expose the server to the public using a load balancer.\n",
        "4. **Use an API Gateway**: Set up an API gateway to route requests from users to your chatbot server.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.4 Using Pre-built Chatbot Platforms**\n",
        "- Pre-built platforms like **Dialogflow** (by Google) and **Microsoft Bot Framework** allow you to integrate your chatbot with minimal effort.\n",
        "- These platforms provide tools to build, test, and deploy chatbots on various platforms (e.g., websites, messaging apps like Facebook Messenger).\n",
        "\n",
        "**How to Use Pre-built Platforms**:\n",
        "1. **Build a Bot**: Use the platform’s tools to design conversation flows and integrate your trained model.\n",
        "2. **Deploy**: The platform automatically handles hosting, scaling, and making the chatbot available on various channels.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.5 Scalability and Performance Considerations**\n",
        "- **Scalability**: As the number of users increases, the chatbot needs to scale to handle more concurrent requests. This can be achieved through load balancing, auto-scaling groups, and serverless architectures.\n",
        "- **Latency**: To provide real-time responses, the chatbot should have minimal inference latency. Consider optimizing the model or running it on high-performance servers or GPUs.\n",
        "- **Caching**: Use caching mechanisms to store frequently used responses or preprocessed data, reducing the load on the model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.6 Observations from Current Research**\n",
        "- **Edge Deployment**: Recent advancements allow chatbots to be deployed on edge devices (e.g., mobile phones, IoT devices) by compressing models using techniques like quantization and pruning. This makes the chatbot accessible without needing cloud infrastructure.\n",
        "- **Real-Time Model Serving**: Tools like **TorchServe** (for PyTorch) and **TensorFlow Serving** streamline the process of deploying machine learning models at scale, offering real-time inference capabilities.\n",
        "- **Multimodal Chatbots**: Emerging chatbots use multiple input modes (text, voice, images) and are deployed using frameworks like **Rasa** and **Alexa Skills Kit** for more interactive experiences.\n",
        "\n",
        "---\n",
        "\n",
        "#### **10.7 Key Takeaways**\n",
        "- Saving and loading the trained model is essential for reuse, deployment, and fine-tuning.\n",
        "- A Flask web app provides a simple way to deploy the chatbot with real-time user interaction.\n",
        "- Cloud platforms offer scalable deployment solutions, making the chatbot accessible to a wide range of users.\n",
        "- Pre-built chatbot platforms and cloud-based deployment options reduce the complexity of deploying and managing chatbots in production environments.\n",
        "- Scalability, performance optimization, and real-time serving are key considerations when deploying chatbots to handle large-scale interactions.\n",
        "\n",
        "---\n",
        "\n",
        "This final section wraps up the process of building, training, and deploying your chatbot. You can now integrate it into real-world applications, making it accessible to users via web interfaces, cloud services, or even pre-built platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemQRC0rcOIA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjCPO3xwcOFK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwaId7xVcOAG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oafEVVq-cN5r"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}