{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNMfRc7ZcFW/EIeBYld6PGK",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L07-Chatbot%20Based%20on%20PyTorch/Chatbot_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the convokit library if not already installed\n",
    "# !pip install convokit  \n",
    "\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# Step 1: Download and load the Cornell Movie Dialogs Corpus\n",
    "# The Corpus class from convokit is used to handle conversational datasets\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eZhv-VpC7Uo",
    "outputId": "8e5ab09a-8489-459a-8281-5b33d37e0944",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:11:08.449297Z",
     "start_time": "2024-10-25T14:09:42.991501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to C:\\Users\\Girija\\.convokit\\downloads\\movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "# This function retrieves dialogues from conversations in the corpus\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []  # Initialize an empty list to store question-answer pairs\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        # Get a list of utterance IDs in the conversation\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        # Iterate through pairs of utterances (current and next) in the conversation\n",
    "        for i in range(len(utterances) - 1):  \n",
    "            # The current utterance is treated as the question\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            # The next utterance is treated as the answer\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            # Append the question-answer pair to the list\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs  # Return the list of question-answer pairs"
   ],
   "metadata": {
    "id": "9l0w-fr2C7RZ",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:11:24.092962Z",
     "start_time": "2024-10-25T14:11:24.082044Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "print(f\"Example pair: {qa_pairs[0]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzp8tot_C7N1",
    "outputId": "6833e023-a25f-4bc8-80f2-bbf80eb81b86",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:11:09.553349Z",
     "start_time": "2024-10-25T14:11:08.492293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 221616 question-answer pairs.\n",
      "Example pair: ['They do not!', 'They do to!']\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SNBOxYJIC203",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:11:23.896845Z",
     "start_time": "2024-10-25T14:11:09.580215Z"
    }
   },
   "source": [
    "# Step 4: Preprocessing - normalize the dataset\n",
    "# Function to convert unicode characters to ASCII\n",
    "def unicode_to_ascii(s):\n",
    "    # Normalize string to NFD form, which separates accents from characters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'  # Remove accents by excluding characters with 'Mn' (nonspacing mark) category\n",
    "    )\n",
    "\n",
    "# Function to normalize text by converting to lowercase, removing unwanted characters, and adding spaces around punctuation\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())  # Convert text to ASCII and lowercase\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)  # Separate punctuation from words with spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # Remove characters that aren't letters, punctuation, or spaces\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()  # Replace multiple spaces with a single space\n",
    "    return s  # Return the normalized string\n",
    "\n",
    "# Preprocess all question-answer pairs by normalizing each question and answer\n",
    "for i in range(len(qa_pairs)):\n",
    "    qa_pairs[i][0] = normalize_string(qa_pairs[i][0])  # Normalize question\n",
    "    qa_pairs[i][1] = normalize_string(qa_pairs[i][1])  # Normalize answer\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 5: Create a simple rule-based chatbot\n",
    "# This function finds the best answer for the user's input by checking each question-answer pair\n",
    "def chatbot_response(user_input, qa_pairs):\n",
    "    # Normalize the user input to match the format of the dataset\n",
    "    user_input = normalize_string(user_input)\n",
    "    # Default response if no match is found\n",
    "    response = \"I'm sorry, I don't understand. Can you rephrase?\"\n",
    "\n",
    "    # Loop through question-answer pairs to find a match for user_input\n",
    "    for pair in qa_pairs:\n",
    "        question, answer = pair  # Unpack the question and answer\n",
    "        if user_input in question:  # Check if user input matches any question\n",
    "            return answer  # Return the answer if a match is found\n",
    "\n",
    "    return response  # Return default response if no match is found"
   ],
   "metadata": {
    "id": "RVEB1n0nDEcr",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:11:23.918476Z",
     "start_time": "2024-10-25T14:11:23.908532Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Chat Simulation**"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 6: Function to run the chatbot with simulated inputs\n",
    "def run_chatbot(qa_pairs, user_inputs):\n",
    "    # Initialize conversation list to store messages\n",
    "    conversation = []\n",
    "    conversation.append(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    \n",
    "    # Process each user input in the list\n",
    "    for user_input in user_inputs:\n",
    "        conversation.append(f\"You: {user_input}\")\n",
    "        \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            conversation.append(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Get chatbot's response based on the user input\n",
    "        response = chatbot_response(user_input, qa_pairs)\n",
    "        conversation.append(f\"Chatbot: {response}\")\n",
    "    \n",
    "    # Print the entire conversation to simulate the chat\n",
    "    output_text = \"\\n\".join(conversation)\n",
    "    print(output_text)\n",
    "\n",
    "# List of simulated user inputs for testing based on typical conversational patterns\n",
    "user_inputs = [\n",
    "    \"What are you doing here?\",\n",
    "    \"Do you believe in love?\",\n",
    "    \"Tell me a secret.\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your best friend?\",\n",
    "    \"Do you ever feel lonely?\",\n",
    "    \"Why do people lie?\",\n",
    "    \"What makes you happy?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Why do people fall in love?\",\n",
    "    \"exit\"\n",
    "    ]\n",
    "\n",
    "# Run the chatbot with simulated user inputs\n",
    "run_chatbot(qa_pairs, user_inputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqHSo1VmDFIV",
    "outputId": "a57fca6d-a81d-4d6d-de97-bfea2cf6fec2",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:23:33.528275Z",
     "start_time": "2024-10-25T14:23:32.702473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: What are you doing here?\n",
      "Chatbot: excuse me have you seen the feminine mystique ? i lost my copy .\n",
      "You: Do you believe in love?\n",
      "Chatbot: no . it s not that .\n",
      "You: Tell me a secret.\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: What's your favorite movie?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: Who is your best friend?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: Do you ever feel lonely?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: Why do people lie?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: What makes you happy?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: Can you tell me a joke?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: Why do people fall in love?\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**User Input Way**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T14:21:31.613119Z",
     "start_time": "2024-10-25T14:21:13.996269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Function to run the chatbot interactively with live user input\n",
    "def run_chatbot(qa_pairs):\n",
    "    # Print initial greeting message from chatbot\n",
    "    print(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    \n",
    "    # Begin a loop to continuously take user input and respond\n",
    "    while True:\n",
    "        # Capture live user input\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        # Check if the user wants to exit the chat\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate chatbot response based on user input\n",
    "        response = chatbot_response(user_input, qa_pairs)\n",
    "        \n",
    "        # Display the response\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot interactively with qa_pairs\n",
    "run_chatbot(qa_pairs)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "Chatbot: be polite . say hello . this is candy .\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "Chatbot: end my career ?\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation:\n",
    "\n",
    "Based on the chat log you provided and your analysis request, it looks like the chatbot struggles to respond correctly to a variety of inputs. The root cause is likely the simplistic matching strategy used in the chatbot, which results in a high rate of \"I don't understand\" responses. Here's a breakdown of the issues and improvements you can implement:\n",
    "\n",
    "##### Issues with Current Implementation:\n",
    "\n",
    "1. **Exact Matching**:\n",
    "   - The chatbot attempts to match user input exactly or checks if the user input is a substring of a pre-existing question. This method fails to accommodate variations in phrasing or sentence structure.\n",
    "   \n",
    "2. **Handling User Input**:\n",
    "   - Questions like \"Do you like movies?\" and \"How are you?\" result in the chatbot defaulting to \"I don't understand.\" because the exact match or simple substring search does not find similar questions in the dataset.\n",
    "\n",
    "3. **Fallback Mechanism**:\n",
    "   - The fallback response \"Can you rephrase?\" appears too often, indicating that the similarity matching and logic behind selecting responses are not sophisticated enough.\n",
    "\n",
    "4. **Response Relevance**:\n",
    "   - Even when responses are provided, they are sometimes nonsensical (e.g., \"be polite. say hello . this is candy.\") because there is no contextual understanding involved.\n",
    "\n",
    "\n",
    "##### Improvements You Can Make:\n",
    "\n",
    "1. **Implementing a More Sophisticated Matching Algorithm**:\n",
    "   - Instead of simple substring matching, you can implement similarity algorithms like **Levenshtein distance**, **Cosine similarity** with **TF-IDF** (Term Frequency-Inverse Document Frequency), or **Word2Vec**/**BERT** embeddings to improve response relevance.\n",
    "\n",
    "2. **Threshold Tuning for Similarity**:\n",
    "   - Set a dynamic threshold for matching questions and adjust it based on user input. If the chatbot struggles to find a close enough match, it can request clarification from the user.\n",
    "\n",
    "3. **Response Diversification**:\n",
    "   - Instead of repeating the same fallback responses, add more varied, contextually relevant responses when the bot cannot understand the user's question.\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "   - Enrich your dataset by augmenting it with more possible variations of common questions and answers. This will help the bot become more adaptable to different ways of phrasing the same question.\n"
   ],
   "metadata": {
    "id": "Xi0kHxgGELRK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Version 2\n"
   ],
   "metadata": {
    "id": "4WdEKs5HEVEv"
   }
  },
  {
   "cell_type": "code",
   "source": "#!pip install convokit scikit-learn  # Install the necessary libraries -- for google colab\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_D1BHwIEdXG",
    "outputId": "dba0721b-98fa-4d18-d898-ac100aff0222",
    "ExecuteTime": {
     "end_time": "2024-10-25T13:55:01.398816Z",
     "start_time": "2024-10-25T13:55:01.394897Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        for i in range(len(utterances) - 1):  # Iterate through the conversation\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs\n"
   ],
   "metadata": {
    "id": "jRhM-6zRDJBT",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:23:57.860931Z",
     "start_time": "2024-10-25T14:23:57.848717Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "\n",
    "# Step 4: Preprocessing - normalize the dataset\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Preprocess the QA pairs\n",
    "questions = [normalize_string(pair[0]) for pair in qa_pairs]\n",
    "answers = [normalize_string(pair[1]) for pair in qa_pairs]\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lK2a7dwEeJo",
    "outputId": "32000105-4567-4a1a-fa22-cf7a8e88a939",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:24:16.860798Z",
     "start_time": "2024-10-25T14:24:00.306955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 221616 question-answer pairs.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 5: Initialize TF-IDF vectorizer and fit it on the questions\n",
    "vectorizer = TfidfVectorizer().fit(questions)\n",
    "\n",
    "# Step 6: Function to find the best response using cosine similarity\n",
    "def chatbot_response(user_input, vectorizer, questions, answers):\n",
    "    user_input = normalize_string(user_input)  # Normalize the user input\n",
    "    user_vec = vectorizer.transform([user_input])  # Convert input to TF-IDF vector\n",
    "    question_vecs = vectorizer.transform(questions)  # Convert all questions to TF-IDF vectors\n",
    "\n",
    "    # Compute cosine similarity between user input and all questions in the dataset\n",
    "    similarities = cosine_similarity(user_vec, question_vecs).flatten()\n",
    "\n",
    "    # Find the index of the question with the highest similarity\n",
    "    best_match_index = similarities.argmax()\n",
    "\n",
    "    # If the best match similarity score is too low, provide a fallback response\n",
    "    if similarities[best_match_index] < 0.5:  # Threshold can be tuned\n",
    "        return random.choice([\n",
    "            \"I'm sorry, I don't understand. Can you rephrase?\",\n",
    "            \"Could you clarify your question?\",\n",
    "            \"I don't have an answer for that, sorry.\",\n",
    "            \"Can you try asking that differently?\"\n",
    "        ])\n",
    "\n",
    "    return answers[best_match_index]  # Return the best matched answer\n"
   ],
   "metadata": {
    "id": "o_-6q6g7EkAA",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:24:21.560313Z",
     "start_time": "2024-10-25T14:24:16.870787Z"
    }
   },
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 7: Function to run the chatbot interactively with user inputs shown\n",
    "def run_chatbot(vectorizer, questions, answers, user_inputs):\n",
    "    # Initialize conversation log to store all user and chatbot messages\n",
    "    conversation = []\n",
    "    conversation.append(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    \n",
    "    # Process each simulated user input\n",
    "    for user_input in user_inputs:\n",
    "        conversation.append(f\"You: {user_input}\")\n",
    "        \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            conversation.append(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate chatbot response\n",
    "        response = chatbot_response(user_input, vectorizer, questions, answers)\n",
    "        conversation.append(f\"Chatbot: {response}\")\n",
    "    \n",
    "    # Display the entire conversation at once\n",
    "    output_text = \"\\n\".join(conversation)\n",
    "    print(output_text)\n",
    "\n",
    "\n",
    "# List of simulated user inputs for testing based on typical conversational patterns\n",
    "user_inputs = [\n",
    "    \"What are you doing here?\",\n",
    "    \"Do you believe in love?\",\n",
    "    \"Tell me a secret.\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your best friend?\",\n",
    "    \"Do you ever feel lonely?\",\n",
    "    \"Why do people lie?\",\n",
    "    \"What makes you happy?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Why do people fall in love?\",\n",
    "    \"exit\"\n",
    "    ]\n",
    "\n",
    "# Run the chatbot with the vectorizer, questions, answers, and user inputs\n",
    "run_chatbot(vectorizer, questions, answers, user_inputs)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhC7H3n1ElMP",
    "outputId": "418cc531-7219-4903-aa8b-b4ae6a1bf22f",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:25:17.598612Z",
     "start_time": "2024-10-25T14:24:47.490862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: What are you doing here?\n",
      "Chatbot: excuse me have you seen the feminine mystique ? i lost my copy .\n",
      "You: Do you believe in love?\n",
      "Chatbot: no . it s not that .\n",
      "You: Tell me a secret.\n",
      "Chatbot: mm hm .\n",
      "You: What's your favorite movie?\n",
      "Chatbot: i don t remember . but off the top of my head i d say black .\n",
      "You: Who is your best friend?\n",
      "Chatbot: i was her best friend .\n",
      "You: Do you ever feel lonely?\n",
      "Chatbot: what ?\n",
      "You: Why do people lie?\n",
      "Chatbot: i lied . to her . she thought she d seen you .\n",
      "You: What makes you happy?\n",
      "Chatbot: i hope you like large weddings .\n",
      "You: Can you tell me a joke?\n",
      "Chatbot: druid hill park .\n",
      "You: Why do people fall in love?\n",
      "Chatbot: so what do i do ?\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chatbot's responses in the latest interaction don't seem coherent in the context of the questions being asked. The model selects responses that are movie dialogues, but they don't align well with the questions. This indicates a gap between matching user input and relevant responses.\n",
    "\n",
    "##### Issues in the Current Chatbot Implementation:\n",
    "1. **Irrelevant Responses**:\n",
    "   - The responses don't seem to fit the questions well (e.g., \"we have a visitor\" and \"i need half a million to buy a script\"). These responses, while part of movie dialogues, are not suitable answers to the questions being asked.\n",
    "   \n",
    "2. **No Contextual Understanding**:\n",
    "   - The current approach does not take context into account and selects responses based purely on surface-level similarity, which is insufficient for meaningful conversations.\n",
    "\n",
    "3. **Randomness of Responses**:\n",
    "   - Since the chatbot uses pre-defined dialogue exchanges, the selected answers may not always make sense. This is a fundamental limitation of using raw movie dialogues without additional processing.\n",
    "\n",
    "---\n",
    "\n",
    "##### Improvements to Make:\n",
    "1. **Contextual Filtering**:\n",
    "   - Apply **semantic similarity** methods using pre-trained models like **BERT** to better understand the context and meaning of both the user's input and the movie dialogues.\n",
    "\n",
    "2. **Dynamic Response Selection**:\n",
    "   - To avoid irrelevant responses, you could filter potential answers based on content type or add rules to ensure answers fit specific question categories (e.g., responses about movies, actions, or greetings).\n",
    "   \n",
    "3. **Better Preprocessing**:\n",
    "   - Improve normalization of both questions and answers by handling a wider range of characters, punctuation, and special symbols.\n",
    "\n",
    "4. **Handling Conversational Context**:\n",
    "   - Implement basic contextual memory to track ongoing conversation topics. For example, after a question about movies, subsequent responses should remain within the movie-related context.\n",
    "   \n",
    "5. **Threshold Tuning**:\n",
    "   - The threshold for similarity (currently set at 0.5) could be tuned more dynamically. Higher thresholds could help in avoiding irrelevant answers, though it might increase the fallback responses.\n",
    "\n",
    "---\n",
    "\n",
    "##### Revised Code with Semantic Similarity (Using Sentence Transformers/BERT):\n",
    "To significantly improve the chatbot's ability to provide relevant answers, let's incorporate **sentence embeddings** via **BERT-based models** from the `sentence-transformers` library.\n"
   ],
   "metadata": {
    "id": "1346HbAAFMNP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#!pip install sentence-transformers convokit  # Install required libraries\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from convokit import Corpus, download\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "# Step 1: Download and load the Cornell Movie Dialogs Corpus\n",
    "#corpus = Corpus(filename=download(\"movie-corpus\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8Jb7R6LElh7",
    "outputId": "4e06f70a-4689-4432-99ac-e86f9457fc3d",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:25:47.204435Z",
     "start_time": "2024-10-25T14:25:43.646994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        for i in range(len(utterances) - 1):  # Iterate through the conversation\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs\n",
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "\n",
    "# Step 4: Preprocessing - normalize the dataset\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Preprocess the QA pairs\n",
    "questions = [normalize_string(pair[0]) for pair in qa_pairs]\n",
    "answers = [normalize_string(pair[1]) for pair in qa_pairs]\n",
    "\n",
    "# Step 5: Load a pre-trained sentence-transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight BERT model\n",
    "\n",
    "# Step 6: Encode all the questions in the dataset using the BERT model\n",
    "encoded_questions = model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# Step 7: Function to find the best response using semantic similarity\n",
    "def chatbot_response(user_input, model, encoded_questions, questions, answers):\n",
    "    user_input = normalize_string(user_input)  # Normalize the user input\n",
    "    user_embedding = model.encode(user_input, convert_to_tensor=True)  # Encode user input using BERT\n",
    "\n",
    "    # Compute cosine similarity between user input and all questions in the dataset\n",
    "    similarities = util.pytorch_cos_sim(user_embedding, encoded_questions).flatten()\n",
    "\n",
    "    # Find the index of the question with the highest similarity\n",
    "    best_match_index = similarities.argmax()\n",
    "\n",
    "    # If the best match similarity score is too low, provide a fallback response\n",
    "    if similarities[best_match_index] < 0.7:  # Adjust the threshold as needed\n",
    "        return random.choice([\n",
    "            \"I'm sorry, I don't understand. Can you rephrase?\",\n",
    "            \"Could you clarify your question?\",\n",
    "            \"I don't have an answer for that, sorry.\",\n",
    "            \"Can you try asking that differently?\"\n",
    "        ])\n",
    "\n",
    "    return answers[best_match_index]  # Return the best matched answer\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDh8kiVlG5Z6",
    "outputId": "726031c5-d4e1-4e7d-bc07-da288f14bb09",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:41:01.679407Z",
     "start_time": "2024-10-25T14:25:57.467284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 221616 question-answer pairs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d9669a04474491095f4d7f111009610"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Girija\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e863233070d43448c71729ea3266974"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f7e48832a114c0fa48ccb956f7ae97d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "830d6b62551444dba1ef264d398b3a12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbf80a276d4c4634aae032eb700928a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3133d1d82dd4e57aec3a99e62c01749"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "004187c3b93c41e7b59a2684ed73cc35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c424c18ce9af41bd9d80faee85630bd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bb17285837c44188af5091c19285a47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ae46b859cda41d985cd8e8600af9824"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10ad3d98d19f4310b3976b6a03b1b5e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 8: Function to run the chatbot interactively with simulated input\n",
    "def run_chatbot(model, encoded_questions, questions, answers, user_inputs):\n",
    "    # Initialize conversation list to store each interaction\n",
    "    conversation = []\n",
    "    conversation.append(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    \n",
    "    # Process each user input in the provided list of simulated inputs\n",
    "    for user_input in user_inputs:\n",
    "        conversation.append(f\"You: {user_input}\")\n",
    "        \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            conversation.append(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate the chatbot's response based on the simulated user input\n",
    "        response = chatbot_response(user_input, model, encoded_questions, questions, answers)\n",
    "        conversation.append(f\"Chatbot: {response}\")\n",
    "    \n",
    "    # Print the entire conversation to simulate the chat\n",
    "    output_text = \"\\n\".join(conversation)\n",
    "    print(output_text)\n",
    "\n",
    "# List of simulated user inputs for testing based on typical conversational patterns\n",
    "user_inputs = [\n",
    "    \"What are you doing here?\",\n",
    "    \"Do you believe in love?\",\n",
    "    \"Tell me a secret.\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your best friend?\",\n",
    "    \"Do you ever feel lonely?\",\n",
    "    \"Why do people lie?\",\n",
    "    \"What makes you happy?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Why do people fall in love?\",\n",
    "    \"exit\"\n",
    "    ]\n",
    "\n",
    "# Run the chatbot with simulated user inputs\n",
    "run_chatbot(model, encoded_questions, questions, answers, user_inputs)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBZ4y8gkG7c3",
    "outputId": "d932b555-ff54-4962-bd5e-f2ec74762576",
    "ExecuteTime": {
     "end_time": "2024-10-25T14:41:51.865333Z",
     "start_time": "2024-10-25T14:41:50.294858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: What are you doing here?\n",
      "Chatbot: excuse me have you seen the feminine mystique ? i lost my copy .\n",
      "You: Do you believe in love?\n",
      "Chatbot: no . it s not that .\n",
      "You: Tell me a secret.\n",
      "Chatbot: mm hm .\n",
      "You: What's your favorite movie?\n",
      "Chatbot: what do you care ? let em have their fun . so what s up ?\n",
      "You: Who is your best friend?\n",
      "Chatbot: was jack goodman your good friend ?\n",
      "You: Do you ever feel lonely?\n",
      "Chatbot: now i make sure that no one has the opportunity to test me .\n",
      "You: Why do people lie?\n",
      "Chatbot: how did we get here ?\n",
      "You: What makes you happy?\n",
      "Chatbot: . . . honest . at least you re honest with me .\n",
      "You: Can you tell me a joke?\n",
      "Chatbot: i m so tired i m about to drive off the road . keep me awake willya ?\n",
      "You: Why do people fall in love?\n",
      "Chatbot: give what up ?\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Improvements in This Version:\n",
    "\n",
    "1. **BERT for Semantic Matching**:\n",
    "   - We now use a **BERT-based model** (`sentence-transformers`) to generate sentence embeddings for both the user input and the dataset questions. This allows the chatbot to understand the meaning and context of the questions and respond with more relevant answers.\n",
    "   \n",
    "2. **Increased Threshold for Response Matching**:\n",
    "   - The similarity threshold has been increased to 0.7. This ensures the chatbot provides a fallback response when there isn't a close match, reducing irrelevant answers.\n",
    "\n",
    "3. **Fallback Responses**:\n",
    "   - More diverse fallback responses are used when the chatbot cannot find a good match, making the conversation feel more dynamic.\n",
    "\n",
    "##### Expected Benefits:\n",
    "- **More Relevant Responses**: Using **semantic similarity** allows the chatbot to find responses that are contextually appropriate rather than simply relying on surface-level similarity.\n",
    "- **Handling a Variety of Input**: The chatbot will be able to handle various ways of phrasing a question, providing more meaningful interactions.\n",
    "- **Fewer Irrelevant Replies**: The increased similarity threshold ensures that the chatbot doesn’t provide answers that don’t fit the user's input.\n"
   ],
   "metadata": {
    "id": "t7TM_pefFWX2"
   }
  }
 ]
}
