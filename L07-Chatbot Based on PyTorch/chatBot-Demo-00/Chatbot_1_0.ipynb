{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNMfRc7ZcFW/EIeBYld6PGK",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/L07-Chatbot%20Based%20on%20PyTorch/Chatbot_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install convokit  # Install the convokit library\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# Step 1: Download and load the Cornell Movie Dialogs Corpus\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eZhv-VpC7Uo",
    "outputId": "8e5ab09a-8489-459a-8281-5b33d37e0944",
    "ExecuteTime": {
     "end_time": "2024-10-25T13:48:46.596824Z",
     "start_time": "2024-10-25T13:48:46.543281Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'convokit'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01municodedata\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconvokit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Corpus, download\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Step 1: Download and load the Cornell Movie Dialogs Corpus\u001B[39;00m\n\u001B[0;32m      8\u001B[0m corpus \u001B[38;5;241m=\u001B[39m Corpus(filename\u001B[38;5;241m=\u001B[39mdownload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmovie-corpus\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'convokit'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        for i in range(len(utterances) - 1):  # Iterate through the conversation\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs\n"
   ],
   "metadata": {
    "id": "9l0w-fr2C7RZ",
    "ExecuteTime": {
     "end_time": "2024-10-25T13:48:48.018638Z",
     "start_time": "2024-10-25T13:48:48.006924Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "print(f\"Example pair: {qa_pairs[0]}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzp8tot_C7N1",
    "outputId": "6833e023-a25f-4bc8-80f2-bbf80eb81b86"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracted 221616 question-answer pairs.\n",
      "Example pair: ['They do not!', 'They do to!']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SNBOxYJIC203"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Preprocessing - normalize the dataset\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Function to normalize the input text\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Preprocess the QA pairs\n",
    "for i in range(len(qa_pairs)):\n",
    "    qa_pairs[i][0] = normalize_string(qa_pairs[i][0])  # Normalize question\n",
    "    qa_pairs[i][1] = normalize_string(qa_pairs[i][1])  # Normalize answer\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 5: Create a simple rule-based chatbot\n",
    "def chatbot_response(user_input, qa_pairs):\n",
    "    user_input = normalize_string(user_input)\n",
    "    response = \"I'm sorry, I don't understand. Can you rephrase?\"  # Default response\n",
    "\n",
    "    for pair in qa_pairs:\n",
    "        question, answer = pair\n",
    "        if user_input in question:\n",
    "            return answer  # Return the most suitable answer\n",
    "\n",
    "    return response\n",
    "\n"
   ],
   "metadata": {
    "id": "RVEB1n0nDEcr"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 6: Function to run the chatbot interactively\n",
    "def run_chatbot(qa_pairs):\n",
    "    print(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        response = chatbot_response(user_input, qa_pairs)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot\n",
    "run_chatbot(qa_pairs)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqHSo1VmDFIV",
    "outputId": "a57fca6d-a81d-4d6d-de97-bfea2cf6fec2"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: hello\n",
      "Chatbot: be polite . say hello . this is candy .\n",
      "You: hello this is baby\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: how are you?\n",
      "Chatbot: is there a problem ?\n",
      "You: no, just asking\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: do you like movies\n",
      "Chatbot: people watched the movies in their cars ?\n",
      "You: which movie would you prefer me to watch\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the chat log you provided and your analysis request, it looks like the chatbot struggles to respond correctly to a variety of inputs. The root cause is likely the simplistic matching strategy used in the chatbot, which results in a high rate of \"I don't understand\" responses. Here's a breakdown of the issues and improvements you can implement:\n",
    "\n",
    "##### Issues with Current Implementation:\n",
    "\n",
    "1. **Exact Matching**:\n",
    "   - The chatbot attempts to match user input exactly or checks if the user input is a substring of a pre-existing question. This method fails to accommodate variations in phrasing or sentence structure.\n",
    "   \n",
    "2. **Handling User Input**:\n",
    "   - Questions like \"Do you like movies?\" and \"How are you?\" result in the chatbot defaulting to \"I don't understand.\" because the exact match or simple substring search does not find similar questions in the dataset.\n",
    "\n",
    "3. **Fallback Mechanism**:\n",
    "   - The fallback response \"Can you rephrase?\" appears too often, indicating that the similarity matching and logic behind selecting responses are not sophisticated enough.\n",
    "\n",
    "4. **Response Relevance**:\n",
    "   - Even when responses are provided, they are sometimes nonsensical (e.g., \"be polite. say hello . this is candy.\") because there is no contextual understanding involved.\n",
    "\n",
    "\n",
    "##### Improvements You Can Make:\n",
    "\n",
    "1. **Implementing a More Sophisticated Matching Algorithm**:\n",
    "   - Instead of simple substring matching, you can implement similarity algorithms like **Levenshtein distance**, **Cosine similarity** with **TF-IDF** (Term Frequency-Inverse Document Frequency), or **Word2Vec**/**BERT** embeddings to improve response relevance.\n",
    "\n",
    "2. **Threshold Tuning for Similarity**:\n",
    "   - Set a dynamic threshold for matching questions and adjust it based on user input. If the chatbot struggles to find a close enough match, it can request clarification from the user.\n",
    "\n",
    "3. **Response Diversification**:\n",
    "   - Instead of repeating the same fallback responses, add more varied, contextually relevant responses when the bot cannot understand the user's question.\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "   - Enrich your dataset by augmenting it with more possible variations of common questions and answers. This will help the bot become more adaptable to different ways of phrasing the same question.\n"
   ],
   "metadata": {
    "id": "Xi0kHxgGELRK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Version 2\n"
   ],
   "metadata": {
    "id": "4WdEKs5HEVEv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install convokit scikit-learn  # Install the necessary libraries\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_D1BHwIEdXG",
    "outputId": "dba0721b-98fa-4d18-d898-ac100aff0222"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: convokit in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.2.2)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.13.1)\n",
      "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.8)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.5)\n",
      "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (6.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2024.9.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.4.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.9.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        for i in range(len(utterances) - 1):  # Iterate through the conversation\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs\n"
   ],
   "metadata": {
    "id": "jRhM-6zRDJBT"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "\n",
    "# Step 4: Preprocessing - normalize the dataset\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Preprocess the QA pairs\n",
    "questions = [normalize_string(pair[0]) for pair in qa_pairs]\n",
    "answers = [normalize_string(pair[1]) for pair in qa_pairs]\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lK2a7dwEeJo",
    "outputId": "32000105-4567-4a1a-fa22-cf7a8e88a939"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracted 221616 question-answer pairs.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 5: Initialize TF-IDF vectorizer and fit it on the questions\n",
    "vectorizer = TfidfVectorizer().fit(questions)\n",
    "\n",
    "# Step 6: Function to find the best response using cosine similarity\n",
    "def chatbot_response(user_input, vectorizer, questions, answers):\n",
    "    user_input = normalize_string(user_input)  # Normalize the user input\n",
    "    user_vec = vectorizer.transform([user_input])  # Convert input to TF-IDF vector\n",
    "    question_vecs = vectorizer.transform(questions)  # Convert all questions to TF-IDF vectors\n",
    "\n",
    "    # Compute cosine similarity between user input and all questions in the dataset\n",
    "    similarities = cosine_similarity(user_vec, question_vecs).flatten()\n",
    "\n",
    "    # Find the index of the question with the highest similarity\n",
    "    best_match_index = similarities.argmax()\n",
    "\n",
    "    # If the best match similarity score is too low, provide a fallback response\n",
    "    if similarities[best_match_index] < 0.5:  # Threshold can be tuned\n",
    "        return random.choice([\n",
    "            \"I'm sorry, I don't understand. Can you rephrase?\",\n",
    "            \"Could you clarify your question?\",\n",
    "            \"I don't have an answer for that, sorry.\",\n",
    "            \"Can you try asking that differently?\"\n",
    "        ])\n",
    "\n",
    "    return answers[best_match_index]  # Return the best matched answer\n"
   ],
   "metadata": {
    "id": "o_-6q6g7EkAA"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 7: Function to run the chatbot interactively\n",
    "def run_chatbot(vectorizer, questions, answers):\n",
    "    print(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        response = chatbot_response(user_input, vectorizer, questions, answers)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot\n",
    "run_chatbot(vectorizer, questions, answers)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhC7H3n1ElMP",
    "outputId": "418cc531-7219-4903-aa8b-b4ae6a1bf22f"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: hello\n",
      "Chatbot: we have a visitor .\n",
      "You: how are you?\n",
      "Chatbot: sun city . i ve been meaning to call you for months .\n",
      "You: can you recomment a movie for me\n",
      "Chatbot: because i need half a million to buy a script .\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chatbot's responses in the latest interaction don't seem coherent in the context of the questions being asked. The model selects responses that are movie dialogues, but they don't align well with the questions. This indicates a gap between matching user input and relevant responses.\n",
    "\n",
    "##### Issues in the Current Chatbot Implementation:\n",
    "1. **Irrelevant Responses**:\n",
    "   - The responses don't seem to fit the questions well (e.g., \"we have a visitor\" and \"i need half a million to buy a script\"). These responses, while part of movie dialogues, are not suitable answers to the questions being asked.\n",
    "   \n",
    "2. **No Contextual Understanding**:\n",
    "   - The current approach does not take context into account and selects responses based purely on surface-level similarity, which is insufficient for meaningful conversations.\n",
    "\n",
    "3. **Randomness of Responses**:\n",
    "   - Since the chatbot uses pre-defined dialogue exchanges, the selected answers may not always make sense. This is a fundamental limitation of using raw movie dialogues without additional processing.\n",
    "\n",
    "---\n",
    "\n",
    "##### Improvements to Make:\n",
    "1. **Contextual Filtering**:\n",
    "   - Apply **semantic similarity** methods using pre-trained models like **BERT** to better understand the context and meaning of both the user's input and the movie dialogues.\n",
    "\n",
    "2. **Dynamic Response Selection**:\n",
    "   - To avoid irrelevant responses, you could filter potential answers based on content type or add rules to ensure answers fit specific question categories (e.g., responses about movies, actions, or greetings).\n",
    "   \n",
    "3. **Better Preprocessing**:\n",
    "   - Improve normalization of both questions and answers by handling a wider range of characters, punctuation, and special symbols.\n",
    "\n",
    "4. **Handling Conversational Context**:\n",
    "   - Implement basic contextual memory to track ongoing conversation topics. For example, after a question about movies, subsequent responses should remain within the movie-related context.\n",
    "   \n",
    "5. **Threshold Tuning**:\n",
    "   - The threshold for similarity (currently set at 0.5) could be tuned more dynamically. Higher thresholds could help in avoiding irrelevant answers, though it might increase the fallback responses.\n",
    "\n",
    "---\n",
    "\n",
    "##### Revised Code with Semantic Similarity (Using Sentence Transformers/BERT):\n",
    "To significantly improve the chatbot's ability to provide relevant answers, let's incorporate **sentence embeddings** via **BERT-based models** from the `sentence-transformers` library.\n"
   ],
   "metadata": {
    "id": "1346HbAAFMNP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install sentence-transformers convokit  # Install required libraries\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from convokit import Corpus, download\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "# Step 1: Download and load the Cornell Movie Dialogs Corpus\n",
    "#corpus = Corpus(filename=download(\"movie-corpus\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8Jb7R6LElh7",
    "outputId": "4e06f70a-4689-4432-99ac-e86f9457fc3d"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
      "Requirement already satisfied: convokit in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.2.2)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.5)\n",
      "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.8)\n",
      "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from clean-text>=0.6.0->convokit) (6.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.26.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2024.9.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.9.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 2: Function to extract conversation pairs (question-answer pairs)\n",
    "def extract_sentence_pairs(corpus):\n",
    "    qa_pairs = []\n",
    "    for conversation in corpus.iter_conversations():\n",
    "        utterances = conversation.get_utterance_ids()\n",
    "        for i in range(len(utterances) - 1):  # Iterate through the conversation\n",
    "            input_sentence = corpus.get_utterance(utterances[i]).text\n",
    "            output_sentence = corpus.get_utterance(utterances[i + 1]).text\n",
    "            qa_pairs.append([input_sentence, output_sentence])\n",
    "    return qa_pairs\n",
    "\n",
    "# Step 3: Extract sentence pairs from the dataset\n",
    "qa_pairs = extract_sentence_pairs(corpus)\n",
    "print(f\"Extracted {len(qa_pairs)} question-answer pairs.\")\n",
    "\n",
    "# Step 4: Preprocessing - normalize the dataset\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Preprocess the QA pairs\n",
    "questions = [normalize_string(pair[0]) for pair in qa_pairs]\n",
    "answers = [normalize_string(pair[1]) for pair in qa_pairs]\n",
    "\n",
    "# Step 5: Load a pre-trained sentence-transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight BERT model\n",
    "\n",
    "# Step 6: Encode all the questions in the dataset using the BERT model\n",
    "encoded_questions = model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# Step 7: Function to find the best response using semantic similarity\n",
    "def chatbot_response(user_input, model, encoded_questions, questions, answers):\n",
    "    user_input = normalize_string(user_input)  # Normalize the user input\n",
    "    user_embedding = model.encode(user_input, convert_to_tensor=True)  # Encode user input using BERT\n",
    "\n",
    "    # Compute cosine similarity between user input and all questions in the dataset\n",
    "    similarities = util.pytorch_cos_sim(user_embedding, encoded_questions).flatten()\n",
    "\n",
    "    # Find the index of the question with the highest similarity\n",
    "    best_match_index = similarities.argmax()\n",
    "\n",
    "    # If the best match similarity score is too low, provide a fallback response\n",
    "    if similarities[best_match_index] < 0.7:  # Adjust the threshold as needed\n",
    "        return random.choice([\n",
    "            \"I'm sorry, I don't understand. Can you rephrase?\",\n",
    "            \"Could you clarify your question?\",\n",
    "            \"I don't have an answer for that, sorry.\",\n",
    "            \"Can you try asking that differently?\"\n",
    "        ])\n",
    "\n",
    "    return answers[best_match_index]  # Return the best matched answer\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDh8kiVlG5Z6",
    "outputId": "726031c5-d4e1-4e7d-bc07-da288f14bb09"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracted 221616 question-answer pairs.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 8: Function to run the chatbot interactively\n",
    "def run_chatbot(model, encoded_questions, questions, answers):\n",
    "    print(\"Chatbot: Hello! Ask me anything (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        response = chatbot_response(user_input, model, encoded_questions, questions, answers)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot\n",
    "run_chatbot(model, encoded_questions, questions, answers)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBZ4y8gkG7c3",
    "outputId": "d932b555-ff54-4962-bd5e-f2ec74762576"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chatbot: Hello! Ask me anything (type 'exit' to quit).\n",
      "You: hello\n",
      "Chatbot: hi patrick . i thought that was you .\n",
      "You: this is babu\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: how are you?\n",
      "Chatbot: sun city . i ve been meaning to call you for months .\n",
      "You: can you have any recommendation for the movies\n",
      "Chatbot: Can you try asking that differently?\n",
      "You: what is chat\n",
      "Chatbot: i have a few people here i can t really chat right now .\n",
      "You: do you have new people\n",
      "Chatbot: I'm sorry, I don't understand. Can you rephrase?\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Improvements in This Version:\n",
    "\n",
    "1. **BERT for Semantic Matching**:\n",
    "   - We now use a **BERT-based model** (`sentence-transformers`) to generate sentence embeddings for both the user input and the dataset questions. This allows the chatbot to understand the meaning and context of the questions and respond with more relevant answers.\n",
    "   \n",
    "2. **Increased Threshold for Response Matching**:\n",
    "   - The similarity threshold has been increased to 0.7. This ensures the chatbot provides a fallback response when there isn't a close match, reducing irrelevant answers.\n",
    "\n",
    "3. **Fallback Responses**:\n",
    "   - More diverse fallback responses are used when the chatbot cannot find a good match, making the conversation feel more dynamic.\n",
    "\n",
    "##### Expected Benefits:\n",
    "- **More Relevant Responses**: Using **semantic similarity** allows the chatbot to find responses that are contextually appropriate rather than simply relying on surface-level similarity.\n",
    "- **Handling a Variety of Input**: The chatbot will be able to handle various ways of phrasing a question, providing more meaningful interactions.\n",
    "- **Fewer Irrelevant Replies**: The increased similarity threshold ensures that the chatbot doesn’t provide answers that don’t fit the user's input.\n"
   ],
   "metadata": {
    "id": "t7TM_pefFWX2"
   }
  }
 ]
}
