{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw20VMj4RgWtYMCsTm6+7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/Supporting%20Works/MCQs/MCQ_Set_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNBmY_GGKTYH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lecture 2 (Basics of PyTorch and RNN Setup)\n",
        "\n",
        "1. **What is PyTorch primarily used for?**  \n",
        "   a) Web development  \n",
        "   b) Deep learning and tensor operations  \n",
        "   c) Database management  \n",
        "   d) Image rendering  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* PyTorch is widely used for deep learning and tensor operations.\n",
        "\n",
        "2. **What is a tensor in PyTorch?**  \n",
        "   a) A type of layer in a neural network  \n",
        "   b) A multi-dimensional array used for numerical computation  \n",
        "   c) A loss function  \n",
        "   d) An optimizer  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* A tensor is a multi-dimensional array that serves as the primary data structure in PyTorch.\n",
        "\n",
        "3. **What is the primary difference between `torch.FloatTensor` and `torch.IntTensor`?**  \n",
        "   a) `FloatTensor` stores integer values, and `IntTensor` stores float values  \n",
        "   b) `FloatTensor` stores floating-point numbers, and `IntTensor` stores integers  \n",
        "   c) `FloatTensor` is faster than `IntTensor`  \n",
        "   d) There is no difference  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `FloatTensor` is used for floating-point numbers, and `IntTensor` is used for integers.\n",
        "\n",
        "4. **What does `torch.zeros()` do in PyTorch?**  \n",
        "   a) Creates a tensor filled with ones  \n",
        "   b) Creates a tensor filled with zeros  \n",
        "   c) Creates a tensor with random values  \n",
        "   d) Creates a tensor based on a normal distribution  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `torch.zeros()` returns a tensor filled with zeros.\n",
        "\n",
        "5. **What is the use of `torch.nn` module in PyTorch?**  \n",
        "   a) For image processing  \n",
        "   b) For defining neural network layers and loss functions  \n",
        "   c) For matrix multiplications  \n",
        "   d) For debugging code  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The `torch.nn` module provides neural network layers, loss functions, and optimizers.\n",
        "\n",
        "---\n",
        "\n",
        "### Lecture 3 (Character-Level RNN)\n",
        "\n",
        "6. **What task does a character-level RNN perform in the context of lecture 3?**  \n",
        "   a) Image classification  \n",
        "   b) Predicting the next word in a sentence  \n",
        "   c) Classifying names by their language of origin  \n",
        "   d) Translating text between languages  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* The character-level RNN in this lecture is used to classify names by their language of origin.\n",
        "\n",
        "7. **How are names represented for input in a character-level RNN?**  \n",
        "   a) As entire sentences  \n",
        "   b) As one-hot encoded vectors of characters  \n",
        "   c) As raw text  \n",
        "   d) As images of the names  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* Each character in a name is one-hot encoded and fed into the RNN sequentially.\n",
        "\n",
        "8. **What does each step of the RNN update in the character-level model?**  \n",
        "   a) The final output  \n",
        "   b) The hidden state  \n",
        "   c) The input tensor  \n",
        "   d) The loss function  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* At each step, the RNN updates its hidden state based on the input character and the previous hidden state.\n",
        "\n",
        "9. **What activation function is used to compute the hidden state in the RNN model?**  \n",
        "   a) Sigmoid  \n",
        "   b) ReLU  \n",
        "   c) Tanh  \n",
        "   d) Softmax  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* The `tanh` activation function is commonly used to compute the hidden state in RNNs.\n",
        "\n",
        "10. **What is the final layer in the RNN model for name classification?**  \n",
        "    a) ReLU  \n",
        "    b) Sigmoid  \n",
        "    c) Softmax  \n",
        "    d) LogSoftmax  \n",
        "    **Answer:** d  \n",
        "    *Explanation:* `LogSoftmax` is used to compute the probability distribution over the language classes.\n",
        "\n",
        "---\n",
        "\n",
        "### Lecture 4 (Training the RNN)\n",
        "\n",
        "11. **What loss function is used in the RNN for name classification?**  \n",
        "    a) Cross-Entropy Loss  \n",
        "    b) Mean Squared Error  \n",
        "    c) Negative Log-Likelihood Loss (NLLLoss)  \n",
        "    d) Hinge Loss  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* Negative Log-Likelihood Loss (NLLLoss) is appropriate for classification tasks with a `LogSoftmax` output layer.\n",
        "\n",
        "12. **What is the purpose of `rnn.zero_grad()` in the training loop?**  \n",
        "    a) To update the weights  \n",
        "    b) To initialize the hidden state  \n",
        "    c) To reset the gradients of all model parameters  \n",
        "    d) To save the model  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* `rnn.zero_grad()` resets the gradients before computing the backpropagation to avoid accumulating them across iterations.\n",
        "\n",
        "13. **What does the `loss.backward()` function do in the training loop?**  \n",
        "    a) Computes the forward pass  \n",
        "    b) Updates the model parameters  \n",
        "    c) Computes the gradients of the loss with respect to the model parameters  \n",
        "    d) Saves the loss value  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* `loss.backward()` computes the gradients of the loss with respect to the model's parameters for backpropagation.\n",
        "\n",
        "14. **What does the optimizer's `step()` function do?**  \n",
        "    a) Computes the loss  \n",
        "    b) Updates the model parameters based on the gradients  \n",
        "    c) Resets the gradients  \n",
        "    d) Shuffles the data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The optimizerâ€™s `step()` function updates the model parameters using the gradients computed during backpropagation.\n",
        "\n",
        "15. **Why is it important to set the initial hidden state as zero at the start of each name input?**  \n",
        "    a) To make predictions faster  \n",
        "    b) To prevent information from the previous name from influencing the next name  \n",
        "    c) To speed up training  \n",
        "    d) To reduce the model's size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Initializing the hidden state as zero ensures that information from the previous name doesn't affect the next name being classified.\n",
        "\n",
        "---\n",
        "\n",
        "### Lecture 5 (Evaluation and Enhancements)\n",
        "\n",
        "16. **What metric is commonly used to evaluate classification models like the RNN in this task?**  \n",
        "    a) Precision  \n",
        "    b) Recall  \n",
        "    c) Accuracy  \n",
        "    d) F1-Score  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* Accuracy, the proportion of correctly classified names, is often used to evaluate classification models.\n",
        "\n",
        "17. **What is the role of the `confusion matrix` in evaluating the RNN?**  \n",
        "    a) To calculate the loss  \n",
        "    b) To compute gradients  \n",
        "    c) To show the correct and incorrect predictions for each class  \n",
        "    d) To reduce overfitting  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* A confusion matrix shows how many names were correctly or incorrectly classified for each class.\n",
        "\n",
        "18. **Why might the model confuse certain languages with each other?**  \n",
        "    a) The dataset is too small  \n",
        "    b) The hidden state size is too large  \n",
        "    c) Some languages have similar name patterns  \n",
        "    d) The loss function is incorrectly set  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* Similar name patterns across languages can confuse the model, leading to incorrect classifications.\n",
        "\n",
        "19. **What is the main reason for using a larger hidden state size in an RNN?**  \n",
        "    a) To reduce computation time  \n",
        "    b) To allow the model to capture more complex patterns in the input sequence  \n",
        "    c) To make the model simpler  \n",
        "    d) To reduce the dataset size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A larger hidden state size allows the model to store more information, enabling it to capture more complex patterns.\n",
        "\n",
        "20. **What is overfitting in the context of RNN training?**  \n",
        "    a) When the model performs well on the training data but poorly on unseen data  \n",
        "    b) When the model performs poorly on both training and test data  \n",
        "    c) When the loss value is zero  \n",
        "    d) When the hidden state does not update  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Overfitting occurs when the model memorizes the training data and fails to generalize to new, unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Concepts\n",
        "\n",
        "21. **What is gradient clipping used for in RNNs?**  \n",
        "    a) To increase the learning rate  \n",
        "    b) To prevent gradients from becoming too large and destabilizing the training  \n",
        "    c) To reduce the model's complexity  \n",
        "    d) To normalize the input data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Gradient clipping limits the magnitude of gradients to prevent them from becoming too large and causing instability during training.\n",
        "\n",
        "22. **What is the role of the Softmax layer in the output of an RNN?**  \n",
        "    a) To compute the loss  \n",
        "    b) To normalize the outputs into probabilities  \n",
        "    c) To compute the hidden state  \n",
        "    d) To update the model parameters  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The Softmax layer normalizes the output values into a probability distribution over the classes.\n",
        "\n",
        "23. **Which optimizer is commonly used in RNN training for its adaptive learning rates?**  \n",
        "    a) SGD  \n",
        "    b) Adam  \n",
        "    c) RMSprop  \n",
        "    d) AdaGrad  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Adam is widely used for training neural networks, including RNNs, due to its adaptive learning rate capabilities.\n",
        "\n",
        "24. **What happens\n",
        "\n",
        " when the learning rate is set too high?**  \n",
        "    a) The model converges faster  \n",
        "    b) The model might overshoot the optimal weights and fail to converge  \n",
        "    c) The training time decreases significantly  \n",
        "    d) The model becomes more accurate  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A high learning rate may cause the model to overshoot the optimal solution, leading to poor convergence.\n",
        "\n",
        "25. **What is a key feature of LSTMs that differentiates them from simple RNNs?**  \n",
        "    a) They process images instead of text  \n",
        "    b) They use gating mechanisms to control the flow of information and prevent vanishing gradients  \n",
        "    c) They don't require training  \n",
        "    d) They use convolutional layers  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs have gating mechanisms (input, output, forget gates) that help control the flow of information, preventing the vanishing gradient problem.\n"
      ],
      "metadata": {
        "id": "Q9rLloQCKUqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. **Which PyTorch function is used to apply dropout in the model to prevent overfitting?**  \n",
        "    a) `torch.nn.Dropout`  \n",
        "    b) `torch.nn.ReLU`  \n",
        "    c) `torch.nn.Softmax`  \n",
        "    d) `torch.optim.Dropout`  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.Dropout` applies dropout during training to randomly deactivate neurons and prevent overfitting.\n",
        "\n",
        "27. **Why is dropout applied during training but not during evaluation?**  \n",
        "    a) Dropout increases model complexity during evaluation  \n",
        "    b) Dropout prevents accurate evaluation by deactivating neurons  \n",
        "    c) Dropout improves accuracy only during training  \n",
        "    d) Dropout should be active during both training and evaluation  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Dropout deactivates neurons during training to prevent overfitting, but during evaluation, all neurons should be active to ensure accurate predictions.\n",
        "\n",
        "28. **What is the purpose of a `torch.nn.Linear` layer in an RNN model?**  \n",
        "    a) To perform a convolution operation  \n",
        "    b) To apply dropout to the input  \n",
        "    c) To compute the weighted sum of inputs and produce output for the next layer  \n",
        "    d) To compute the loss  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* `torch.nn.Linear` is a fully connected layer that computes a weighted sum of inputs and produces output for the next layer.\n",
        "\n",
        "29. **In PyTorch, what does `torch.cat()` function do?**  \n",
        "    a) Slices a tensor  \n",
        "    b) Concatenates multiple tensors along a specified dimension  \n",
        "    c) Normalizes a tensor  \n",
        "    d) Adds two tensors element-wise  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `torch.cat()` concatenates multiple tensors along a specified dimension, often used for combining inputs or outputs.\n",
        "\n",
        "30. **Why is it important to call `optimizer.zero_grad()` before `loss.backward()`?**  \n",
        "    a) To accumulate gradients  \n",
        "    b) To reset gradients from the previous iteration  \n",
        "    c) To update the weights  \n",
        "    d) To compute the loss  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `optimizer.zero_grad()` resets gradients from the previous iteration, preventing them from being accumulated.\n",
        "\n",
        "---\n",
        "\n",
        "### Data Representation in RNN\n",
        "\n",
        "31. **How are characters represented as inputs to an RNN model?**  \n",
        "    a) As integers  \n",
        "    b) As one-hot encoded vectors  \n",
        "    c) As raw text  \n",
        "    d) As images  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Characters are typically represented as one-hot encoded vectors before being fed into an RNN.\n",
        "\n",
        "32. **Why is one-hot encoding used in character-level models?**  \n",
        "    a) It reduces dimensionality  \n",
        "    b) It creates unique binary representations for each character  \n",
        "    c) It normalizes the input  \n",
        "    d) It is used for output layers only  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* One-hot encoding creates unique binary vectors for each character, where only one element is 1, representing the specific character.\n",
        "\n",
        "33. **What shape would a one-hot encoded tensor for a name with 5 characters and 26 possible characters (alphabet) have?**  \n",
        "    a) (5, 1)  \n",
        "    b) (5, 26)  \n",
        "    c) (26, 5)  \n",
        "    d) (1, 5, 26)  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A one-hot encoded tensor for a name with 5 characters and 26 possible letters would have the shape `(5, 26)` where 5 is the sequence length and 26 is the number of possible characters.\n",
        "\n",
        "34. **What does the hidden state in an RNN represent?**  \n",
        "    a) The final output of the model  \n",
        "    b) The memory of the sequence processed so far  \n",
        "    c) The loss value  \n",
        "    d) The one-hot encoded input  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The hidden state represents the memory of the sequence processed so far and is updated at each time step.\n",
        "\n",
        "35. **What is the primary role of the `hidden` parameter in RNN models?**  \n",
        "    a) To store weights of the model  \n",
        "    b) To pass information from one time step to the next  \n",
        "    c) To store the model's loss function  \n",
        "    d) To initialize the optimizer  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The `hidden` parameter in RNNs stores information (context) from the previous time step and passes it to the next time step.\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Training and Optimizer Usage\n",
        "\n",
        "36. **Which function updates the model parameters after backpropagation?**  \n",
        "    a) `optimizer.step()`  \n",
        "    b) `optimizer.backward()`  \n",
        "    c) `loss.zero_grad()`  \n",
        "    d) `model.train()`  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `optimizer.step()` updates the model parameters using the gradients computed during backpropagation.\n",
        "\n",
        "37. **Why is `torch.nn.CrossEntropyLoss` not used in this character-level classification model?**  \n",
        "    a) It doesn't support RNNs  \n",
        "    b) `LogSoftmax` and `NLLLoss` are used instead for better numerical stability  \n",
        "    c) It cannot compute gradients  \n",
        "    d) It is used only for regression tasks  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `LogSoftmax` combined with `NLLLoss` is preferred over `CrossEntropyLoss` for better numerical stability when dealing with log probabilities.\n",
        "\n",
        "38. **What is the impact of using `torch.optim.SGD` with a low learning rate?**  \n",
        "    a) It speeds up training  \n",
        "    b) It may cause the model to converge slowly or not at all  \n",
        "    c) It causes overfitting  \n",
        "    d) It prevents vanishing gradients  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A low learning rate may cause the model to converge very slowly or not converge at all, as weight updates become too small.\n",
        "\n",
        "39. **What is the default behavior of `torch.optim.Adam` in PyTorch?**  \n",
        "    a) It uses momentum to accelerate training  \n",
        "    b) It adapts learning rates for each parameter automatically  \n",
        "    c) It increases the learning rate exponentially  \n",
        "    d) It prevents the model from overfitting  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `torch.optim.Adam` automatically adapts the learning rate for each parameter based on the gradients and their historical values.\n",
        "\n",
        "40. **Which learning rate scheduling technique reduces the learning rate after a plateau in performance?**  \n",
        "    a) `StepLR`  \n",
        "    b) `ReduceLROnPlateau`  \n",
        "    c) `ExponentialLR`  \n",
        "    d) `CyclicLR`  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `ReduceLROnPlateau` reduces the learning rate when the performance (validation loss) stops improving for a specified number of epochs.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation and Metrics\n",
        "\n",
        "41. **How is accuracy computed in a classification task?**  \n",
        "    a) By summing up the loss values  \n",
        "    b) By dividing the number of correct predictions by the total number of predictions  \n",
        "    c) By calculating the mean of all outputs  \n",
        "    d) By subtracting the loss from the predicted values  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Accuracy is computed by dividing the number of correct predictions by the total number of predictions.\n",
        "\n",
        "42. **What is a confusion matrix used for?**  \n",
        "    a) To display the number of correct and incorrect predictions for each class  \n",
        "    b) To calculate the loss  \n",
        "    c) To plot the training curve  \n",
        "    d) To update the hidden state  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A confusion matrix displays the correct and incorrect predictions for each class, helping to analyze the model's classification performance.\n",
        "\n",
        "43. **What does overfitting mean in the context of training an RNN model?**  \n",
        "    a) The model performs well on training data but poorly on unseen test data  \n",
        "    b) The model fails to learn patterns in the data  \n",
        "    c) The model performs better on test data than training data  \n",
        "    d) The model takes too long to converge  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Overfitting occurs when the model performs well on training data but fails to generalize to unseen test data.\n",
        "\n",
        "44. **Which evaluation metric combines both precision and recall into a single score?**  \n",
        "    a) F1-score  \n",
        "    b) Accuracy  \n",
        "    c) Mean Squared Error  \n",
        "    d) Cross Entropy  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The F1-score is the harmonic mean of precision and recall, providing a single metric that balances the two.\n",
        "\n",
        "45. **What does the `model.eval()` function do in PyTorch?**  \n",
        "    a) Initializes the model for training  \n",
        "    b) Switches the model to evaluation mode, disabling dropout and batch normalization  \n",
        "    c) Resets the gradients  \n",
        "    d) Computes the loss  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `model.eval()` switches the model to evaluation mode, which disables operations like dropout and batch normalization that are only required during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Regularization and Generalization\n",
        "\n",
        "46. **What is the main purpose of regularization techniques like dropout?**  \n",
        "    a) To prevent overfitting by adding noise during training  \n",
        "    b) To improve accuracy on training data  \n",
        "    c) To speed up the training process  \n",
        "    d) To reduce the size of the dataset  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Regularization techniques like dropout are used to prevent overfitting by adding noise during training, forcing the model to generalize better.\n",
        "\n",
        "47. **What does `torch.nn.Dropout`\n",
        "\n",
        " do during training?**  \n",
        "    a) Deactivates neurons randomly to reduce overfitting  \n",
        "    b) Increases the learning rate  \n",
        "    c) Adds random noise to the input  \n",
        "    d) Reduces the number of hidden states  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.Dropout` randomly deactivates neurons during training to prevent overfitting.\n",
        "\n",
        "48. **Why is early stopping used during training?**  \n",
        "    a) To stop training when the model's performance on the validation set stops improving  \n",
        "    b) To reduce the training time by limiting the number of epochs  \n",
        "    c) To increase the size of the model  \n",
        "    d) To shuffle the training data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Early stopping halts training when the modelâ€™s performance on the validation set stops improving, helping to avoid overfitting.\n",
        "\n",
        "49. **What is L2 regularization?**  \n",
        "    a) A method that adds a penalty proportional to the squared values of the weights  \n",
        "    b) A method that reduces the number of hidden layers  \n",
        "    c) A method that increases the batch size  \n",
        "    d) A method that skips updating certain neurons  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* L2 regularization adds a penalty to the loss function that is proportional to the squared magnitude of the weights, encouraging smaller weight values.\n",
        "\n",
        "50. **How does increasing dropout probability affect the model?**  \n",
        "    a) It increases the model's capacity to memorize training data  \n",
        "    b) It increases the chance of underfitting  \n",
        "    c) It decreases the size of the dataset  \n",
        "    d) It has no impact on performance  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Increasing dropout probability may increase the chance of underfitting because too many neurons are deactivated, reducing the model's ability to learn complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Enhancements\n",
        "\n",
        "51. **What is the impact of using a larger hidden state size in an RNN?**  \n",
        "    a) It allows the model to capture more complex patterns in the input sequence  \n",
        "    b) It reduces the computation time  \n",
        "    c) It increases the dropout rate  \n",
        "    d) It makes the model prone to overfitting  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A larger hidden state size allows the model to capture more complex patterns, making it better suited for learning from input sequences.\n",
        "\n",
        "52. **Why might the model confuse similar language classes in name classification tasks?**  \n",
        "    a) Because some language classes have similar spelling patterns  \n",
        "    b) Because the model's hidden state size is too large  \n",
        "    c) Because of incorrect one-hot encoding  \n",
        "    d) Because the dataset is too large  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The model might confuse similar language classes because some languages share similar spelling patterns in surnames.\n",
        "\n",
        "53. **What is a common enhancement to RNNs to better handle long-term dependencies?**  \n",
        "    a) Using LSTMs or GRUs instead of simple RNNs  \n",
        "    b) Increasing the learning rate  \n",
        "    c) Using dropout at each time step  \n",
        "    d) Adding more hidden layers  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are RNN variants designed to better handle long-term dependencies and avoid the vanishing gradient problem.\n",
        "\n",
        "54. **Why would using a smaller batch size in training be beneficial for some models?**  \n",
        "    a) It leads to faster convergence  \n",
        "    b) It provides more frequent updates, which can help the model generalize better  \n",
        "    c) It requires less computation per batch  \n",
        "    d) It improves overfitting  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A smaller batch size provides more frequent updates, which may help the model generalize better to unseen data.\n",
        "\n",
        "55. **How does `torch.nn.LSTM` improve upon `torch.nn.RNN`?**  \n",
        "    a) It uses gating mechanisms to control the flow of information, improving the model's ability to learn long-term dependencies  \n",
        "    b) It is faster than RNN  \n",
        "    c) It has fewer parameters than a simple RNN  \n",
        "    d) It processes inputs in parallel  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.LSTM` improves upon simple RNNs by using gates to control the flow of information, making it better at handling long-term dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Training Details\n",
        "\n",
        "56. **What does `model.train()` do in PyTorch?**  \n",
        "    a) Initializes the model for inference  \n",
        "    b) Sets the model to training mode, enabling dropout and batch normalization  \n",
        "    c) Updates the model's parameters  \n",
        "    d) Resets the hidden state  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* `model.train()` sets the model to training mode, enabling operations like dropout and batch normalization that are required only during training.\n",
        "\n",
        "57. **How does `torch.nn.Embedding` layer help in NLP tasks?**  \n",
        "    a) By transforming tokens (words/characters) into dense vectors representing their semantic meanings  \n",
        "    b) By normalizing the input sequence  \n",
        "    c) By reducing the size of the input data  \n",
        "    d) By increasing the sequence length  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.Embedding` is used to convert tokens (such as words or characters) into dense vectors that capture semantic relationships between tokens.\n",
        "\n",
        "58. **How does teacher forcing work during RNN training?**  \n",
        "    a) It feeds the actual output from the previous time step as the input to the next time step during training  \n",
        "    b) It uses the predicted output as input for the next step  \n",
        "    c) It prevents the model from learning during training  \n",
        "    d) It increases the hidden state size  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Teacher forcing involves feeding the actual output from the previous time step as input to the next time step during training, which helps the model converge faster.\n",
        "\n",
        "59. **What is the role of the output gate in an LSTM?**  \n",
        "    a) It decides how much of the cell state should be passed to the next hidden state  \n",
        "    b) It updates the weights  \n",
        "    c) It applies dropout to the input sequence  \n",
        "    d) It normalizes the input  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The output gate in an LSTM determines how much of the cell state should be passed to the next hidden state and the output at the current time step.\n",
        "\n",
        "60. **How does a bidirectional RNN differ from a standard RNN?**  \n",
        "    a) It processes input sequences in both forward and backward directions  \n",
        "    b) It uses more hidden states  \n",
        "    c) It can only handle fixed-length sequences  \n",
        "    d) It skips over time steps  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A bidirectional RNN processes input sequences in both forward and backward directions, allowing it to capture context from both past and future time steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Architectures and Optimization\n",
        "\n",
        "61. **What is gradient clipping primarily used for in RNN training?**  \n",
        "    a) To prevent gradients from exploding, ensuring stability during backpropagation  \n",
        "    b) To increase the learning rate  \n",
        "    c) To reduce overfitting  \n",
        "    d) To shuffle the input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Gradient clipping is used to prevent gradients from becoming too large (exploding gradients), which can destabilize the training process.\n",
        "\n",
        "62. **Why is backpropagation through time (BPTT) used in RNNs?**  \n",
        "    a) To update the model's parameters using gradients computed over multiple time steps  \n",
        "    b) To increase the number of hidden layers  \n",
        "    c) To reset the hidden state  \n",
        "    d) To reduce the sequence length  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* BPTT is used in RNNs to compute gradients over multiple time steps, allowing the model to learn dependencies between distant time steps.\n",
        "\n",
        "63. **Which optimizer is known for adapting the learning rate for each parameter individually?**  \n",
        "    a) SGD  \n",
        "    b) Adam  \n",
        "    c) RMSprop  \n",
        "    d) AdaGrad  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Adam optimizer adapts the learning rate for each parameter individually based on the magnitude of the gradients.\n",
        "\n",
        "64. **Why might `torch.optim.Adam` be preferred over `SGD` for training deep RNNs?**  \n",
        "    a) It adapts the learning rate dynamically, which helps stabilize training  \n",
        "    b) It converges faster without needing a learning rate scheduler  \n",
        "    c) It requires less memory  \n",
        "    d) It doesn't need gradient clipping  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `Adam` adapts the learning rate dynamically, which is particularly useful for stabilizing training in deep networks like RNNs.\n",
        "\n",
        "65. **What is one advantage of using mini-batch training in RNNs?**  \n",
        "    a) It allows for faster training by updating weights more frequently  \n",
        "    b) It reduces the memory needed for training  \n",
        "    c) It prevents the vanishing gradient problem  \n",
        "    d) It eliminates the need for dropout  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Mini-batch training updates weights more frequently than full-batch training, allowing for faster convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Set of Questions\n",
        "\n",
        "66. **What does `torch.nn.LSTMCell` do in PyTorch?**  \n",
        "    a) Processes only one time step at a time and updates both the hidden and cell states  \n",
        "    b) Processes the entire sequence at once  \n",
        "    c) Increases the model's learning rate  \n",
        "    d) Reduces the sequence length  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.LSTMCell` processes one time step at a time, updating the hidden state and cell state based on the input and previous states.\n",
        "\n",
        "67. **How does using multiple layers in\n",
        "\n",
        " an RNN help improve model performance?**  \n",
        "    a) It enables the model to learn more complex patterns from the data  \n",
        "    b) It reduces training time  \n",
        "    c) It simplifies the architecture  \n",
        "    d) It prevents overfitting  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Using multiple layers allows the model to capture more complex patterns and hierarchical relationships in the data.\n",
        "\n",
        "68. **Why is `torch.nn.Module` used as the base class for defining models in PyTorch?**  \n",
        "    a) It provides the necessary functionality for defining and managing neural network layers and parameters  \n",
        "    b) It simplifies the training process  \n",
        "    c) It is only used for convolutional layers  \n",
        "    d) It computes gradients automatically  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.Module` provides the core functionality needed to define layers, track parameters, and manage the forward and backward passes in neural networks.\n",
        "\n",
        "69. **What is the purpose of the hidden-to-output linear layer in an RNN?**  \n",
        "    a) To convert the hidden state into the final output prediction for each time step  \n",
        "    b) To update the hidden state  \n",
        "    c) To apply dropout  \n",
        "    d) To reduce the input sequence length  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The hidden-to-output linear layer transforms the hidden state into the final output prediction, which can then be used for classification or regression.\n",
        "\n",
        "70. **Why is the `tanh` activation function often used in RNN hidden layers?**  \n",
        "    a) It helps normalize the hidden state between -1 and 1, maintaining stable gradients  \n",
        "    b) It increases the learning rate  \n",
        "    c) It simplifies the model architecture  \n",
        "    d) It prevents overfitting  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The `tanh` activation function normalizes the hidden state between -1 and 1, which helps maintain stable gradients during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation and Model Interpretability\n",
        "\n",
        "71. **What does `torch.no_grad()` do during evaluation?**  \n",
        "    a) Disables gradient calculations, reducing memory usage and speeding up evaluation  \n",
        "    b) Enables dropout for better predictions  \n",
        "    c) Resets the model parameters  \n",
        "    d) Updates the optimizer  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.no_grad()` disables gradient calculations during evaluation, which reduces memory usage and speeds up the process.\n",
        "\n",
        "72. **What is a potential issue with gradient vanishing in RNNs?**  \n",
        "    a) The model fails to learn long-term dependencies because gradients become too small to affect weights  \n",
        "    b) The gradients increase exponentially  \n",
        "    c) The model cannot handle short sequences  \n",
        "    d) The hidden states are reset after every time step  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Gradient vanishing is a common problem in RNNs where gradients become too small to affect the weights, making it difficult for the model to learn long-term dependencies.\n",
        "\n",
        "73. **What is one method to reduce overfitting in an RNN model?**  \n",
        "    a) Add dropout layers between hidden layers  \n",
        "    b) Increase the learning rate  \n",
        "    c) Increase the hidden state size  \n",
        "    d) Reduce the number of training examples  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adding dropout layers between hidden layers helps prevent overfitting by randomly deactivating neurons during training.\n",
        "\n",
        "74. **Why is a validation set used during training?**  \n",
        "    a) To evaluate the model's performance on unseen data during training and prevent overfitting  \n",
        "    b) To increase the training time  \n",
        "    c) To shuffle the training data  \n",
        "    d) To compute the gradients  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A validation set is used to evaluate the model's performance on unseen data during training, helping to monitor generalization and prevent overfitting.\n",
        "\n",
        "75. **How does increasing the number of epochs affect the training process?**  \n",
        "    a) It allows the model to learn more from the data, but can also lead to overfitting if continued for too long  \n",
        "    b) It decreases the chance of overfitting  \n",
        "    c) It reduces the size of the hidden state  \n",
        "    d) It simplifies the architecture  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Increasing the number of epochs allows the model to learn more from the data but can lead to overfitting if training continues after the model has already converged.\n",
        "\n",
        "---\n",
        "\n",
        "### LSTMs, GRUs, and Performance Improvements\n",
        "\n",
        "76. **What is a major advantage of using GRUs over LSTMs in some tasks?**  \n",
        "    a) GRUs are simpler and require fewer parameters, making them computationally more efficient  \n",
        "    b) GRUs are more complex and require more parameters  \n",
        "    c) GRUs prevent all overfitting  \n",
        "    d) GRUs can process images more efficiently than LSTMs  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* GRUs are simpler than LSTMs and require fewer parameters, making them computationally more efficient while still handling long-term dependencies.\n",
        "\n",
        "77. **What is the purpose of the forget gate in an LSTM?**  \n",
        "    a) To control how much of the previous cell state should be passed forward  \n",
        "    b) To reset the hidden state  \n",
        "    c) To increase the learning rate  \n",
        "    d) To normalize the input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The forget gate in an LSTM controls how much of the previous cell state should be passed forward to the next time step.\n",
        "\n",
        "78. **Why might a model trained with LSTMs generalize better than one trained with a simple RNN?**  \n",
        "    a) LSTMs handle long-term dependencies better, making them less prone to overfitting on short-term patterns  \n",
        "    b) LSTMs reduce the size of the dataset  \n",
        "    c) LSTMs require no validation set  \n",
        "    d) LSTMs increase the learning rate  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* LSTMs are better at handling long-term dependencies, which helps them generalize better to unseen data by focusing on important patterns over time.\n",
        "\n",
        "79. **What is the role of the input gate in an LSTM?**  \n",
        "    a) To determine how much of the current input should be added to the cell state  \n",
        "    b) To control the output of the LSTM  \n",
        "    c) To apply dropout to the input  \n",
        "    d) To clip the gradients  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The input gate in an LSTM controls how much of the current input should be added to the cell state, allowing the model to decide which information is important.\n",
        "\n",
        "80. **How does a bidirectional RNN differ from a unidirectional RNN?**  \n",
        "    a) It processes sequences in both forward and backward directions, capturing more context  \n",
        "    b) It only processes fixed-length sequences  \n",
        "    c) It increases the number of parameters significantly  \n",
        "    d) It ignores past time steps  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A bidirectional RNN processes sequences in both forward and backward directions, allowing it to capture more context from both past and future time steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Questions (81-100)\n",
        "\n",
        "81. **Why might a larger batch size lead to faster training?**  \n",
        "    a) It reduces the number of weight updates per epoch  \n",
        "    b) It increases the learning rate  \n",
        "    c) It increases the number of neurons in each layer  \n",
        "    d) It decreases the amount of data processed at each step  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A larger batch size reduces the number of weight updates per epoch, which can lead to faster training, although it may affect generalization.\n",
        "\n",
        "82. **What does `torch.nn.Sequential` allow you to do in PyTorch?**  \n",
        "    a) Chain multiple layers together in a sequence to create a model  \n",
        "    b) Train the model faster  \n",
        "    c) Increase the hidden state size  \n",
        "    d) Compute gradients automatically  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `torch.nn.Sequential` allows you to chain multiple layers together, defining the forward pass of the model as a sequence of operations.\n",
        "\n",
        "83. **What is one potential drawback of using a very deep RNN model?**  \n",
        "    a) It increases the risk of the vanishing gradient problem, making it harder to learn long-term dependencies  \n",
        "    b) It reduces the training time  \n",
        "    c) It eliminates the need for dropout  \n",
        "    d) It increases the accuracy on the training data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Very deep RNNs are prone to the vanishing gradient problem, where gradients become too small to effectively update weights, hindering learning of long-term dependencies.\n",
        "\n",
        "84. **What is the purpose of using an embedding layer in sequence models?**  \n",
        "    a) To transform input tokens (words or characters) into dense vectors that capture semantic meaning  \n",
        "    b) To apply dropout to the input sequence  \n",
        "    c) To reduce the size of the dataset  \n",
        "    d) To increase the length of the sequence  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Embedding layers transform input tokens into dense vectors that capture semantic relationships between tokens, which are then used in the model.\n",
        "\n",
        "85. **Which optimizer might you choose if your model shows slow convergence with `SGD`?**  \n",
        "    a) Adam  \n",
        "    b) RMSprop  \n",
        "    c) Adagrad  \n",
        "    d) SGD with momentum  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adam is often chosen when `SGD` results in slow convergence, as it adapts learning rates and tends to converge faster.\n",
        "\n",
        "86. **How does using a learning rate scheduler improve training?**  \n",
        "    a) It adjusts the learning rate during training, preventing the model from getting stuck in local minima  \n",
        "    b) It increases the dataset size  \n",
        "    c) It removes the need for gradient clipping  \n",
        "    d) It automatically resets the hidden state  \n",
        "   \n",
        "\n",
        " **Answer:** a  \n",
        "    *Explanation:* Learning rate schedulers adjust the learning rate during training, helping the model avoid getting stuck in local minima or overshooting the optimal solution.\n",
        "\n",
        "87. **Why is a smaller learning rate sometimes preferred for fine-tuning models?**  \n",
        "    a) It makes smaller updates to the model parameters, allowing for more precise adjustments  \n",
        "    b) It speeds up training  \n",
        "    c) It prevents overfitting  \n",
        "    d) It increases the size of the dataset  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A smaller learning rate makes smaller updates to the model parameters, which is especially useful during fine-tuning to make more precise adjustments without overshooting.\n",
        "\n",
        "88. **What is the primary goal of using an LSTM in sequential tasks?**  \n",
        "    a) To learn long-term dependencies in the data by mitigating the vanishing gradient problem  \n",
        "    b) To speed up training  \n",
        "    c) To increase the dataset size  \n",
        "    d) To shuffle the input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* LSTMs are designed to learn long-term dependencies in sequences, making them particularly effective for tasks that require memory over time.\n",
        "\n",
        "89. **Why might a model perform poorly on unseen data despite high accuracy on training data?**  \n",
        "    a) The model is overfitting to the training data and failing to generalize  \n",
        "    b) The model's hidden state size is too large  \n",
        "    c) The learning rate is too low  \n",
        "    d) The batch size is too small  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Poor performance on unseen data, despite high accuracy on training data, is usually a sign of overfitting, where the model has learned the training data too well but fails to generalize to new data.\n",
        "\n",
        "90. **What is the purpose of using `torch.utils.data.DataLoader` in PyTorch?**  \n",
        "    a) To efficiently load batches of data during training and evaluation  \n",
        "    b) To normalize the input data  \n",
        "    c) To apply dropout  \n",
        "    d) To increase the learning rate  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `DataLoader` is used to efficiently load batches of data during training and evaluation, making it easier to shuffle and iterate over the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**91. What is the impact of the sequence length in RNN models?**  \n",
        "    a) Longer sequences may introduce more dependencies but also increase the risk of vanishing gradients  \n",
        "    b) Shorter sequences always improve performance  \n",
        "    c) Sequence length has no impact on the model  \n",
        "    d) Sequence length reduces the learning rate  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Longer sequences allow the model to capture more dependencies but also increase the likelihood of the vanishing gradient problem, making training more difficult.\n",
        "\n",
        "**92. What is the difference between a simple RNN and a GRU?**  \n",
        "    a) GRU has fewer gates and is computationally more efficient than a simple RNN  \n",
        "    b) GRU requires fewer training examples  \n",
        "    c) GRU is only used for classification tasks  \n",
        "    d) GRU has more parameters than an LSTM  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* GRUs are computationally more efficient than simple RNNs because they have fewer gates, allowing them to learn dependencies without as much computational cost.\n",
        "\n",
        "**93. What is a Seq2Seq model commonly used for?**  \n",
        "    a) Machine translation  \n",
        "    b) Image recognition  \n",
        "    c) Sentiment analysis  \n",
        "    d) Text classification  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Seq2Seq models are commonly used for tasks like machine translation, where the goal is to map one sequence (input) to another sequence (output).\n",
        "\n",
        "**94. How does a convolutional layer in a CNN differ from an RNN?**  \n",
        "    a) A convolutional layer processes spatial data, while an RNN processes sequential data  \n",
        "    b) A convolutional layer processes sequences in reverse order  \n",
        "    c) A convolutional layer uses hidden states  \n",
        "    d) A convolutional layer processes input in batches only  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Convolutional layers process spatial data, such as images, by applying filters to local regions, while RNNs process sequential data such as text or time series.\n",
        "\n",
        "**95. What is the role of `hidden_dim` in an RNN model?**  \n",
        "    a) It defines the size of the hidden state, controlling how much information the model can store  \n",
        "    b) It controls the output size  \n",
        "    c) It reduces the number of layers  \n",
        "    d) It increases the learning rate  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* `hidden_dim` defines the size of the hidden state, which controls how much information the model can store and process from previous time steps.\n",
        "\n",
        "**96. How does using batch normalization help in deep learning models?**  \n",
        "    a) It normalizes the activations of each layer, making training faster and more stable  \n",
        "    b) It increases the size of the hidden state  \n",
        "    c) It decreases the learning rate  \n",
        "    d) It prevents vanishing gradients  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Batch normalization normalizes the activations of each layer, improving training stability and convergence speed.\n",
        "\n",
        "**97. What is the primary advantage of using teacher forcing in RNNs?**  \n",
        "    a) It speeds up convergence by feeding the actual output as input during training  \n",
        "    b) It reduces the learning rate  \n",
        "    c) It prevents overfitting  \n",
        "    d) It allows for parallel processing  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Teacher forcing speeds up convergence by feeding the actual output from the previous time step as input for the next time step during training.\n",
        "\n",
        "**98. How does bidirectional LSTM improve performance in sequence tasks?**  \n",
        "    a) It captures both forward and backward dependencies in the sequence  \n",
        "    b) It reduces the sequence length  \n",
        "    c) It increases the number of hidden states  \n",
        "    d) It prevents gradient clipping  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Bidirectional LSTMs process sequences in both forward and backward directions, capturing dependencies from both sides of the input sequence.\n",
        "\n",
        "**99. What is one of the key challenges when training RNNs with long sequences?**  \n",
        "    a) Vanishing gradient problem, where gradients become too small to update weights effectively  \n",
        "    b) Overfitting occurs less frequently  \n",
        "    c) Learning rate increases over time  \n",
        "    d) Training time is significantly reduced  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The vanishing gradient problem is a key challenge when training RNNs with long sequences, as gradients become too small to update weights effectively.\n",
        "\n",
        "**100. What does the `hidden-to-output` layer in an RNN model do?**  \n",
        "    a) It converts the hidden state into the final output  \n",
        "    b) It applies dropout to the hidden state  \n",
        "    c) It shuffles the input sequence  \n",
        "    d) It initializes the hidden state  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The hidden-to-output layer converts the hidden state into the final output, which is then used for predictions or further processing.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WHvvqE3LKUpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Advanced Code-Level Questions**\n",
        "\n",
        "**101.** **What is the primary purpose of `torch.no_grad()` in a PyTorch model?**  \n",
        "   a) To enable gradient computation  \n",
        "   b) To disable gradient computation during inference  \n",
        "   c) To reset gradients  \n",
        "   d) To update model parameters  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `torch.no_grad()` is used during inference to prevent gradients from being computed, saving memory and computational power.\n",
        "\n",
        "**102.** **What does `letterToTensor` function do in the character-level RNN?**  \n",
        "   a) Converts a letter into a one-hot encoded tensor  \n",
        "   b) Converts a word into a list of characters  \n",
        "   c) Converts a tensor into a scalar  \n",
        "   d) Concatenates characters  \n",
        "   **Answer:** a  \n",
        "   *Explanation:* `letterToTensor` turns each letter into a one-hot encoded tensor that is fed into the RNN.\n",
        "\n",
        "**103.** **Why is a one-hot encoded vector used for representing characters in the RNN model?**  \n",
        "   a) To reduce the size of the dataset  \n",
        "   b) To allow the model to distinguish between different characters  \n",
        "   c) To convert letters into images  \n",
        "   d) To normalize the dataset  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* One-hot encoding allows each character to be uniquely represented by a vector, making it easy for the model to process.\n",
        "\n",
        "**104.** **Which loss function is used in the name classification RNN?**  \n",
        "   a) Mean Squared Error  \n",
        "   b) Cross Entropy Loss  \n",
        "   c) Negative Log Likelihood Loss (NLLLoss)  \n",
        "   d) Hinge Loss  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* NLLLoss is used in this model because it works with the `LogSoftmax` activation function used in the output layer.\n",
        "\n",
        "**105.** **How is the hidden state initialized in the RNN model?**  \n",
        "   a) As random values  \n",
        "   b) As zeros  \n",
        "   c) As the input tensor  \n",
        "   d) As the output tensor  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The hidden state is initialized as a tensor of zeros, which is passed through the RNN and updated after every time step.\n",
        "\n",
        "**106.** **What is the function of `topk` in the character-level RNN?**  \n",
        "   a) To compute the loss  \n",
        "   b) To retrieve the top `k` predictions from the output  \n",
        "   c) To reset the gradients  \n",
        "   d) To compute gradients  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `topk` is used to select the top `k` predicted categories (e.g., languages) from the output of the model.\n",
        "\n",
        "**107.** **What does `rnn.initHidden()` do in the RNN model?**  \n",
        "   a) Initializes the model weights  \n",
        "   b) Resets the model parameters  \n",
        "   c) Initializes the hidden state to zero  \n",
        "   d) Initializes the input tensor  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* `rnn.initHidden()` is used to initialize the hidden state tensor to zero at the beginning of each sequence.\n",
        "\n",
        "**108.** **Which activation function is used in the hidden layer of the RNN model?**  \n",
        "   a) ReLU  \n",
        "   b) Sigmoid  \n",
        "   c) Tanh  \n",
        "   d) Softmax  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* The `tanh` activation function is used to compute the hidden state in the RNN.\n",
        "\n",
        "**109.** **Why is `criterion.backward()` called after computing the loss?**  \n",
        "   a) To compute the gradients for backpropagation  \n",
        "   b) To update the hidden state  \n",
        "   c) To reset the gradients  \n",
        "   d) To apply dropout  \n",
        "   **Answer:** a  \n",
        "   *Explanation:* `loss.backward()` computes the gradients with respect to the model parameters, which are used for backpropagation.\n",
        "\n",
        "**110.** **In the training loop, why is `optimizer.zero_grad()` called before the backward pass?**  \n",
        "   a) To accumulate gradients  \n",
        "   b) To prevent accumulation of gradients from previous iterations  \n",
        "   c) To update the optimizer  \n",
        "   d) To normalize the input  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `optimizer.zero_grad()` resets the gradients from the previous iteration, ensuring they do not accumulate over iterations.\n",
        "\n",
        "---\n",
        "\n",
        "### **Functions and Data Handling**\n",
        "\n",
        "**111.** **What is the role of `randomTrainingExample()` function in the code?**  \n",
        "   a) It selects a random word from the dataset  \n",
        "   b) It selects a random category (language) and word (name) for training  \n",
        "   c) It shuffles the input data  \n",
        "   d) It initializes the hidden state  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `randomTrainingExample()` randomly selects a name and its corresponding language from the dataset for training.\n",
        "\n",
        "**112.** **What does the `categoryFromOutput()` function do in the RNN model?**  \n",
        "   a) Computes the gradients  \n",
        "   b) Converts the output tensor into a category label  \n",
        "   c) Initializes the output tensor  \n",
        "   d) Resets the model parameters  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `categoryFromOutput()` takes the model's output tensor and converts it into the predicted category label (e.g., language).\n",
        "\n",
        "**113.** **Why is `LogSoftmax` used as the final layer in the RNN model?**  \n",
        "   a) To compute the final loss  \n",
        "   b) To normalize the output into a probability distribution  \n",
        "   c) To initialize the model  \n",
        "   d) To reset the hidden state  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* `LogSoftmax` converts the output into a log-probability distribution, which is useful for classification tasks like name classification.\n",
        "\n",
        "**114.** **How is the input word represented before feeding it into the RNN model?**  \n",
        "   a) As a raw string  \n",
        "   b) As a one-hot encoded tensor  \n",
        "   c) As an integer index  \n",
        "   d) As an image  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* Each letter in the word is represented as a one-hot encoded tensor, which is then fed into the RNN.\n",
        "\n",
        "**115.** **What is the output of the RNN at each time step?**  \n",
        "   a) A single scalar value  \n",
        "   b) A hidden state and a probability distribution over categories  \n",
        "   c) A loss value  \n",
        "   d) A one-hot encoded vector  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The RNN outputs a hidden state and a probability distribution over the possible categories (languages) at each time step.\n",
        "\n",
        "**116.** **Why is the final prediction of the RNN based on the last time step?**  \n",
        "   a) Because the final time step contains the most important information  \n",
        "   b) Because the RNN can only output at the last time step  \n",
        "   c) To reduce computational complexity  \n",
        "   d) To prevent overfitting  \n",
        "   **Answer:** a  \n",
        "   *Explanation:* The final time step contains the processed information from all previous steps, which is used to make the final prediction.\n",
        "\n",
        "**117.** **What does the `readLines` function do when loading the dataset?**  \n",
        "   a) Reads the dataset line by line and converts each line to ASCII  \n",
        "   b) Computes the loss for each line  \n",
        "   c) Converts each line into a tensor  \n",
        "   d) Initializes the hidden state  \n",
        "   **Answer:** a  \n",
        "   *Explanation:* `readLines` reads the names from the dataset, converting each to ASCII format for easier processing.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0SBUpw5QKUnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sbd7ow5KKUk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FkigvvWlKUh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GeNhBCoPKUcl"
      }
    }
  ]
}