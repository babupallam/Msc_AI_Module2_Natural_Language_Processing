{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYbE3xCwkNvtvigPCRJB57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module2_Natural_Language_Processing/blob/main/Supporting%20Works/MCQs/MCQ_Set_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUWkf2g9is-H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction to the Task**\n",
        "\n",
        "1. **What is the main task discussed in the lecture?**  \n",
        "   a) Image classification  \n",
        "   b) Classifying names by language of origin  \n",
        "   c) Sentiment analysis  \n",
        "   d) Time-series forecasting  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The task is to classify surnames based on their language of origin using a character-level RNN.\n",
        "\n",
        "2. **What type of problem is name classification?**  \n",
        "   a) Regression  \n",
        "   b) Clustering  \n",
        "   c) Classification  \n",
        "   d) Dimensionality reduction  \n",
        "   **Answer:** c  \n",
        "   *Explanation:* Name classification is a classification problem where the model assigns a surname to a specific language class.\n",
        "\n",
        "3. **What is the input to the RNN in the name classification task?**  \n",
        "   a) A sequence of sentences  \n",
        "   b) A sequence of characters in a name  \n",
        "   c) A sequence of words in a sentence  \n",
        "   d) A sequence of images  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The input to the RNN is a sequence of characters in a name, as the model operates at the character level.\n",
        "\n",
        "4. **What does the RNN predict as output in this task?**  \n",
        "   a) The name's origin language  \n",
        "   b) The meaning of the name  \n",
        "   c) The number of characters in the name  \n",
        "   d) The first letter of the name  \n",
        "   **Answer:** a  \n",
        "   *Explanation:* The RNN predicts the language of origin based on the spelling of the name.\n",
        "\n",
        "---\n",
        "\n",
        "### **Recurrent Neural Network (RNN) Basics**\n",
        "\n",
        "5. **What is the primary function of an RNN in this task?**  \n",
        "   a) To process image data  \n",
        "   b) To process sequences of characters in a name  \n",
        "   c) To tokenize sentences  \n",
        "   d) To convert characters into integers  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* RNNs are used to process sequences of characters in a name, updating the hidden state with each character.\n",
        "\n",
        "6. **How does an RNN maintain information about previous inputs?**  \n",
        "   a) By storing all previous inputs  \n",
        "   b) Through a hidden state that is updated at each time step  \n",
        "   c) By using an attention mechanism  \n",
        "   d) By directly remembering all the characters  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* An RNN maintains information about previous inputs using a hidden state that is updated as each new character is processed.\n",
        "\n",
        "7. **What is the hidden state in an RNN used for?**  \n",
        "   a) To store the final prediction  \n",
        "   b) To maintain a memory of the sequence processed so far  \n",
        "   c) To directly predict the output at each step  \n",
        "   d) To normalize the input  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The hidden state stores information about the sequence processed so far and is updated with each input character.\n",
        "\n",
        "---\n",
        "\n",
        "### **Dataset and Preprocessing**\n",
        "\n",
        "8. **How is the dataset structured in the name classification task?**  \n",
        "   a) As a list of paragraphs  \n",
        "   b) As a dictionary where the keys are languages and the values are lists of surnames  \n",
        "   c) As a sequence of sentences  \n",
        "   d) As images of surnames  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* The dataset is a dictionary where each key is a language, and the corresponding value is a list of surnames.\n",
        "\n",
        "9. **What is the main preprocessing step before feeding names into the RNN?**  \n",
        "   a) Tokenization of words  \n",
        "   b) One-hot encoding of characters  \n",
        "   c) Lemmatization of words  \n",
        "   d) Removing stop words  \n",
        "   **Answer:** b  \n",
        "   *Explanation:* Each character in the name is one-hot encoded into a vector, which allows the RNN to process it.\n",
        "\n",
        "10. **What is a one-hot encoded vector?**  \n",
        "    a) A vector that represents all words as unique integers  \n",
        "    b) A vector that contains all zeros except for a single 1 at the index of the character  \n",
        "    c) A vector of random numbers  \n",
        "    d) A vector that sums the character occurrences  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A one-hot encoded vector has all zeros except for a single 1 at the index corresponding to the current character, making it suitable for input to the model.\n",
        "\n",
        "11. **What is the purpose of one-hot encoding in this task?**  \n",
        "    a) To reduce the number of characters in the dataset  \n",
        "    b) To convert characters into a numeric format that can be fed into the RNN  \n",
        "    c) To remove irrelevant characters  \n",
        "    d) To compress the input data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* One-hot encoding converts characters into a numeric format that the RNN can process.\n",
        "\n",
        "---\n",
        "\n",
        "### **RNN Architecture**\n",
        "\n",
        "12. **What type of neural network is used for the name classification task?**  \n",
        "    a) Convolutional Neural Network (CNN)  \n",
        "    b) Recurrent Neural Network (RNN)  \n",
        "    c) Transformer  \n",
        "    d) Multilayer Perceptron (MLP)  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The task uses a Recurrent Neural Network (RNN) to process sequences of characters.\n",
        "\n",
        "13. **What role does the hidden state play in an RNN?**  \n",
        "    a) It stores previous predictions  \n",
        "    b) It holds a representation of the sequence processed so far  \n",
        "    c) It stores the input data  \n",
        "    d) It normalizes the output  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The hidden state holds information about the characters processed so far in the sequence, helping the model remember the context.\n",
        "\n",
        "14. **How does the RNN handle sequences of characters in a name?**  \n",
        "    a) By processing all characters at once  \n",
        "    b) By processing characters one by one in sequence  \n",
        "    c) By processing the entire name as a single input  \n",
        "    d) By ignoring the order of characters  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The RNN processes characters one by one in sequence, updating the hidden state at each step.\n",
        "\n",
        "15. **What is the final layer used in the RNN for the name classification task?**  \n",
        "    a) Softmax  \n",
        "    b) LogSoftmax  \n",
        "    c) ReLU  \n",
        "    d) Sigmoid  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The final layer in the RNN is a `LogSoftmax` layer, which converts the output into probabilities for each language class.\n",
        "\n",
        "---\n",
        "\n",
        "### **Training the RNN**\n",
        "\n",
        "16. **What is the main loss function used for classification in this task?**  \n",
        "    a) Mean Squared Error (MSE)  \n",
        "    b) Negative Log Likelihood Loss (NLLLoss)  \n",
        "    c) Cross Entropy Loss  \n",
        "    d) Hinge Loss  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Negative Log Likelihood Loss (NLLLoss) is commonly used for classification tasks in PyTorch when combined with `LogSoftmax`.\n",
        "\n",
        "17. **What is the purpose of backpropagation in the RNN?**  \n",
        "    a) To pass the hidden state forward  \n",
        "    b) To update the weights based on the error between predictions and actual labels  \n",
        "    c) To make predictions for each character  \n",
        "    d) To convert characters into one-hot vectors  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Backpropagation is used to update the weights of the RNN based on the error between its predictions and the true labels.\n",
        "\n",
        "18. **What happens during a forward pass in the RNN?**  \n",
        "    a) The hidden state is initialized  \n",
        "    b) The characters are passed sequentially through the network, and the hidden state is updated  \n",
        "    c) The model is reset  \n",
        "    d) The output probabilities are discarded  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* During the forward pass, each character is processed in sequence, and the hidden state is updated after each character.\n",
        "\n",
        "19. **How are gradients used in the training process?**  \n",
        "    a) To predict the next character  \n",
        "    b) To update the weights during backpropagation  \n",
        "    c) To shuffle the input data  \n",
        "    d) To make predictions more random  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Gradients are used to update the weights of the model during backpropagation, based on the error between predicted and actual outputs.\n",
        "\n",
        "20. **What optimizer is commonly used to adjust weights in this task?**  \n",
        "    a) Adam  \n",
        "    b) SGD (Stochastic Gradient Descent)  \n",
        "    c) Adagrad  \n",
        "    d) RMSprop  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Stochastic Gradient Descent (SGD) is commonly used as an optimizer to adjust the weights of the RNN.\n",
        "\n",
        "---\n",
        "\n",
        "### **Evaluation and Performance**\n",
        "\n",
        "21. **What evaluation metric is used to assess the model's performance?**  \n",
        "    a) Mean Squared Error  \n",
        "    b) Confusion Matrix  \n",
        "    c) BLEU score  \n",
        "    d) Perplexity  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A confusion matrix is used to evaluate how well the model performs across different language classes.\n",
        "\n",
        "22. **What does a confusion matrix show?**  \n",
        "    a) The number of characters in the dataset  \n",
        "    b) The model's performance for each class, showing correct and incorrect predictions  \n",
        "    c) The accuracy of the optimizer  \n",
        "    d) The number of hidden states used  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A confusion matrix shows the correct and incorrect predictions for each class, helping to identify where the model is confused between classes.\n",
        "\n",
        "23. **Why might the model confuse\n",
        "\n",
        " certain languages with each other?**  \n",
        "    a) Some languages have similar surname spelling patterns  \n",
        "    b) The dataset is too large  \n",
        "    c) The hidden state is not updated  \n",
        "    d) The names are too short  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The model might confuse languages that have similar spelling patterns, as certain surnames may be common across multiple languages.\n",
        "\n",
        "24. **What is one way to improve the performance of the RNN for name classification?**  \n",
        "    a) Increase the learning rate  \n",
        "    b) Use a larger hidden state size  \n",
        "    c) Use fewer training samples  \n",
        "    d) Disable backpropagation  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Increasing the hidden state size allows the model to capture more complex patterns in the name sequences, improving performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Challenges and Enhancements**\n",
        "\n",
        "25. **What is a potential challenge when classifying names using a character-level RNN?**  \n",
        "    a) The model cannot handle long sequences  \n",
        "    b) Many languages share similar surname patterns, making classification harder  \n",
        "    c) The RNN only processes images  \n",
        "    d) The model has too much memory  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A key challenge is that many languages share similar surname patterns, which can lead to confusion in classification.\n",
        "\n",
        "26. **Which of the following could improve model performance?**  \n",
        "    a) Adding more layers to the RNN  \n",
        "    b) Reducing the dataset size  \n",
        "    c) Using random hidden states  \n",
        "    d) Removing certain characters from the input  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adding more layers to the RNN can improve its ability to capture complex patterns in the input data, potentially leading to better classification accuracy.\n",
        "\n",
        "27. **How might using an LSTM or GRU improve the RNN model?**  \n",
        "    a) By reducing the input size  \n",
        "    b) By addressing the vanishing gradient problem, especially for longer sequences  \n",
        "    c) By removing the need for backpropagation  \n",
        "    d) By making the model simpler  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTM or GRU networks are designed to handle the vanishing gradient problem, which can occur when processing long sequences, making them better suited for tasks like this.\n",
        "\n",
        "28. **Why is a class-imbalanced dataset a challenge in this task?**  \n",
        "    a) It causes overfitting to minority classes  \n",
        "    b) It causes the model to be biased toward majority classes  \n",
        "    c) It increases the hidden state size  \n",
        "    d) It leads to fewer predictions  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* In class-imbalanced datasets, the model tends to be biased toward the majority class, leading to poor performance on minority classes.\n",
        "\n",
        "29. **What is a potential benefit of using a larger dataset?**  \n",
        "    a) The model requires fewer layers  \n",
        "    b) The model generalizes better to unseen names  \n",
        "    c) The RNN will not need backpropagation  \n",
        "    d) The hidden state size will decrease  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A larger dataset provides more examples for the model to learn from, helping it generalize better to unseen names.\n",
        "\n",
        "30. **What is one advantage of using character-level RNNs for name classification?**  \n",
        "    a) They can handle variable-length inputs  \n",
        "    b) They are faster than CNNs  \n",
        "    c) They require no hidden state  \n",
        "    d) They process entire names at once  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Character-level RNNs can process names of varying lengths by processing each character sequentially, making them well-suited for text classification.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jxtW-Bb7i35x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. **What is one disadvantage of character-level RNNs for name classification?**  \n",
        "    a) They cannot handle variable-length inputs  \n",
        "    b) They can struggle with long sequences due to vanishing gradients  \n",
        "    c) They do not work with text data  \n",
        "    d) They require fixed-length input sequences  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Character-level RNNs can struggle with long sequences due to vanishing gradients, where information from earlier time steps is \"forgotten\" over time.\n",
        "\n",
        "32. **What is the vanishing gradient problem in RNNs?**  \n",
        "    a) Gradients become too small, making it difficult for the network to learn long-term dependencies  \n",
        "    b) The gradients are too large, causing the network to diverge  \n",
        "    c) The gradients are averaged over the dataset  \n",
        "    d) The gradients increase the number of neurons in the network  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The vanishing gradient problem occurs when the gradients become too small during backpropagation, making it difficult for the network to learn long-term dependencies.\n",
        "\n",
        "33. **What type of RNN variant is designed to handle the vanishing gradient problem?**  \n",
        "    a) Simple RNN  \n",
        "    b) LSTM (Long Short-Term Memory)  \n",
        "    c) CNN (Convolutional Neural Network)  \n",
        "    d) MLP (Multilayer Perceptron)  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs are a variant of RNNs designed to address the vanishing gradient problem, making them more effective for processing longer sequences.\n",
        "\n",
        "34. **What is the primary advantage of using an LSTM over a simple RNN?**  \n",
        "    a) LSTMs are faster to train  \n",
        "    b) LSTMs can better capture long-term dependencies in sequences  \n",
        "    c) LSTMs do not require any training data  \n",
        "    d) LSTMs only work with fixed-length inputs  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs are designed to better capture long-term dependencies in sequences by preventing the vanishing gradient problem.\n",
        "\n",
        "35. **What does GRU stand for in the context of RNNs?**  \n",
        "    a) Gradient Regression Unit  \n",
        "    b) Gated Recurrent Unit  \n",
        "    c) Gradient Rectifier Unit  \n",
        "    d) Generalized Recurrent Update  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* GRU stands for Gated Recurrent Unit, which is a variant of RNNs that simplifies the LSTM architecture while still addressing the vanishing gradient problem.\n",
        "\n",
        "---\n",
        "\n",
        "### **RNN Variants and Their Features**\n",
        "\n",
        "36. **How does a GRU differ from an LSTM?**  \n",
        "    a) A GRU has fewer gates and parameters than an LSTM  \n",
        "    b) A GRU requires fixed-length input sequences  \n",
        "    c) A GRU is slower to train than an LSTM  \n",
        "    d) A GRU uses more memory than an LSTM  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A GRU has fewer gates and parameters than an LSTM, which simplifies its architecture while still addressing the vanishing gradient issue.\n",
        "\n",
        "37. **What is one reason to use GRU instead of LSTM?**  \n",
        "    a) GRUs are better at processing image data  \n",
        "    b) GRUs are computationally more efficient due to fewer gates  \n",
        "    c) GRUs require no training data  \n",
        "    d) GRUs work only with short sequences  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* GRUs are computationally more efficient than LSTMs because they have fewer gates, making them faster to train.\n",
        "\n",
        "38. **What is a key component of LSTMs that helps them retain information for longer periods?**  \n",
        "    a) Convolutional layers  \n",
        "    b) Forget and input gates  \n",
        "    c) Pooling layers  \n",
        "    d) Fully connected layers  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs use forget and input gates to control the flow of information, allowing them to retain relevant information and forget irrelevant data over time.\n",
        "\n",
        "39. **What is the \"forget gate\" in an LSTM used for?**  \n",
        "    a) To forget the gradients from the previous time step  \n",
        "    b) To control how much of the previous hidden state should be passed forward  \n",
        "    c) To remove irrelevant characters from the input  \n",
        "    d) To increase the learning rate  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The forget gate in an LSTM controls how much of the previous hidden state should be passed forward, helping the model retain or discard information as needed.\n",
        "\n",
        "40. **How does the hidden state in an LSTM differ from that in a simple RNN?**  \n",
        "    a) The LSTM hidden state has an additional cell state that carries long-term information  \n",
        "    b) The LSTM hidden state is fixed for all time steps  \n",
        "    c) The LSTM hidden state is discarded after each step  \n",
        "    d) The LSTM hidden state is not updated during training  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The LSTM has both a hidden state and a cell state, which helps it carry long-term information more effectively than a simple RNN.\n",
        "\n",
        "---\n",
        "\n",
        "### **Training and Backpropagation**\n",
        "\n",
        "41. **What is backpropagation through time (BPTT)?**  \n",
        "    a) A method for updating weights by propagating gradients backward in time through an RNN  \n",
        "    b) A method for increasing the size of the input  \n",
        "    c) A method for training CNNs  \n",
        "    d) A method for updating input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Backpropagation through time (BPTT) is the process used to update weights in an RNN by propagating gradients backward through time steps.\n",
        "\n",
        "42. **Why is backpropagation through time important in RNNs?**  \n",
        "    a) It helps the model remember only the first input  \n",
        "    b) It allows the model to learn dependencies between earlier and later steps in a sequence  \n",
        "    c) It prevents overfitting  \n",
        "    d) It reduces the number of parameters in the model  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Backpropagation through time allows the RNN to learn dependencies between earlier and later steps in a sequence by adjusting weights based on errors at each step.\n",
        "\n",
        "43. **What happens during gradient clipping in RNN training?**  \n",
        "    a) Gradients are artificially reduced to prevent them from becoming too large and causing instability  \n",
        "    b) Gradients are increased to speed up learning  \n",
        "    c) Gradients are set to zero  \n",
        "    d) Gradients are only calculated at the last step of the sequence  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Gradient clipping prevents gradients from becoming too large (which can cause instability during training) by limiting their magnitude.\n",
        "\n",
        "44. **What optimizer is often used in RNN training to adapt the learning rate dynamically?**  \n",
        "    a) Adam  \n",
        "    b) SGD  \n",
        "    c) Adagrad  \n",
        "    d) RMSprop  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adam is an optimizer commonly used in RNN training because it adapts the learning rate dynamically, making the optimization process more efficient.\n",
        "\n",
        "45. **What is the role of the learning rate in RNN training?**  \n",
        "    a) To control how much the model weights are updated during training  \n",
        "    b) To define the number of hidden units  \n",
        "    c) To determine the number of epochs  \n",
        "    d) To increase the input size  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The learning rate controls how much the weights are updated during training. A smaller learning rate makes smaller updates, while a larger rate makes bigger updates.\n",
        "\n",
        "---\n",
        "\n",
        "### **Evaluation and Performance Metrics**\n",
        "\n",
        "46. **What evaluation metric can be used to measure classification accuracy in the name classification task?**  \n",
        "    a) Confusion matrix  \n",
        "    b) MSE  \n",
        "    c) Perplexity  \n",
        "    d) BLEU score  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A confusion matrix is used to evaluate classification accuracy by showing how well the model predicted each class compared to the actual classes.\n",
        "\n",
        "47. **How does a confusion matrix help evaluate model performance?**  \n",
        "    a) It shows the number of correct and incorrect predictions for each class  \n",
        "    b) It calculates the average loss  \n",
        "    c) It displays the final hidden state  \n",
        "    d) It visualizes the gradients  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A confusion matrix shows the number of correct and incorrect predictions for each class, helping to identify where the model is making mistakes.\n",
        "\n",
        "48. **What is precision in the context of model evaluation?**  \n",
        "    a) The fraction of correctly predicted positive instances out of all predicted positives  \n",
        "    b) The fraction of correct predictions out of all predictions made  \n",
        "    c) The difference between predicted and actual values  \n",
        "    d) The time taken to train the model  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Precision measures how many of the predicted positive instances are actually correct, making it useful for evaluating performance in imbalanced datasets.\n",
        "\n",
        "49. **What is recall in classification?**  \n",
        "    a) The fraction of actual positives that were correctly identified  \n",
        "    b) The fraction of correct predictions made by the model  \n",
        "    c) The speed of training  \n",
        "    d) The size of the hidden state  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Recall measures how many of the actual positive instances were correctly predicted by the model.\n",
        "\n",
        "50. **Why is F1 score important when evaluating the model?**  \n",
        "    a) It combines precision and recall into a single metric  \n",
        "    b) It calculates the loss function  \n",
        "    c) It only measures the recall of the model  \n",
        "    d) It determines the size of the input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The F1 score is the harmonic mean of precision and recall, providing a\n",
        "\n",
        " balanced evaluation metric for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overfitting and Generalization**\n",
        "\n",
        "51. **What is overfitting in neural networks?**  \n",
        "    a) When the model performs well on training data but poorly on unseen data  \n",
        "    b) When the model performs equally well on both training and test data  \n",
        "    c) When the model learns only from random noise  \n",
        "    d) When the model converges too quickly  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Overfitting occurs when the model performs well on the training data but fails to generalize to unseen test data, often because it has memorized the training set.\n",
        "\n",
        "52. **What is a common method to prevent overfitting in neural networks?**  \n",
        "    a) Use a smaller dataset  \n",
        "    b) Add dropout during training  \n",
        "    c) Increase the number of hidden states  \n",
        "    d) Skip training steps  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Dropout randomly deactivates neurons during training to prevent overfitting and encourage the model to generalize better to unseen data.\n",
        "\n",
        "53. **How does dropout help in preventing overfitting?**  \n",
        "    a) By adding more neurons to the network  \n",
        "    b) By randomly deactivating neurons during training, forcing the model to generalize  \n",
        "    c) By reducing the size of the input  \n",
        "    d) By increasing the learning rate  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Dropout prevents overfitting by randomly deactivating neurons during training, which forces the model to learn more robust features that generalize well.\n",
        "\n",
        "54. **What is early stopping in training neural networks?**  \n",
        "    a) Stopping the training process when the model’s performance on the validation set stops improving  \n",
        "    b) Increasing the learning rate after each epoch  \n",
        "    c) Reducing the number of training samples  \n",
        "    d) Stopping training after the first epoch  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Early stopping halts training when the model’s performance on the validation set stops improving, preventing overfitting and saving time.\n",
        "\n",
        "55. **What is regularization in the context of training neural networks?**  \n",
        "    a) A method to reduce the complexity of the model and prevent overfitting  \n",
        "    b) A technique to increase training data  \n",
        "    c) A process to decrease the learning rate  \n",
        "    d) A method to adjust the size of the hidden state  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Regularization techniques, such as L2 regularization or dropout, are used to reduce the complexity of the model and prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advanced Concepts and Enhancements**\n",
        "\n",
        "56. **How can using more layers in the RNN improve performance?**  \n",
        "    a) By allowing the network to learn more complex patterns in the data  \n",
        "    b) By decreasing the learning rate  \n",
        "    c) By reducing the number of parameters  \n",
        "    d) By simplifying the model  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adding more layers to the RNN allows the model to learn more complex patterns and representations in the data, potentially improving performance.\n",
        "\n",
        "57. **Why is increasing the hidden state size useful in RNNs?**  \n",
        "    a) It allows the model to process shorter sequences  \n",
        "    b) It enables the model to capture more detailed information about the input  \n",
        "    c) It decreases the number of computations  \n",
        "    d) It reduces the training time  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Increasing the hidden state size allows the model to store more detailed information about the input sequence, improving its ability to learn complex patterns.\n",
        "\n",
        "58. **What is the primary advantage of using LSTMs or GRUs over simple RNNs?**  \n",
        "    a) They are computationally less expensive  \n",
        "    b) They handle long-term dependencies better by preventing the vanishing gradient problem  \n",
        "    c) They require fewer training examples  \n",
        "    d) They do not use a hidden state  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs and GRUs are better at handling long-term dependencies in sequences by preventing the vanishing gradient problem, making them more effective than simple RNNs.\n",
        "\n",
        "59. **What is the vanishing gradient problem in RNNs?**  \n",
        "    a) The gradients become too small to make significant updates, hindering the learning of long-term dependencies  \n",
        "    b) The gradients increase exponentially, leading to overfitting  \n",
        "    c) The gradients are too large, causing the model to memorize the training data  \n",
        "    d) The gradients are ignored during backpropagation  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The vanishing gradient problem occurs when gradients become too small during backpropagation, preventing the network from learning long-term dependencies.\n",
        "\n",
        "60. **Why is the vanishing gradient problem less likely in LSTMs and GRUs?**  \n",
        "    a) They use convolutional layers instead of recurrent layers  \n",
        "    b) They have mechanisms like gates that control the flow of information and preserve important signals  \n",
        "    c) They do not require backpropagation  \n",
        "    d) They ignore the hidden state during training  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs and GRUs use mechanisms like gates (forget, input, and output gates) that control the flow of information, helping preserve important signals over time and avoiding the vanishing gradient problem.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Set of Questions**\n",
        "\n",
        "61. **What is one way to improve the generalization of an RNN model?**  \n",
        "    a) Use more dropout  \n",
        "    b) Skip training on the validation set  \n",
        "    c) Increase the hidden state size  \n",
        "    d) Use fewer training examples  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Dropout improves generalization by preventing the model from relying too heavily on any single neuron or feature during training.\n",
        "\n",
        "62. **What is gradient clipping?**  \n",
        "    a) A technique to prevent gradients from becoming too large and causing instability during training  \n",
        "    b) A method to reduce the learning rate  \n",
        "    c) A process to increase the number of hidden states  \n",
        "    d) A method to ignore gradients during backpropagation  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Gradient clipping limits the magnitude of the gradients during backpropagation to prevent them from becoming too large, which can destabilize the training process.\n",
        "\n",
        "63. **Which technique helps reduce training time while maintaining model accuracy?**  \n",
        "    a) Batch normalization  \n",
        "    b) Early stopping  \n",
        "    c) Dropout  \n",
        "    d) Reducing the dataset size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Early stopping halts training when the model's performance stops improving on the validation set, helping reduce unnecessary training time while maintaining accuracy.\n",
        "\n",
        "64. **What is the purpose of using mini-batches in training RNNs?**  \n",
        "    a) To process all the data in one go  \n",
        "    b) To update the weights more frequently and reduce memory usage  \n",
        "    c) To increase the learning rate  \n",
        "    d) To shuffle the data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Mini-batch training updates the weights more frequently, which can speed up training and reduce memory usage compared to processing the entire dataset at once.\n",
        "\n",
        "65. **How does an RNN model handle variable-length input sequences?**  \n",
        "    a) By padding shorter sequences  \n",
        "    b) By ignoring longer sequences  \n",
        "    c) By processing all inputs in fixed-size batches  \n",
        "    d) By discarding variable-length inputs  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* RNNs handle variable-length input sequences by padding shorter sequences so that all inputs have the same length, allowing them to be processed in batches.\n"
      ],
      "metadata": {
        "id": "I7GPCHuli32W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "66. **Why is padding used for variable-length sequences in RNNs?**  \n",
        "    a) To reduce the size of the dataset  \n",
        "    b) To make all sequences the same length for batch processing  \n",
        "    c) To increase the learning rate  \n",
        "    d) To increase the hidden state size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Padding is used to make all sequences the same length, allowing for efficient batch processing in RNNs.\n",
        "\n",
        "67. **How does the RNN update its hidden state at each time step?**  \n",
        "    a) By storing all previous inputs  \n",
        "    b) By applying a function to the current input and the previous hidden state  \n",
        "    c) By discarding the previous hidden state  \n",
        "    d) By updating only after the last time step  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The RNN updates its hidden state at each time step by applying a function (such as a weighted sum and activation) to the current input and the previous hidden state.\n",
        "\n",
        "68. **Which activation function is commonly used in the hidden layers of an RNN?**  \n",
        "    a) ReLU  \n",
        "    b) Sigmoid  \n",
        "    c) Tanh  \n",
        "    d) Softmax  \n",
        "    **Answer:** c  \n",
        "    *Explanation:* The `tanh` activation function is commonly used in the hidden layers of RNNs, allowing for the transformation of the hidden state values between -1 and 1.\n",
        "\n",
        "69. **Why is the Softmax function used in the final layer of an RNN for classification tasks?**  \n",
        "    a) To compute the gradients  \n",
        "    b) To normalize the output probabilities over the possible classes  \n",
        "    c) To reduce the input size  \n",
        "    d) To increase the hidden state size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The Softmax function converts the raw output scores into normalized probabilities, allowing the model to assign a probability to each class.\n",
        "\n",
        "70. **What type of loss function is suitable for multi-class classification in RNNs?**  \n",
        "    a) Mean Squared Error  \n",
        "    b) Negative Log-Likelihood Loss (NLLLoss)  \n",
        "    c) Cross Entropy Loss  \n",
        "    d) Hinge Loss  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Negative Log-Likelihood Loss (NLLLoss) is commonly used for multi-class classification tasks, particularly when combined with the LogSoftmax layer.\n",
        "\n",
        "---\n",
        "\n",
        "### **Training and Hyperparameters**\n",
        "\n",
        "71. **What is the effect of increasing the hidden state size in an RNN?**  \n",
        "    a) It allows the model to process fewer inputs  \n",
        "    b) It enables the model to capture more detailed information about the input sequence  \n",
        "    c) It decreases the training time  \n",
        "    d) It reduces the model's capacity  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Increasing the hidden state size allows the model to store more detailed information about the input sequence, improving its ability to learn complex patterns.\n",
        "\n",
        "72. **What is the role of the optimizer in training RNNs?**  \n",
        "    a) To adjust the learning rate after each epoch  \n",
        "    b) To update the model's weights by minimizing the loss function  \n",
        "    c) To predict the next character in the sequence  \n",
        "    d) To shuffle the input data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* The optimizer updates the model's weights by minimizing the loss function, helping the model learn to make better predictions over time.\n",
        "\n",
        "73. **What happens if the learning rate is set too high?**  \n",
        "    a) The model will converge faster  \n",
        "    b) The model may overshoot the optimal solution and fail to converge  \n",
        "    c) The model will learn more efficiently  \n",
        "    d) The model will underfit the data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A learning rate that is too high can cause the model to overshoot the optimal solution during training, leading to instability and failure to converge.\n",
        "\n",
        "74. **What is gradient clipping, and why is it used in training RNNs?**  \n",
        "    a) A method to increase the learning rate during training  \n",
        "    b) A technique to limit the magnitude of gradients to prevent instability during training  \n",
        "    c) A process to reduce the number of hidden states  \n",
        "    d) A method to increase the input size  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Gradient clipping limits the magnitude of the gradients during training to prevent them from becoming too large, which can cause instability in RNN training.\n",
        "\n",
        "75. **Which optimization algorithm is commonly used in RNN training due to its adaptive learning rates?**  \n",
        "    a) Adam  \n",
        "    b) Stochastic Gradient Descent (SGD)  \n",
        "    c) RMSprop  \n",
        "    d) Adagrad  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Adam is a popular optimizer for RNN training because it adapts the learning rate during training based on the gradients, improving convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### **Regularization and Generalization**\n",
        "\n",
        "76. **What is the primary goal of regularization in neural networks?**  \n",
        "    a) To increase the model's complexity  \n",
        "    b) To prevent overfitting by discouraging the model from learning noise  \n",
        "    c) To increase the number of hidden units  \n",
        "    d) To shuffle the input data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Regularization helps prevent overfitting by discouraging the model from memorizing the training data, thereby improving its ability to generalize to unseen data.\n",
        "\n",
        "77. **How does dropout work in RNNs?**  \n",
        "    a) By randomly deactivating neurons during training to prevent overfitting  \n",
        "    b) By reducing the size of the input  \n",
        "    c) By increasing the size of the hidden state  \n",
        "    d) By normalizing the input data  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Dropout prevents overfitting by randomly deactivating neurons during training, forcing the model to learn more robust and generalizable features.\n",
        "\n",
        "78. **When is early stopping useful in training neural networks?**  \n",
        "    a) When the model's performance on the validation set stops improving  \n",
        "    b) When the learning rate is too high  \n",
        "    c) When the model has too few hidden states  \n",
        "    d) When the optimizer reaches its final epoch  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Early stopping is useful when the model's performance on the validation set stops improving, helping to prevent overfitting and reducing unnecessary training.\n",
        "\n",
        "79. **What does L2 regularization do to the model's weights?**  \n",
        "    a) It adds a penalty based on the sum of the squared values of the weights  \n",
        "    b) It increases the number of neurons in the hidden state  \n",
        "    c) It decreases the learning rate  \n",
        "    d) It reduces the number of output classes  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* L2 regularization adds a penalty to the loss function based on the squared values of the model's weights, discouraging large weights and helping to prevent overfitting.\n",
        "\n",
        "80. **Why is batch normalization typically not used in RNNs?**  \n",
        "    a) Because it increases the number of hidden layers  \n",
        "    b) Because RNNs process sequences step-by-step, making it difficult to apply batch normalization across time steps  \n",
        "    c) Because it increases the model's complexity  \n",
        "    d) Because it reduces the size of the input  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Batch normalization is difficult to apply in RNNs because RNNs process sequences step-by-step, and applying batch normalization across time steps can disrupt the temporal dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "### **RNN vs. Other Architectures**\n",
        "\n",
        "81. **How do CNNs differ from RNNs when applied to text data?**  \n",
        "    a) CNNs process text sequentially, while RNNs process text in parallel  \n",
        "    b) CNNs extract local patterns (such as n-grams), while RNNs capture sequential dependencies  \n",
        "    c) CNNs handle long-term dependencies better than RNNs  \n",
        "    d) RNNs do not require input data to be sequential  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* CNNs extract local patterns (such as n-grams) from text data, while RNNs are designed to capture sequential dependencies across longer sequences.\n",
        "\n",
        "82. **What advantage do RNNs have over traditional feedforward neural networks for sequence data?**  \n",
        "    a) RNNs can handle fixed-size input only  \n",
        "    b) RNNs maintain a hidden state that allows them to process sequences of variable length and remember past information  \n",
        "    c) RNNs are faster to train  \n",
        "    d) RNNs do not need training data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* RNNs maintain a hidden state that allows them to process sequences of variable length and remember information from earlier time steps, making them well-suited for sequence data.\n",
        "\n",
        "83. **What is one major advantage of LSTMs and GRUs over simple RNNs?**  \n",
        "    a) They are more efficient for handling fixed-length sequences  \n",
        "    b) They prevent the vanishing gradient problem and capture long-term dependencies better  \n",
        "    c) They do not require backpropagation  \n",
        "    d) They require less training data  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* LSTMs and GRUs prevent the vanishing gradient problem and are better at capturing long-term dependencies compared to simple RNNs.\n",
        "\n",
        "84. **What does an attention mechanism do in sequence models?**  \n",
        "    a) It focuses the model on specific parts of the input sequence, helping it learn which parts are most relevant  \n",
        "    b) It discards irrelevant input data  \n",
        "    c) It increases the model's hidden state size  \n",
        "    d) It reduces the training time  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The attention mechanism allows the model to focus on specific parts of the input sequence, improving its ability to learn which parts of the sequence are most relevant\n",
        "\n",
        " for making predictions.\n",
        "\n",
        "85. **In which type of task would an RNN be more useful than a CNN?**  \n",
        "    a) Image classification  \n",
        "    b) Text generation  \n",
        "    c) Object detection  \n",
        "    d) Image segmentation  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* RNNs are more useful for tasks involving sequential data, such as text generation, where the model needs to process and generate data one step at a time.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advanced RNN Architectures**\n",
        "\n",
        "86. **What is an example of an RNN variant that uses attention mechanisms?**  \n",
        "    a) Simple RNN  \n",
        "    b) Seq2Seq models with attention  \n",
        "    c) CNN  \n",
        "    d) MLP  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Seq2Seq models with attention are RNN variants that use attention mechanisms to focus on relevant parts of the input sequence during prediction.\n",
        "\n",
        "87. **What type of task is a Seq2Seq model commonly used for?**  \n",
        "    a) Image classification  \n",
        "    b) Machine translation  \n",
        "    c) Sentiment analysis  \n",
        "    d) Object recognition  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Seq2Seq models are commonly used for machine translation tasks, where the model translates a sequence in one language to a sequence in another language.\n",
        "\n",
        "88. **What does bidirectional RNN mean?**  \n",
        "    a) An RNN that processes input sequences in both forward and reverse directions  \n",
        "    b) An RNN that uses both LSTMs and GRUs  \n",
        "    c) An RNN that skips every other character  \n",
        "    d) An RNN that does not use hidden states  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* A bidirectional RNN processes input sequences in both forward and reverse directions, allowing the model to learn from past and future context.\n",
        "\n",
        "89. **How does a bidirectional RNN improve performance on sequence tasks?**  \n",
        "    a) It decreases the number of parameters  \n",
        "    b) It allows the model to learn from both past and future context in the sequence  \n",
        "    c) It removes the need for backpropagation  \n",
        "    d) It increases the learning rate  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* A bidirectional RNN improves performance by allowing the model to learn from both past and future context, making predictions more accurate for tasks like translation and text classification.\n",
        "\n",
        "90. **What is the role of the cell state in LSTMs?**  \n",
        "    a) To store long-term information and control how much information is passed forward to the next time step  \n",
        "    b) To discard irrelevant input data  \n",
        "    c) To increase the size of the hidden state  \n",
        "    d) To reduce the number of neurons  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The cell state in LSTMs stores long-term information and helps control how much information is passed forward to the next time step.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Set of Questions**\n",
        "\n",
        "91. **What is the main reason for using GRUs instead of LSTMs?**  \n",
        "    a) GRUs are simpler and computationally more efficient, with fewer gates than LSTMs  \n",
        "    b) GRUs handle longer sequences better than LSTMs  \n",
        "    c) GRUs require no hidden state  \n",
        "    d) GRUs do not require backpropagation  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* GRUs are simpler than LSTMs because they have fewer gates, making them computationally more efficient while still capturing long-term dependencies.\n",
        "\n",
        "92. **Which gate in the LSTM controls how much of the previous cell state should be passed forward?**  \n",
        "    a) Forget gate  \n",
        "    b) Input gate  \n",
        "    c) Output gate  \n",
        "    d) Softmax gate  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The forget gate in an LSTM controls how much of the previous cell state should be passed forward, helping the model discard irrelevant information.\n",
        "\n",
        "93. **What is the function of the input gate in LSTMs?**  \n",
        "    a) To control how much new information should be added to the cell state  \n",
        "    b) To pass information directly to the output  \n",
        "    c) To reduce the learning rate  \n",
        "    d) To clip the gradients  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The input gate in LSTMs controls how much new information from the current input should be added to the cell state.\n",
        "\n",
        "94. **What is a vanishing gradient problem most commonly associated with?**  \n",
        "    a) Simple RNNs  \n",
        "    b) LSTMs  \n",
        "    c) CNNs  \n",
        "    d) GRUs  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The vanishing gradient problem is most commonly associated with simple RNNs, where gradients become too small during backpropagation, making it difficult for the network to learn long-term dependencies.\n",
        "\n",
        "95. **How does attention improve Seq2Seq models?**  \n",
        "    a) By allowing the model to focus on relevant parts of the input sequence  \n",
        "    b) By reducing the model's complexity  \n",
        "    c) By increasing the learning rate  \n",
        "    d) By skipping irrelevant inputs  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Attention improves Seq2Seq models by allowing the model to focus on relevant parts of the input sequence, improving performance on tasks like translation and summarization.\n",
        "\n",
        "96. **What does a teacher forcing technique do in RNN training?**  \n",
        "    a) Uses the actual output from the previous time step as the input to the next time step during training  \n",
        "    b) Uses the model’s own predictions as the input to the next time step  \n",
        "    c) Forces the model to increase its hidden state size  \n",
        "    d) Reduces the number of parameters  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Teacher forcing is a technique used in RNN training where the actual output from the previous time step is fed as input to the next time step during training, helping the model converge faster.\n",
        "\n",
        "97. **What is the primary benefit of using Seq2Seq models for language translation?**  \n",
        "    a) They process entire sequences at once  \n",
        "    b) They handle variable-length input and output sequences, making them ideal for translation tasks  \n",
        "    c) They require no hidden state  \n",
        "    d) They do not use backpropagation  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* Seq2Seq models handle variable-length input and output sequences, making them well-suited for tasks like language translation, where input and output sequences can differ in length.\n",
        "\n",
        "98. **How does a convolutional layer in a CNN differ from a recurrent layer in an RNN?**  \n",
        "    a) Convolutional layers process spatial data, while recurrent layers process sequential data  \n",
        "    b) Recurrent layers process data in parallel, while convolutional layers process data sequentially  \n",
        "    c) Recurrent layers have more parameters than convolutional layers  \n",
        "    d) Convolutional layers only work with fixed-size input  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* Convolutional layers process spatial data (e.g., images), while recurrent layers are designed to process sequential data (e.g., text, time series).\n",
        "\n",
        "99. **Why are RNNs commonly used for text generation tasks?**  \n",
        "    a) They process text in parallel  \n",
        "    b) They can generate text character-by-character or word-by-word, capturing sequential dependencies  \n",
        "    c) They do not require training  \n",
        "    d) They use hidden layers to process images  \n",
        "    **Answer:** b  \n",
        "    *Explanation:* RNNs are well-suited for text generation tasks because they generate text character-by-character or word-by-word, capturing the sequential dependencies necessary for coherent text generation.\n",
        "\n",
        "100. **What is a key challenge when training RNNs on long sequences?**  \n",
        "    a) The vanishing gradient problem can make it difficult to learn long-term dependencies  \n",
        "    b) RNNs cannot process sequences longer than a fixed size  \n",
        "    c) RNNs require too much memory for short sequences  \n",
        "    d) RNNs do not use backpropagation  \n",
        "    **Answer:** a  \n",
        "    *Explanation:* The vanishing gradient problem is a key challenge when training RNNs on long sequences, as it makes it difficult for the model to learn long-term dependencies from earlier parts of the sequence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LKFaWCBsi3zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lohX6OcJi3w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dbE7_lv8i3t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PT6j1Zg1i3qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ay0P4NFRi3oJ"
      }
    }
  ]
}